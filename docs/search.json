[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods for human population genetics and ancient DNA",
    "section": "",
    "text": "Preface\nThis book summarises prepared mini-courses for various computational tools and methods in the field of human archaeogenetic data analysis, with a particular emphasis on population genetics.\nThe chapters are contributed by different authors, as indicated in the Yaml-frontmatter of each chapter’s .qmd source file."
  },
  {
    "objectID": "Quarto_intro.html#setting-up",
    "href": "Quarto_intro.html#setting-up",
    "title": "1  Introduction to Quarto",
    "section": "1.1 Setting up",
    "text": "1.1 Setting up\nQuarto is the “next generation” of R Markdown and is usable on different tools.\n\nHere, we will describe how to set up your environment to use Quarto in RStudio, and VSCode.\nQuarto in general is set up to be very intuitive and user friendly. And while it is possible to set up different documents simultaneously, those can also easily be set up to in just the way you need for whatever occasion. So, for either communicating your results to collaborators, discuss code with your supervisor, setting up a website or writing your paper, to just name some scenarios, quarto comes in quite handy. So, let’s begin:"
  },
  {
    "objectID": "Quarto_intro.html#rstudio",
    "href": "Quarto_intro.html#rstudio",
    "title": "1  Introduction to Quarto",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nFor this, you have to download RStudio first. If you have done this already, we can get started.\n\n1.2.1 Getting started\nFirst, we have to install the quarto package using the following command in our console:\ninstall.packages(\"quarto\")\nNow we are ready to set up a quarto document.\nFor this we open a new project and select the quarto document we want to create. You can choose to set up a git repository as well. For practicality, I usually also tick the box visual markdown editor.\n\nThe new project will look like this:\n\n\n\n1.2.2 Universal instructions\nWhen setting up your document, quarto will always provide you with some presets. So first, we will have a look into the .qmd files, for they are handled the same way, regardless the format you are setting up (website, book, presentation, etc.).\n\n1.2.2.1 Render & save\nIf we now click on render, we will be provided with the preview of our final version if the project in the viewer.\nImportant: Do not mistake “save” with “render”. Just by saving, your document did not get rendered automatically, unless you tick the box “Render on Save”. You have to tick that box on each .qmd file individually though.\n\n\n\n1.2.2.2 Visual and Source\nIf you have chosen the Visual option on your toolbar, the preview will mostly resemble your .qmd files. If you are more comfortable with the R markdown optics, you can switch to Source.\nIn the Source version, you can write up your document in LaTeX.\n\n\n\n1.2.2.3 .qmd files\nQuarto will automatically provide you with two .qmd files, as well as a .yml file.\nThe .qmd files respond to the individual pages of your website or chapter of your book or pages of your presentation, etc. You can shape them individually or define the layout for all of them in the .yml file, to which we will get later.\nIn your .qmd files you also find a yaml at the top of your document, separated by\n---\n---\nWithin these, you can define the outline of .qmd file individually, starting with the page header. Other options, you might be using in the future are:\nauthor: Jessi Doe\n-&gt; will add an author underneath the header\nexecute:    echo: true\n-&gt; if &gt;true&lt;, code will be visible\ntoc:true\n-&gt; if &gt;true&lt;, a table of content will be automatically added\nbibliography: your_references.bib\n-&gt; file or list of files for your references\nIt is crucial to stick to the spacing. Otherwise, an error will occur.\n\n1.2.2.3.1 Insert\nIf you click Insert, you will realize, quarto provides you with a lot of options and shortcuts as well. So by just selecting on your chosen item to insert, it will be added to the document, while you are also provided with options on the appearance (in the case of figure/images or tables, etc). We will here have a brief look into how to work with R code and how to use the reference option.\n\n\n1.2.2.3.2 R code\nTo add R code to your file, you select Insert, select Code Cell and choose the kind of code you want to insert. In this case, R. There are some things to keep in mind though.\nDepending on how you have set up your .qmd file (or your .yml), your code will be visible or not on your website. To check your output, you can click the green arrow for the latest bit of code or the grey arrow above a bar to run the previous code.\n\nBut in case there are some chunks of code, you do not want to show all the time, there are some nice sets.\nSo if we just load the library tidyverse, for example, the additional information regardless and it will be also visible on our website.\n\nTo avoid this, we can set up a code chunk, looking like this:\n\nThis will prevent the output of this code chunk to be depicted on your website, while the package is otherwise active and can be used in the following R code. This is, by the way, true for all R code and data sets you will use: they will be active in your document and can be used in different code chunks, once provided.\nA code block included in your document could look like the following. Here I used the option\ncode-fold: true so the code can be extended. This option is only available in html though.\n\n\n\n1.2.2.3.3 References\nDepending on which citation program you are using, quarto is able to connect to it (Zotero works, for example). So, when selecting to insert a Citation, you can choose to simply add a reference from your program.\nAlternatively, you are provided with some options to choose from:\n\nIn your source code and your website, a citation will be depicted as follows:\n\nThe citation will also automatically be added to a references.bib as well as to a references.qmd and is therefore available on your website on the page “References”, which also will be created automatically.\n\n\n\n\n1.2.3 Render your document\nWhen done with setting up your documents, you would like to have the actual output. Depending on the format you set in your .yml file, your output can be a html, pdf, MS Word, OpenOffice, or ePUB file. To create those, the terminal in your RStudio Project is used.\nBy using the command:\nquarto render\nall formats you predefined in your .yml file will be rendered.\nIn case you are only interested in one format to be rendered, you can specify your command:\nquarto render -to pdf\nYour rendered document should now look like this:\n\n\n\n1.2.4 Quarto website\nIn the provided .qmd files, the index is also the first page of your website. As you might have noticed, quarto already sets up a navbar as well as a search function on your website.\n\n\n1.2.4.1 .yml files\nWhile your .qmd files contain information about one page, the .yml file defines the overall looks of the website.\nHere, you can define the type of your project (in this case a website), you can change the name of the website (Title), define your navbar (which shall be your landing page and in what order your pages shall be set up) or the overall appearance of your website in general (theme, css, toc, backgroundcolor, etc.). For different styles and layouts, check out the quarto website again.\n\n\n\n\n1.2.5 Quarto book\nThe setup of the .yml file in a quarto book is slightly different than that of the website. So here are some general remarks about them.\n\n1.2.5.1 .yml files\nWe see, for example, that instead of a navbar, we find chapters. Those will appear in the listed order in your book.\nWe also already get provided with a bibliography and the responding .bib file. If you have other .bib files, those can be included in your references, by just adding them to bibliography.\n\n_________________________________________________________________________\nWith this, you should at least have some ideas on how you can use quarto in your daily work routine. Happy coding and please feel free to contact me for any remarks or questions. I am happy to try and help."
  },
  {
    "objectID": "Quarto_intro.html#vscode",
    "href": "Quarto_intro.html#vscode",
    "title": "1  Introduction to Quarto",
    "section": "1.3 VSCode",
    "text": "1.3 VSCode\nMuch of the concepts as described in the RStudio tutorial above apply equally well to using Quarto in VSCode, just with a different interface to execute them.\nHere we will describe how to set up VSCode and Quarto, and how to preview and render Quarto objects within the VSCode interface.\nTo understand about more about the details of which files to edit etc., please see the RStudio description above.\n\n1.3.1 Getting Started\n\nInstall the Quarto CLI tool for your operating system from the Quarto Website\nInstall the VSCode Quarto extension\n\n\n\n1.3.2 Using Quarto\nThe basic workflow is as follows:\n\nCreate or modify .qmd objects etc as described above in the Rstudio section about Quarto markdown files\nWithin VSCode, make sure you’ve opened it in the repository containing all the files\nPress ctrl + shift + p to bring up your command palette\n\nTo preview a local ‘live’ version of HTML or website versions, you can type Quarto: preview. To close the live preview, press ctrl+c in the VSCode terminal.\nTo render all the files e.g. final HTML and/or PDF versions, you can type Quarto: Render Project, where you will be given different options depending on the formats defined in the _quarto.yml file.\n\n\nFor further VSCode integrations, just type Quarto: into your command palette and explore all the options."
  },
  {
    "objectID": "poseidon.html#the-components-of-poseidon",
    "href": "poseidon.html#the-components-of-poseidon",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.1 The components of Poseidon",
    "text": "2.1 The components of Poseidon\n\n\n\nOverview of the Poseidon framework\n\n\nPoseidon is an entire ecosystem built on top of the data format specification of the Poseidon package.\n\n2.1.1 The Poseidon package\nA Poseidon package bundles genotype data in EIGENSTRAT/PLINK format with human- and machine-readable meta-data.\n\n\n\nThe files in a Poseidon package\n\n\nIt includes sample-wise context information like spatio-temporal origin and genetic data quality in the .janno-file, literature in the .bib-file, and pointers to sequencing data in the .ssf-file.\n.janno and .ssf have many predefined and specified columns, but can store arbitrary additional variables.\n\n\n2.1.2 The software tools\ntrident is a command line application to create, download, inspect and merge Poseidon packages – and therefore the central tool of the Poseidon framework. The init subcommand creates new packages from genotype data, fetch downloads them from the public archives through the Web-API, and forge merges and subsets them as specified. list gives an overview over entities in a set of packages and validate confirms their structural integrity.\nxerxes is derived from trident and allows to directly perform various basic and experimental genomic data analyses on Poseidon packages. It implements allele sharing statistics (\\(F_2\\), \\(F_3\\), \\(F_4\\), \\(F_{ST}\\)) with a flexible permutation interface.\njanno is an R package to simplify reading .janno files into R and the popular tidyverse ecosystem (Wickham et al. (2019)). It provides an S3 class janno that inherits from tibble.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nqjanno is another command line tool to perform SQL queries on .janno files. On start-up it creates an SQLite database in memory and reads .janno files into it. It then sends any user-provided SQL query to the database server and forwards its output.\n\n\n2.1.3 The public archives\nThe Poseidon community maintains public archives for Poseidon packages to establish a central open point of access for published archaeogenetic genotype data.\n\nThe Community Archive: Author supplied per-paper packages with the genotype data published in the respective papers. Partially pre-populated from various versions of the AADR.\nThe AADR Archive: Complete and structurally unified releases of the Allen Ancient DNA Resource (Mallick et al. (2023)) repackaged in the Poseidon package format.\nThe Minotaur Archive: Per-paper packages with genotype data reprocessed by the Minotaur workflow (see below).\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\nThe data is versioned with Git and hosted on GitHub for easy co-editing and automatic structural validation.\nIt can be accessed through a Web-API with various endpoints at server.poseidon-adna.org, e.g. /packages for a JSON list of packages in the community archive.\nThis API enables a little Archive explorer web app on the Poseidon website.\n\n\n2.1.4 The Minotaur workflow\nThe Minotaur workflow is a semi-automatic workflow to reproducibly process published sequencing data from the International Nucleotide Sequence Database Collaboration (INSDC) archives into Poseidon packages.\nCommunity members can request new packages by submitting a build recipe as a Pull Request against a dedicated submission GitHub repository. This recipe is derived from a Sequencing Source File (.ssf), describing the sequencing data for the package and where it can be downloaded.\nUsing the recipe, the sequencing data gets processed through nf-core/eager (Fellows Yates et al. (2021)) on computational infrastructure of MPI-EVA, using a standardised, yet flexible, set of parameters.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\nThe generated genotypes, together with descriptive statistics of the sequencing data (Endogenous, Damage, Nr_SNPs, Contamination), are compiled into a Poseidon package and made available to users in the Minotaur archive."
  },
  {
    "objectID": "poseidon.html#forging-a-dataset-with-trident",
    "href": "poseidon.html#forging-a-dataset-with-trident",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.2 Forging a dataset with trident",
    "text": "2.2 Forging a dataset with trident\nforge creates new Poseidon packages by extracting and merging packages, populations and individuals/samples from your Poseidon repositories. It can also work directly with your genotype data. In addition, forge allows merging of multiple data sets (packages), in contrast to mergeit which merges only two data sets at a time.\n(-f/--forgeString) can be used to query entire packages, groups or individuals. In general --forgeString query consists of multiple entities, inside \"\" separated by , .\n\nTo include all individuals in a Poseidon package, use * to surround the package title (e.g. *2019_Jeong_InnerEurasia*) . In cases where only genotype files are available, use the file name prefix.\nTo include certain group(s) from a Poseidon package, simply add them to the -f query. No specific markers are required. Russia_HG_Karelia. You don’t have to specify the group, as trident will search all packages for the given group.\nTo extract individuals only, surround them by &lt; and &gt;. &lt;ALA026&gt; . To exclude individuals add package name *package* and &lt;individual&gt; with a dash sign. \"*2021_Saag_EastEuropean-3.2.0*,-&lt;NIK003&gt;\"\n\n\ntrident forge \\\n  -p pileupcaller.double.geno \\\n  -d 2021_Saag_EastEuropean-3.2.0 \\\n  -d 2016_FuNature-2.1.1 \\\n  -f \"*pileupcaller.double*,Russia_AfontovaGora3,&lt;NIK003&gt;\" \\\n  -o testpackage \\\n  --outFormat EIGENSTRAT \\\n  /\nForge selection language\nforge has a an optional flag --intersect, that defines whether the genotype data from different packages should be merged with an intersect instead of the default union operation. The default is to output the union of all SNPs, by setting the additional SNPs from the other merged package as missing in the samples that did not have them originally. This option is useful for making a data set based on Human Origins (HO) SNPs for analysis like PCA and ADMIXTURE.\ntrident forge \\\n  --intersect \\\n  -p pileupcaller.double.geno \\\n  -d 2012_PattersonGenetics-2.1.3 \\\n  -o testpackage_HO \\\n  --outFormat EIGENSTRAT \\\n  /\nIn case of PCA, --forgeFile can be used to merge necessary populations/groups from the available packages in the community archive to create specific PCA configurations.\ntrident forge \\\n  -d /path/to/community/archive \\\n  --forgeFile WestEurasia_poplist.txt \\\n  -o WE_PCA \\\n  /\nIn addition, --selectSnps allows to provide forge with a SNP file in EIGENSTRAT (.snp) or PLINK (.bim) format to create a package with a specific selection. This option generates a package with exactly the SNPs listed in this file."
  },
  {
    "objectID": "poseidon.html#contributing-to-the-community-archive",
    "href": "poseidon.html#contributing-to-the-community-archive",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.3 Contributing to the community archive",
    "text": "2.3 Contributing to the community archive\n\n\n\nPoseidon needs your data as soon as it is published\n\n\nTo maintain the public data archives, specifically the community archive and the minotaur archive, Poseidon depends on work donations by an interested community.\nMany practitioners of archaeogenetics both produce genotype data from archaological contexts and require the reference data from other publications, provided in public archives, to contextualize it.\nIf authors themselves provide high-quality, easily accessible versions of their data beyond the raw data available at the INSDC databases, they gain at least three important advantages:\n\nTheir work will be easily findable and potentially cited more often.\nThey have primacy over how their data is communicated. That is, which genotypes, dates or group names they consider correct.\nTheir results for derived, genotype based analyses (PCA, F-Statistics, etc.) can be reproduced exactly.\n\nAnd the whole community wins, because sharing the tedious data preparation tasks empowers all researchers to achieve more in shorter time.\n\n\n\n\nThis tutorial explains the main cornerstones of a workflow to add a new Poseidon package to the community archive after publishing the corresponding dataset. The process is documented in more detail in a Submission guide on the website.\n\nMake yourself familiar with a number of core technologies. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful beyond this particular task.\n\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\n\nCreate a package from your genotype data and fill it with a suitable set of meta and context information.\n\n\ntrident init allows to wrap genotype data in a dummy Poseidon package. Imagine we had genotype data for a number of individuals in EIGENSTRAT format:\n\n\nmyData.ind\n\nSample1  M       ExamplePop1\nSample2  F       ExamplePop1\nSample3  M       ExamplePop2\n\n\n\nmyData.snp\n\n           rs3094315     1        0.020130          752566 G A\n          rs12124819     1        0.020242          776546 A G\n          rs28765502     1        0.022137          832918 T C\n\n\n\nmyData.geno\n\n099\n922\n999\n\nWith trident init -p myData.geno -o myPackage we can create a basic package around this data.\n$ ls myPackage\nmyData.geno  myData.snp     myPackage.janno\nmyData.ind   myPackage.bib  POSEIDON.yml\nIn a next step we modify POSEIDON.yml, .janno and .bib to include the context information we consider relevant. All of these files are well specified and documented, so we only demonstrate a minimal change for this example:\nWe replace the main contributor for the package.\n\n\nmyPackage/POSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: myPackage\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Clemens Schmid               #- name: Josiah Carberry\n  email: clemens_schmid@eva.mpg.de   #  email: carberry@brown.edu\n  orcid: 0000-0003-3448-5715         #  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2023-10-18\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: myData.geno\n  snpFile: myData.snp\n  indFile: myData.ind\n  snpSet: Other\njannoFile: myPackage.janno\nbibFile: myPackage.bib\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d myPackage.\n\n\nSubmit the package to the community archive.\n\n\nTo submit the package we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:&lt;yourGitHubUserName&gt;/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\nin the community-archive directory\n\ncp -r ../myPackage myPackage\ngit add myPackage\ngit commit -m \"added a first draft of myPackage\"\ngit push\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository"
  },
  {
    "objectID": "fstats.html#admixture---f3-statistics",
    "href": "fstats.html#admixture---f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.1 Admixture - F3 Statistics",
    "text": "3.1 Admixture - F3 Statistics\nF3 statistics are a useful analytical tool to understand population relationships. F3 statistics, just as F4 and F2 statistics measure allele frequency correlations between populations and were introduced by Nick Patterson (Patterson et al. 2012), but see also (Peter 2016) for another introduction.\nF3 statistics are used for two purposes: i) as a test whether a target population (C) is admixed between two source populations (A and B), and ii) to measure shared drift between two test populations (A and B) from an outgroup (C).\nF3 statistics are in both cases defined as the product of allele frequency differences between population C to A and B, respectively:\n\\[F3(A,B;C)=\\langle(c−a)(c−b)\\rangle\\]\nHere, \\(\\langle\\cdot\\rangle\\) denotes the average over all genotyped sites, and a, b and c denote the allele frequency for a given site in the three populations A, B and C.\nIt can be shown that if \\(F3(A, B; C)\\) is negative, it provides unambiguous proof that population C is admixed between populations A and B, as in the following phylogeny (taken from Figure 1 from (Patterson et al. 2012):\n\nIntuitively, an F3 statistics becomes negative if the allele frequency of the target population C is on average intermediate between the allele frequencies of A and B. Consider as an extreme example a genomic site where \\(a=0\\), \\(b=1\\) and \\(c=0.5\\). Then we have \\((c−a)(c−b)=−0.25\\), which is negative. So if the entire statistics is negative, it suggests that in many positions, the allele frequency c is indeed intermediate, suggesting admixture between the two sources.\nOne way to understand this is by looking what happens to a list of SNPs and allele frequencies for groups A, B and C:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0.5\n-0.25\n\n\n2\n0.8\n0\n0\n0\n\n\n3\n0\n0.7\n0\n0\n\n\n4\n0.1\n0.5\n0.3\n-0.04\n\n\n5\n0\n0.1\n0.2\n0.02\n\n\n6\n1\n0.2\n0.9\n-0.07\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n-0.057\n\n\n\nEvery SNP where C has an allele frequency intermediate between A and B contributes negatively. Here, the average is also negative, providing evidence for admixture. For statistical certainty, an error bar for this estimate is needed, which is typically computed via Jackknife (see for example the xerxes whitepaper).\n\n\n\n\n\n\nCaution\n\n\n\nIf an F3 statistics is not negative, it does not proof that there is no admixture!\n\n\n\n3.1.1 Computing Admixture-F3 with xerxes\nWe will use this statistics to test if Finnish are admixed between East and West, using different Eastern and Western sources. In the West, we use French, Icelandic, Lithuanian and Norwegian as source, and in the East we use Nganasan and one of the ancient individuals analysed in (Lamnidis et al. 2018), from the site of Bolshoy Oleni Ostrov from the Northern Russian Kola-peninsula, and dating to 3500 years before present.\n\n\n\n\n\n\nTip\n\n\n\nIf you happen to have downloaded a copy of the Poseidon Community Archive already, then just use the path to that archive in the following commands. Otherwise you download the entire archive via\ntrident fetch -d /somewhere/to/store/the/archive --downloadAll \nor just the relevant packages for the examples in this chapter:\ntrident fetch -d /somewhere/to/store/the/archive -f \"*2012_PattersonGenetics*,*2014_RaghavanNature*,*2014_LazaridisNature*,*2018_Lamnidis_Fennoscandia*\"\n\n\nWe use the software xerxes fstats from the Poseidon Framework. Here is a command line that computes 8 statistics for us:\nxerxes fstats -d ~/dev/poseidon-framework/community-archive \\\n    --stat \"F3(Nganasan,French,Finnish)\" \\\n    --stat \"F3(Nganasan, Icelandic, Finnish)\" \\\n    --stat \"F3(Nganasan, Lithuanian, Finnish)\" \\\n    --stat \"F3(Nganasan, Norwegian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, French, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Icelandic, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Lithuanian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Norwegian, Finnish)\" \\\n\n\n\n\n\n\nNote\n\n\n\nNote that xerxes fstats will automatically find the right packages from your local archive that contain these groups. You can see in the output of the program which packages contribute:\n[Info]    5 relevant packages for chosen statistics identified:\n[Info]    *2012_PattersonGenetics-2.1.3*\n[Info]    *2014_LazaridisNature-4.0.2*\n[Info]    *2016_LazaridisNature-2.1.3*\n[Info]    *2018_Lamnidis_Fennoscandia-2.1.0*\n[Info]    *2019_Flegontov_PalaeoEskimo-2.2.1*\nSo these five packages contain the samples requested in these statistics. You can inquire about this also more manually using trident list\n\n\nHere is the result that you should get, nicely layouted in a Text-table:\n.-----------.----------------.------------.---------.---.---------.----------------.--------------------.------------------.---------------------.\n| Statistic |       a        |     b      |    c    | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife |  Z_score_Jackknife  |\n:===========:================:============:=========:===:=========:================:====================:==================:=====================:\n| F3        | Nganasan       | French     | Finnish |   | 593124  | -1.0450e-3     | -1.0451e-3         | 1.2669e-4        | -8.249133659451905  |\n| F3        | Nganasan       | Icelandic  | Finnish |   | 593124  | -1.1920e-3     | -1.1920e-3         | 1.3381e-4        | -8.908381869946188  |\n| F3        | Nganasan       | Lithuanian | Finnish |   | 593124  | -1.1605e-3     | -1.1605e-3         | 1.5540e-4        | -7.4680182465607245 |\n| F3        | Nganasan       | Norwegian  | Finnish |   | 593124  | -1.0913e-3     | -1.0914e-3         | 1.3921e-4        | -7.83945981272796   |\n| F3        | Russia_Bolshoy | French     | Finnish |   | 542789  | -6.1807e-4     | -6.1809e-4         | 1.0200e-4        | -6.059872900228102  |\n| F3        | Russia_Bolshoy | Icelandic  | Finnish |   | 542789  | -6.2801e-4     | -6.2802e-4         | 1.1792e-4        | -5.325695961373772  |\n| F3        | Russia_Bolshoy | Lithuanian | Finnish |   | 542789  | -3.7310e-4     | -3.7310e-4         | 1.2973e-4        | -2.8760029685791637 |\n| F3        | Russia_Bolshoy | Norwegian  | Finnish |   | 542789  | -3.7646e-4     | -3.7653e-4         | 1.1630e-4        | -3.2375830440434323 |\n'-----------'----------------'------------'---------'---'---------'----------------'--------------------'------------------'---------------------'\n\n\n\n\n\n\nTip\n\n\n\nUse the option -f &lt;FILE&gt; to output the results additionally to a tab-separated file, or --raw if you prefer the standard output to be tab-separated\n\n\nYou can see that in all cases this statistic is negative (Estimate_Total). The next two columns ( Estimate_Jackknife and StdErr_Jackknife) are computed using Jackknifing (see the xerxes whitepaper for details). The last column is the Z score, and it is important here: It gives the deviation of the f3 statistic from zero in units of the standard error. As general rule, a Z score of -3 or more suggests a significant rejection of the Null hypothesis that the statistic is not negative. In this case, all of the statistics are significantly negative (with one borderline exception), proving that Finnish have ancestral admixture of East and West Eurasian ancestry. Here, Eastern ancestry is represented by Nganasan or Russia_Bolshoy, respectively, while Western ancestry by French, Icelandic, Lithuanian and Norwegian, respectively. Note that the statistics does not suggest when this admixture happened!\n\n\n3.1.2 Running xerxes via a configuration file\nAs you will have noticed, the command line above is getting quite long, since a separate --stat option has to be entered for every statistic to be computed. There is a more powerful and elegant interface to xerxes, which uses a configuration file in YAML format. To illustrate it, let us consider the configuration file (which can be found in fstats_working/F3_finnish.config in the git-repository of this book) needed to compute the same statistic as above:\nfstats:\n- type: F3\n  a: [\"Nganasan\", \"Russia_Bolshoy\"]\n  b: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  c: [\"Finnish\"]\nYou can then run xerxes as\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F3_finnish.config\nSo xerxes then automatically creates all combinations of populations listed in slots a, b and c.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are actually three types of F3-statistics supported by xerxes:\n\nF3vanilla: The purest form, defined literally as \\(\\langle(c−a)(c−b)\\rangle\\)\nF3: A bias-corrected version, which is only valid for groups in C that have non-zero heterozygosity\nF3star: This one is normalised by the heterozgygosity of the third population, C, as suggested in (Patterson et al. 2012) and implemented in the Admixtools package.\n\nThe white-paper explains this in detail.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe configuration file format has a lot more options. Here is a bit more complex example, but see also the documentation:\n# You can define groups right within the configuration file.\n# here we use negative selection to remove individuals from the\n# newly defined groups\ngroupDefs:\n  CEU2: [\"CEU.SG\", \"-&lt;NA12889.SG&gt;\", \"-&lt;NA12890.SG&gt;\"]\n  FIN2: [\"FIN.SG\", \"-&lt;HG00383.SG&gt;\", \"-&lt;HG00384.SG&gt;\"]\n  GBR2: [\"GBR.SG\", \"-&lt;HG01791.SG&gt;\", \"-&lt;HG02215.SG&gt;\"]\n  IBS2: [\"IBS.SG\", \"-&lt;HG02238.SG&gt;\", \"-&lt;HG02239.SG&gt;\"]\nfstats:\n- type: F2 # this will create 2x2 = 4 F2-Statistics\n  a: [\"French\", \"Spanish\"]\n  b: [\"Han\", \"CEU2\"]\n- type: F3vanilla # This will create 3x2x1 = 6 Statistics\n  a: [\"French\", \"Spanish\", \"Mbuti\"]\n  b: [\"Han\", \"CEU2\"]\n  c: [\"&lt;Chimp.REF&gt;\"]\n- type: F4 # This will create 5x5x4x1 = 100 Statistics\n  a: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  b: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  c: [\"CEU2\", \"FIN2\", \"GBR2\", \"IBS2\"]\n  d: [\"&lt;Chimp.REF&gt;\"]\n# Altogether: 110 statistics of different types\nwhich will not just create multiple statistic using row-combinations, as described, but also uses newly defined groups and combines multiple statistic types (F2, F3 and F4) in one run."
  },
  {
    "objectID": "fstats.html#f4-statistics",
    "href": "fstats.html#f4-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.2 F4 Statistics",
    "text": "3.2 F4 Statistics\nA different way to test for admixture is by “F4 statistics” (or “D statistics” which is very similar), also introduced in (Patterson et al. 2012).\nF4 statistics are also defined in terms of correlations of allele frequency differences, similarly to F3 statistics, but involving four different populations, not just three. Specifically we define\n\\[F4(A,B;C,D)=\\langle(a−b)(c−d)\\rangle.\\]\n\n3.2.1 Shaping intuition - the ABBA- and BABA-sites\nTo understand the statistics, consider the following tree:\n\nIn this tree, without any additional admixture, the allele frequency difference between A and B should be completely independent from the allele frequency difference between C and D. In that case, F4(A, B; C, D) should be zero, or at least not statistically different from zero. However, if there was gene flow from C or D into A or B, the statistic should be different from zero. Specifically, if the statistic is significantly negative, it implies gene flow between either C and B, or D and A. If it is significantly positive, it implies gene flow between A and C, or B and D.\nIt is helpful to again consider an example using a SNP list, this time assuming that every population is just a single (haploid) individual, so each allele frequency can just be 0 or 1. For example:\n\n\n\nSNP\nA\nB\nC\nD\n\\((a-b)(c-d)\\)\n\n\n\n\n1\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n1\n1\n0\n\n\n3\n0\n1\n1\n0\n-1\n\n\n4\n0\n1\n0\n1\n1\n\n\n5\n1\n0\n0\n1\n-1\n\n\n6\n1\n0\n0\n0\n0\n\n\n\\(F_4(A, B;C, D)\\)\n\n\n\n\n-0.0167\n\n\n\nYou can see that the only SNPs that contribute positively to this statistics are SNPs where the alleles are distributed as 1010 and 0101, and the only SNPs that contribute negatively are 1001 and 0110. In the literature, the two patterns have been dubbed “ABBA” and “BABA”, which is why the statistical test behind this statistic (see below) was sometimes called the ABBA-BABA test (see for example (Martin, Davey, and Jiggins 2015)).\nThe intuition here is straight-forward: In positions that are polymorphic in both \\((A,B)\\) and \\((C,D)\\), this statistic asks whether B is genetically more similar to C than it is to D. This is most useful as a test for “treeness”: If A, B, C, D are related to each other as indicated in the above tree, then B should be equally closely related to C as to D. But if we actually find evidence that B is closer to C than to D, or vice versa, then this means that the tree above cannot be correct, but that there must be a closer connection between B and C or B and D, depending on the sign of the statistic.\n\n\n3.2.2 From single samples to allele frequencies\nSo the ABBA- and BABA-categories of SNPs help shape intuition for how this statistic behaves for single haploid genomes. But what about population allele frequencies? Looking back at the formula \\(\\langle(a−b)(c−d)\\rangle\\) this doesn’t help very much with intuition how this behaves with frequencies. Well, a nice feature of F4-Statistics is that averages factor out. This means, that if you have multiple samples in one or multiple slots A, B, C or D, the total F4-statistic of the groups is exactly equal to the average of F4-Statistics of the individuals. Here is a more mathematical definition.\nLet’s say we have 2 individuals in each of A and B, so we may perhaps write \\(A=\\{A_1,A_2\\}\\) and \\(B=\\{B_1,B_2\\}\\). Then one can show to have\n\\[F4(A, B; C, D) = \\text{Average of}[F4(A_1, B_1; C, D), F4(A_1, B_2; C, D), F4(A_2, B_1; C, D), F4(A_2, B_2; C, D)]\\]\nso just thte average over all individual-based F4-statistics. And this can be shown to be true for arbitrary numbers of samples. So in other words: An F4-Statistic always measures the average excess of pairwise BABA SNPs over ABBA SNPs. To me, this is a useful insight, as I find thinking in terms of ABBA-BABA somehow more helpful than thinking in terms of correlations of allele-frequency differences (which is really what the original formula is).\n\n\n\n\n\n\nNote\n\n\n\nF4-statistics have been famously used to show that Neanderthals are more closely related to Non-African populations than to Africans, suggesting gene-flow between Neanderthals and Non-Africans (shown in (Green et al. 2010)). You can reproduce this famous result with\nxerxes fstats -d ~/poseidon_repo --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Yoruba,French)' \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Sardinian,French)'\nwhich shows that the first statistic is significantly positive with a Z-score of 7.99, while the second one is insignificantly different from zero (Z=1.01)\n\n\nThe way this statistic is often used, is to put a divergent outgroup as population A, for which we know for sure that there was no admixture into either C or D. With this setup, we can then test for gene flow between B and D (if the statistic is positive), or B and C (if it is negative).\n\n\n3.2.3 Running F4-Statistics with xerxes\nHere, we can use this statistic to test for East Asian admixture in Finns, similarly to the test using Admixture F3 statistics above. We will again use xerxes fstats. We again prepare a configuration file (in fstats_working/F4_finish.config in the git-repository of this book), this time with four populations (A, B, C, D):\nfstats:\n- type: F4\n  a: [\"Mbuti\"]\n  b: [\"Nganasan\", \"Russia_Bolshoy\"]\n  c: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  d: [\"Finnish\"]\nYou can again run via\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F4_finnish.config\nThe result is:\n.-----------.-------.----------------.------------.---------.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |   a   |       b        |     c      |    d    | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=======:================:============:=========:=========:================:====================:==================:====================:\n| F4        | Mbuti | Nganasan       | French     | Finnish | 593124  | 2.3114e-3      | 2.3115e-3          | 1.2676e-4        | 18.235604067907143 |\n| F4        | Mbuti | Nganasan       | Icelandic  | Finnish | 593124  | 1.6590e-3      | 1.6590e-3          | 1.4861e-4        | 11.163339072181776 |\n| F4        | Mbuti | Nganasan       | Lithuanian | Finnish | 593124  | 1.3290e-3      | 1.3290e-3          | 1.4681e-4        | 9.052979707622278  |\n| F4        | Mbuti | Nganasan       | Norwegian  | Finnish | 593124  | 1.6503e-3      | 1.6503e-3          | 1.5358e-4        | 10.745850997260929 |\n| F4        | Mbuti | Russia_Bolshoy | French     | Finnish | 542789  | 1.8785e-3      | 1.8785e-3          | 1.2646e-4        | 14.854487416366263 |\n| F4        | Mbuti | Russia_Bolshoy | Icelandic  | Finnish | 542789  | 1.0829e-3      | 1.0828e-3          | 1.4963e-4        | 7.236818881873822  |\n| F4        | Mbuti | Russia_Bolshoy | Lithuanian | Finnish | 542789  | 5.4902e-4      | 5.4907e-4          | 1.4601e-4        | 3.7605973064589096 |\n| F4        | Mbuti | Russia_Bolshoy | Norwegian  | Finnish | 542789  | 9.3473e-4      | 9.3475e-4          | 1.5302e-4        | 6.108881868125652  |\n'-----------'-------'----------------'------------'---------'---------'----------------'--------------------'------------------'--------------------'\nAs you can see, in all cases, the Z score is positive and larger than 3, indicating a significant deviation from zero, and implying gene flow between Nganasan and Finnish, and BolshoyOleniOstrov and Finnish, when compared to French, Icelandic, Lithuanian or Norwegian."
  },
  {
    "objectID": "fstats.html#outgroup-f3-statistics",
    "href": "fstats.html#outgroup-f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.3 Outgroup-F3-Statistics",
    "text": "3.3 Outgroup-F3-Statistics\nOutgroup F3 statistics are a special case how to use F3 statistics. The definition is the same as for Admixture F3 statistics, but instead of a target C and two source populations A and B, one now gives an outgroup C and two test populations A and B.\nTo get an intuition for this statistics, consider the following tree:\n\nIn this scenario, the statistic F3(A, B; C) measures the branch length from C to the common ancestor of A and B, coloured red. So this statistic is simply a measure of how closely two population A and B are related with each other, as measured from a distant outgroup. It is thus a similarity measure: The higher the statistic, the more genetically similar A and B are to one another.\nHere is again a SNP table to illustrate, using haploid individuals:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n0\n1\n1\n\n\n4\n1\n0\n1\n0\n\n\n5\n1\n1\n0\n1\n\n\n6\n0\n0\n1\n1\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n0.5\n\n\n\nYou can see that each position which is similar between A and B, but different to C contributes 1, all other SNPs 0. So it directly measures similarity between A and B on alleles that differ from the outgroup C.\n\n\n\n\n\n\nNote\n\n\n\nNote that the averaging-relation shown for F4 statistics above is also true for Outgroup-F3 statistics, but only for populations A and B, not for C. So if you have multiple samples in A and B, you may think of this statistic being the average over all pairwise nucleotide similarities between individuals in A and B with respect to the same outgroup C.\n\n\nWe can use this statistic to measure for example the genetic affinity to East Asia, by performing the statistic F3(Han, X; Mbuti), where Mbuti is a distant African population and acts as outgroup here, Han denote Han Chinese, and X denotes various European populations that we want to test.\nYou can again define a configuration file that performs looping over various populations X for you:\nfstats:\n- type: F3\n  a: [\"Han\"]\n  b: [\"Chuvash\", \"Albanian\", \"Armenian\", \"Bulgarian\", \"Czech\", \"Druze\", \"English\",\n      \"Estonian\", \"Finnish\", \"French\", \"Georgian\", \"Greek\", \"Hungarian\", \"Icelandic\",\n      \"Italian_North\", \"Italian_South\", \"Lithuanian\", \"Maltese\", \"Mordovian\", \"Norwegian\",\n      \"Orcadian\", \"Russian\", \"Sardinian\", \"Scottish\", \"Sicilian\", \"Spanish_North\",\n      \"Spanish\", \"Ukrainian\", \"Finland_Levanluhta\", \"Russia_Bolshoy\", \"Russia_Chalmny_Varre\", \"Saami.DG\"]\n  c: [\"Mbuti\"]\nwhich cycles through many populations from Europe, including the ancient individuals from Chalmny Varre, Bolshoy Oleni Ostrov and Levänluhta (described in (Lamnidis et al. 2018)). We store this file in a file called fstats_working/OutgroupF3_europe.config and run via:\nxerxes fstats --statConfig fstats_working/OutgroupF3_europe.config -d ~/dev/poseidon-framework/community-archive -f fstats_working/outgroupf3_europe.tsv\n\n\n\n\n\n\nWarning\n\n\n\nOften in Outgroup-F3-statistics you use single genomes for population C, sometimes even single haploid genomes. In this case, F3 and F3star will get undefined results, because ordinary F3 and F3star statistics require population C to have non-zero average heterozygosity, so you will need at least one diploid sample, or multiple haploid or diploid samples.\nUse F3vanilla if your third population C is a single pseudo-haploid sample.\n\n\nHere is the output of this run (but note that a tab-separated version was also stored in fstats_working/outgroupf3_europe.tsv using the option -f):\n.-----------.-----.----------------------.-------.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |  a  |          b           |   c   | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=====:======================:=======:===:=========:================:====================:==================:====================:\n| F3        | Han | Chuvash              | Mbuti |   | 593124  | 5.3967e-2      | 5.3967e-2          | 5.0668e-4        | 106.51180329550319 |\n| F3        | Han | Albanian             | Mbuti |   | 593124  | 4.9972e-2      | 4.9973e-2          | 4.9520e-4        | 100.91326321202445 |\n| F3        | Han | Armenian             | Mbuti |   | 593124  | 4.9531e-2      | 4.9531e-2          | 4.7771e-4        | 103.68366652942314 |\n| F3        | Han | Bulgarian            | Mbuti |   | 593124  | 5.0103e-2      | 5.0103e-2          | 4.8624e-4        | 103.04188532686614 |\n| F3        | Han | Czech                | Mbuti |   | 593124  | 5.0536e-2      | 5.0536e-2          | 4.9261e-4        | 102.58792370749681 |\n| F3        | Han | Druze                | Mbuti |   | 593124  | 4.8564e-2      | 4.8564e-2          | 4.6788e-4        | 103.79674299622445 |\n| F3        | Han | English              | Mbuti |   | 593124  | 5.0280e-2      | 5.0281e-2          | 4.9183e-4        | 102.23198323949656 |\n| F3        | Han | Estonian             | Mbuti |   | 593124  | 5.1154e-2      | 5.1155e-2          | 5.0350e-4        | 101.59882496016485 |\n| F3        | Han | Finnish              | Mbuti |   | 593124  | 5.1784e-2      | 5.1784e-2          | 5.0603e-4        | 102.33488758899031 |\n| F3        | Han | French               | Mbuti |   | 593124  | 5.0207e-2      | 5.0208e-2          | 4.8552e-4        | 103.40976592749682 |\n| F3        | Han | Georgian             | Mbuti |   | 593124  | 4.9711e-2      | 4.9711e-2          | 4.8100e-4        | 103.34881140790415 |\n| F3        | Han | Greek                | Mbuti |   | 593124  | 4.9874e-2      | 4.9874e-2          | 4.8994e-4        | 101.79554640756365 |\n| F3        | Han | Hungarian            | Mbuti |   | 593124  | 5.0497e-2      | 5.0498e-2          | 4.9878e-4        | 101.24215699276706 |\n| F3        | Han | Icelandic            | Mbuti |   | 593124  | 5.0680e-2      | 5.0680e-2          | 4.9729e-4        | 101.91303336514295 |\n| F3        | Han | Italian_North        | Mbuti |   | 593124  | 4.9903e-2      | 4.9904e-2          | 4.8436e-4        | 103.03094306099203 |\n| F3        | Han | Italian_South        | Mbuti |   | 592980  | 4.9201e-2      | 4.9201e-2          | 5.1170e-4        | 96.15239597674244  |\n| F3        | Han | Lithuanian           | Mbuti |   | 593124  | 5.0896e-2      | 5.0896e-2          | 5.0638e-4        | 100.50984037418753 |\n| F3        | Han | Maltese              | Mbuti |   | 593124  | 4.8751e-2      | 4.8751e-2          | 4.7500e-4        | 102.63442479673623 |\n| F3        | Han | Mordovian            | Mbuti |   | 593124  | 5.1820e-2      | 5.1820e-2          | 4.8853e-4        | 106.07409963190884 |\n| F3        | Han | Norwegian            | Mbuti |   | 593124  | 5.0724e-2      | 5.0724e-2          | 4.9514e-4        | 102.4454387098217  |\n| F3        | Han | Orcadian             | Mbuti |   | 593124  | 5.0469e-2      | 5.0469e-2          | 4.9485e-4        | 101.98814656611475 |\n| F3        | Han | Russian              | Mbuti |   | 593124  | 5.1277e-2      | 5.1277e-2          | 4.8613e-4        | 105.48070801791317 |\n| F3        | Han | Sardinian            | Mbuti |   | 593124  | 4.9416e-2      | 4.9417e-2          | 4.8908e-4        | 101.04049389691913 |\n| F3        | Han | Scottish             | Mbuti |   | 593124  | 5.0635e-2      | 5.0635e-2          | 5.0565e-4        | 100.13962104744425 |\n| F3        | Han | Sicilian             | Mbuti |   | 593124  | 4.9194e-2      | 4.9194e-2          | 4.8157e-4        | 102.15353663091187 |\n| F3        | Han | Spanish_North        | Mbuti |   | 593124  | 5.0032e-2      | 5.0032e-2          | 4.9377e-4        | 101.32594226555439 |\n| F3        | Han | Spanish              | Mbuti |   | 593124  | 4.9693e-2      | 4.9693e-2          | 4.8551e-4        | 102.35200847948641 |\n| F3        | Han | Ukrainian            | Mbuti |   | 593124  | 5.0731e-2      | 5.0731e-2          | 4.9506e-4        | 102.47529111692852 |\n| F3        | Han | Finland_Levanluhta   | Mbuti |   | 303033  | 5.4488e-2      | 5.4488e-2          | 5.7681e-4        | 94.46487653920919  |\n| F3        | Han | Russia_Bolshoy       | Mbuti |   | 542789  | 5.7273e-2      | 5.7273e-2          | 5.2875e-4        | 108.31739594898687 |\n| F3        | Han | Russia_Chalmny_Varre | Mbuti |   | 428215  | 5.4000e-2      | 5.4000e-2          | 5.6936e-4        | 94.84371082564112  |\n| F3        | Han | Saami.DG             | Mbuti |   | 585193  | 5.4727e-2      | 5.4728e-2          | 5.5546e-4        | 98.5265149143263   |\n'-----------'-----'----------------------'-------'---'---------'----------------'--------------------'------------------'--------------------'\nNow it’s time to plot these results using R. Let’s first read in the table:\n\nd &lt;- read.csv(\"fstats_working/outgroupf3_europe.tsv\", sep = \"\\t\")\n\nWe can check that it worked:\n\nhead(d)\n\n  Statistic   a         b     c  d NrSites Estimate_Total Estimate_Jackknife\n1        F3 Han   Chuvash Mbuti NA  593124       0.053967           0.053967\n2        F3 Han  Albanian Mbuti NA  593124       0.049972           0.049973\n3        F3 Han  Armenian Mbuti NA  593124       0.049531           0.049531\n4        F3 Han Bulgarian Mbuti NA  593124       0.050103           0.050103\n5        F3 Han     Czech Mbuti NA  593124       0.050536           0.050536\n6        F3 Han     Druze Mbuti NA  593124       0.048564           0.048564\n  StdErr_Jackknife Z_score_Jackknife\n1       0.00050668          106.5118\n2       0.00049520          100.9133\n3       0.00047771          103.6837\n4       0.00048624          103.0419\n5       0.00049261          102.5879\n6       0.00046788          103.7967\n\n\nNice, now on to plotting (here I’m using Base R for zero-dependency pain, you’re welcome!):\n\norder &lt;- order(d$Estimate_Total) # order the estimates for visual effect\nx &lt;- d$Estimate_Jackknife[order]\nxErr &lt;- d$StdErr_Jackknife[order]\ny &lt;- seq_along(d$b)\nplot(x, y, xlab = \"Z Score\", ylab = NA, yaxt = \"n\", # plot no y-axis ticks\n     xlim = c(0.048,0.06),\n     main = \"F3(Han, X; Mbuti)\")\n# plot the labels\ntext(x + 0.001, y, labels = d$b, adj=0)\n# plot the error bars\narrows(x - xErr, y, x + xErr, y, length=0.05, angle=90, code=3)\n\n\n\n\n\n\n\n\nAs expected, the ancient samples and modern Saami are the ones with the highest allele sharing with present-day East Asians (as represented by Han) compared to many other Europeans.\n\n\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic, Udo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft Sequence of the Neandertal Genome.” Science 328 (5979): 710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela, Anna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018. “Ancient Fennoscandian Genomes Reveal Origin and Spread of Siberian Ancestry in Europe.” Nature Communications 9 (1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015. “Evaluating the Use of ABBA-BABA Statistics to Locate Introgressed Loci.” Molecular Biology and Evolution 32 (1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and F-Statistics.” Genetics 202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913."
  },
  {
    "objectID": "authentiCT.html#contamination-estimation-methods",
    "href": "authentiCT.html#contamination-estimation-methods",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "4.1 Contamination Estimation Methods",
    "text": "4.1 Contamination Estimation Methods\nA number of methods are available to estimate contamination which use a variety of signals and are appropriate for different types of data. Here, we will focus on the case of ancient human samples with present-day human DNA contamination.\nThree main signals are informative about the presence of contamination in aDNA datasets:\n1. Differences in the DNA Sequence: Sites that differ between the genome of interest and likely contaminants can be identified when their genome sequences are known in advance. Contamination can be estimated by measuring the proportion of sequences that present differences at specific sites (Box A).\n\nExample tool: contamMix\n2. Deviation from the expected ploidy: Contamination can cause a sample to show unusual patterns of ploidy. For instance, heterozygous sites on the X or Y-chromosomes in males, or Y-chromosome sequences in females, are signs of contamination (Box B). \nExample tools: X/Autosome coverage ratio, ANGSD, Schmutzi\n3. Ancient DNA degradation patterns: The degradation of DNA leaves characteristic patterns that can be used to distinguish aDNA sequences from those from present-day DNA contamination. The most common damage in aDNA originates from cytosine-deamination (Box C).\nExample tool: AuthentiCT\nThe figure below summarizes the classification of signals used to estimate contamination, as presented by Peyrégné (2020)\n\n\n\nBox A shows the genealogical relationship of present-day and ancient individuals, including two derived variants that are informative for either group. The presence of a red variant indicates a contaminant sequence, whereas a blue variant indicates endogenous sequences. Box B shows the expected ploidy for the autosomes (A), the X- and Y-chromosomes (X, Y), and the mitochondrial genome (MT). Deviations from these expectations indicate contamination from the opposite sex. The illustration below shows sequences aligned to a reference genome. These sequences carry two different alleles represented by dots. However, one allele (white) is rare compared to the other allele (blue). This observation is not compatible with the 50:50 ratio expected for a heterozygous site; therefore the discordant allele may originate from contamination. Box C illustrates aDNA damage. Left: aDNA fragments often contain U caused by aDNA damage whereas U are typically absent from present-day DNA. Right: When no repair enzymes are used, U will be misread as T and their presence will result in high rates of C-to-T substitutions that occur primarily at the ends of sequences. Note that this signal depends on the library preparation protocol, and that high rates of G-to-A toward the 3ʹ-end. Neither pattern is expected for present-day DNA sequences .\n\n\n\n4.1.1 aDNA properties: Quick recap\nAncient DNA is typically fragmented into pieces shorter than 100 bp and exhibits miscoding base modifications that accumulate over time.\nThe most common miscoding lesions observed in aDNA are the results of cytosine deamination that converts cytosine (C) into uracil (U), which is then misread as thymine (T), or 5- methylcytosine into thymine. This leads to C-to-T substitutions in the sequence data.\n\n\n\n\n\n\n\nThe deamination rate is higher for single-stranded DNA than for double-stranded DNA (Lindahl and Nyberg 1974). The prevalence of deamination-induced C-T substitutions increases with the age of the sample, and it is climate dependent (Kistler et al. 2017; Sawyer et al. 2012) .\n💡 To estimate present-day DNA contamination, these properties need to be formalised in a model of aDNA damage..\n\n\n4.1.2 aDNA Common Damage Models\n1. Conditional substitution model (based on last positions)\nAssumes independence between C-to-T substitutions at both ends (no correlation).\nCytosine deamination is used to filter out present-day human contaminant DNA, as this type of deamination occurs less frequently in such DNA. To estimate contamination, patterns of ‘regular’ substitution are compared with ‘conditional’ substitution patterns. The frequency of conditional substitutions is calculated by identifying fragments that show a C-T change at either the 5’ or 3’ end and then determining the C-T substitution rate at the opposite end. Frequencies derived from conditional substitution are presumed to be higher than those from ‘regular’ substitution. This method is used as a ‘conservative’ proxy to estimate the expected deamination in endogenous DNA. It was introduced by Meyer et al. (2016)\nLet’s say we have sample X with damage rate of 30% at the 5’ end and 25% on the 3’ end, as illustrated below:\n\n\n\n\n\nTo calculate the conditional substitution rate at the 5’ end, we start by selecting only those reads that have a deamination signal at the 3’ end. hence maximizing it to a 100% rate. Similarly, to find the substitution rate at the 3’ end, we pick out reads that show a deamination signal at the 5’ end, aiming for the same 100% rate. The process is depicted in the figure below.\n\nThe table below summarizes the output of the calculation:\n\n\n\n\nregular substitution\nconditional substitution\n\n\n\n\n5’ end\n30\n30\n\n\n3’ end\n25\n28\n\n\n\nIf conditional substitution rate is larger than regular substitution rate at both ends, this could indicate presence of contamination. This is due to the background of present-day contamination that dilutes the deamination signal.\nExample\nIn Meyer et al. 2016, nuclear DNA sequences from femur XIII, a sample of the Sima de los Huesos (SH femur XIII) Middle Pleistocene hominins, showed regular C to T substitution frequencies of 12% and 17% at the 5’ and 3’ ends, respectively. When conditioned on C to T substitutions at the opposite ends of fragments, these frequencies dramatically increased to 55% and 62%, indicating the specimen contains a mixture of highly deaminated endogenous nuclear DNA and less deaminated human contamination. The output is outlined in the table below:\n\nA custom script to calculate the conditional substitution rate can be requested from Matthias Meyer.\n\n\n\nregular substitution\nconditional substitution\n\n\n\n\n5’ end\n17\n62\n\n\n3’ end\n12\n55\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe conditional substitution rate does not provide a robust estimate; instead, it offers an indication of contamination. Therefore, it is considered as qualitative rather than quantitative method.\n\n\n2. Non-conditional substitution model (based on pattern)\nUnlike the conditional substitution method, which looks at two point estimates, the non-conditional substitution model—on which the AuthentiCT tool is based—relies on a complex model that learns the patterns of post-mortem damage.\nAuthentiCT follows that model ⤵️"
  },
  {
    "objectID": "authentiCT.html#authentict-tool",
    "href": "authentiCT.html#authentict-tool",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "4.2 AuthentiCT tool",
    "text": "4.2 AuthentiCT tool\nAuthentiCT is based on model that learns the patterns of post-mortem damage. It models all C-T substitutions, irrespective of their position in the sequence, and accounts for clusters of C-T substitutions in the internal parts of sequences.\n\n4.2.1 Theory\nHow it works?\n- Modeling C-to-T Substitutions: AuthentiCT models all C-T substitutions across the DNA sequence. It does not limit this analysis to substitutions near the ends of DNA sequences, where such changes are most expected, but also considers them throughout the sequence.\n- Accounting for Clusters: While C-T substitutions occur predominantly at the ends of DNA fragments, they are also found in the internal parts. These internal clusters are not independent from each other.\n\n\n\nExcluding the first and last five bases to mask potential overhangs, C-to-Ts are found to particularly common in adjacent positions in many samples, with a significant deviation from the geometric distribution expected from independent events [Peyrégne and Peter 2020]\n\n\nAuthentiCT uses a a hidden Markov model (HMM)—a statistical model for predicting a sequence of unseen state changes based on observed sequences—to jointly model all C-T substitutions.\n- Observation: This refers to matches and mismatches to the reference, including deaminations, errors, and polymorphisms.\n- Unobserved states: These are the DNA conditions not directly observed, such as single-stranded areas (including 3’ or 5’ overhangs, haplotypes) and double-stranded regions.\n\n\n\n\n\nAfter evaluating each position within the DNA sequence, the model identifies three distinct single-stranded (ss) regions: inside the molecule, at the 5’ end, or at the 3’ end. It then calculates the probability for each model (contaminant vs. ancient) at the sequence’s end.\nThe model distinguishes between single-stranded and double-stranded parts of the DNA molecule using four hidden states to represent either double-stranded (ds) or single-stranded stretches. By compiling these probabilities across all sequences, AuthentiCT efficiently estimates the proportion of contamination.\n\n\n\nStates are depicted by nodes and transitions by edges. Each state emits a match to the reference M (blue) or a mismatch, which can either be compatible with cytosine deamination, D (red), or an error (or polymorphism), E (yellow). Single-stranded states (5’o, 3’o and ss) and the double-stranded state (ds) are in light and dark green, respectively. b The posterior probability for each state is shown with different colours [Peyrégne & Peter 2020]\n\n\n\n\n4.2.2 Applicability\n\n\n\n\n\n\nTip with Title\n\n\n\n\nAuthentiCT is applicable to any species, if suitable reference genome is available for alignment.\nLimited to single-stranded libraries\nPerforms well for datasets of 10,000 or more sequences (less reliable for &lt;1000) - bias decreases with longer sequences or higher GC contents.\nNot applicable to libraries generated after treatments that alter deamination patterns, e.g., UDG.\nIt is not possible to run it with PMD-filtered data, since such data lacks the non-deaminated reads.\n\n\n\n\n\n4.2.3 Method parameters\n\nAuthentiCT overestimates contamination for low contamination rates.\nConsistency between Shotgun (SG) vs capture (1240k) data.\n\n\n\nAssumes the absence of significant levels of deamination in the contaminating DNA, which can lead to underestimating the proportion of contamination.\nDNA fragments with different rates of damaged bases (differences in preservation, different treatments), may lead to an overestimate of present-day DNA contamination.\n\n\n\n4.2.4 Running & Output\nTo install AuthentiCT, follow the instructions available at AuthentiCT GitHub Repository.\n\nPython Version: Requires Python 3.6 or higher.\n\nEnsure the following dependencies are installed:\n\nnumpy (version 1.17.3)\nnumdifftools (version 0.9.39)\npandas (version 0.25.2)\nscipy (version 1.3.1)\n\nMain commands:\n\ndeam2cont: Estimates the level of contamination from deamination patterns.\ndeamination: Prints C-to-T substitution frequencies.\n\nScript\n#!/usr/bin/env bash\n\nBAM=LGJ001_ss.A0101_rmdup #file name (deduplicated bam)\n\nsamtools view $BAM.bam | AuthentiCT deam2cont -o $BAM.deam2cont.out -m 25 -b 30 -\n\n# -m  mapping quality cutoff\n# -b  base quality cutoff\nor for multiple bam files\n\n# Directory containing BAM files\nBAM_DIR=\"/path/to/your/deduplicated_bam/directory\"\n\n# Iterate over all BAM files in the directory\nfor BAM_FILE in \"$BAM_DIR\"/*.bam; do\n# Extract the file name without the extension\n    BAM_BASE=$(basename \"${BAM_FILE%.bam}\")\n\n# Run the samtools view and AuthentiCT commands for each BAM file\n  samtools view \"$BAM_FILE\" | AuthentiCT deam2cont -o \"$BAM_BASE.deam2cont.out\" -m 25 -b 30 -\ndone\n\n# -m    mapping quality cutoff\n# -b    base quality cutoff\nOutput example\n|       | Parameter    | Std.Err      | Output Explanation                                                                              |\n|------------------|------------------|------------------|-------------------|\n| e     | 0.003659     | 0.000079     | Error rate: we expect around 1 or 2 mismatches every 1000                                       |\n| rss   | 0.663170     | 0.016033     | Rate of C-to-T substitutions in single-stranded regions \\&gt; damage rate                          |\n| lo    | 0.732971     | 0.003014     | Parameter of the geometric distribution modeling the length of single-stranded overhang         |\n| lss   | 0.727519     | 0.036484     | Parameter of the geometric distribution modeling the length of internal single-stranded regions |\n| lds   | 0.001000     | 0.000091     | Parameter of the geometric distribution modeling the length of double-stranded regions          |\n| contm | 0.098113    | 0.011459 | Contamination estimate (rate from 0 to 1)                                                   |\n| o     | 0.626460     | 0.018110     | Frequency of 5' single-stranded overhangs                                                       |\n| o2    | 0.657778     | 0.020543     | o2 is proportional to the frequency of 3' single-stranded overhangs                             |"
  },
  {
    "objectID": "authentiCT.html#practical-examples",
    "href": "authentiCT.html#practical-examples",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "4.3 Practical examples",
    "text": "4.3 Practical examples\n\n4.3.1 Consistency between Shotgun (SG) vs capture (1240k) data\nAim: We evaluated the consistency of AuthentiCT estimates between shotgun and 1240k-captured data derived from the same samples.\nDataset: consists of 1240k-captured sequences from individuals dating back to 3000 BP, showing low levels of contamination.\nOutcome: We did not observe high discrepancies when comparing samples with high number of input reads and C-T damage (&gt;30%). However, we observed higher SE for samples with low number of input reads (less than 10.000), mainly for SG data. The table below outlines the coverage and the C-T substitution rates for each samples.\n\n\n\nPlot comparing AuthentiCT output for SG and capture (1240k) data, showing 2x standard error. We did not observe high discrepancies when comparing samples with high number of input reads and C-T damage (&gt;30%). However, we observed higher SE for samples with low number of input reads (less than 10.000), mainly for SG data. Data used are shown in table below.\n\n\n\n| Seq ID | SG_Nr. Mapped Reads Passed Post-Filter | SG_Endogenous DNA Post (%) | SG_5 Prime C&gt;T 1st base | TF_Nr. Mapped Reads Passed Post-Filter | TF_Endogenous DNA Post (%) | TF_5 Prime C&gt;T 1st base |\n|--------|-----------------------------------------|----------------------------|-------------------------|-----------------------------------------|----------------------------|-------------------------|\n| 002.A  | 1,173,469                               | 21.38                       | 31.60%                  | 19,718,426                              | 65.53                      | 32.80%                  |\n| 003.A  | 85,840                                  | 1.58                        | 40.10%                  | 13,387,538                              | 40.04                      | 40.90%                  |\n| 005.B  | 67,131                                  | 1.51                        | 56.40%                  | 16,556,830                              | 3.52                       | 55.40%                  |\n| 006.B  | 1,896                                   | 0.05                        | 47.80%                  | 33,820                                  | 0.09                       | 51.00%                  |\n| 007.B  | 916,024                                 | 20.49                       | 41.40%                  | 13,232,270                              | 36.93                      | 39.90%                  |\n| 008.B  | 5,595                                   | 0.13                        | 57.00%                  | 74,134                                  | 0.24                       | 59.20%                  |\n| 010.A  | 36,098                                  | 1.06                        | 43.60%                  | 12,425,284                              | 34.83                      | 45.30%                  |\n| 012.A  | 522,067                                 | 10.01                       | 42.00%                  | 17,379,143                              | 52.79                      | 41.90%                  |\n| A001.A | 2,626                                   | 0.05                        | 39.40%                  | 43,414                                  | 0.11                       | 41.00%                  |\n| A005.B | 72,628                                  | 1.69                        | 38.50%                  | 3,624,986                               | 8.99                       | 38.50%                  |\n| A008.C | 80,062                                  | 1.74                        | 36.40%                  | 2,958,958                               | 7.87                       | 35.40%                  |\n| A010.A | 12,212                                  | 0.28                        | 52.00%                  | 198,887                                 | 0.63                       | 50.70%                  |\n| A015.A | 3,440                                   | 0.06                        | 38.00%                  | 64,418                                  | 0.16                       | 45.60%                  |\n\n\n4.3.2 Impact of sequence count used on AuthentiCT estimates\nAim: We evaluated whether lowering the sequence count for the deamination model in AuthentiCT’s -s option from the default 100,000 to 10,000 sequences—which speeds up the process—alters contamination estimates.\nDataset: Our analysis used 1240k-captured sequences from individuals dating back to the first millennium BC, varying in contamination levels.\nOutcome: Our results indicate that a reduced sequence count speeds up analysis but may broaden standard errors and slightly alter contamination estimates.\n\n\n\n\n\n\n\n4.3.3 Impact of sequencing setup used on AuthentiCT estimates\nAim and dataset: We explored AuthentiCT estimates in samples that were single-end (SE) sequenced and paired-end (PE) sequenced. The mean read length for all the samples was between 45 to 50 bp (insert size), and the sequencing read length was 75 bp in both the SE and PE setups. The adapters were trimmed using AdpaterRemoval tool.\nOutcome: The results indicate that AuthentiCT estimate ovaerall does not change with changing the sequencing setup.\n\n\n\n\n\n💡 Terminal end damage discrepancies in single-stranded libraries\nDuring the preparation of single-stranded libraries, an adapter is ligated to the 3’ ends of the molecules. This step tends to introduce a small to moderate bias against the ligation of uracils, leading to a common observation: the terminal 3’ C-to-T substitution rates are typically 10-20% lower compared to those at the 5’ terminal ends.This bias is amplified in the presence of inhibitory substances in the DNA extracts, or if the library prep is saturated with large amounts of input DNA.\n\n\n\n\n\n\nNote\n\n\n\nAuthentiCT compensates for these variances by accounting for the differences in the length of the single-stranded end and the frequency of this occurrence.\n\n\n\n\n\n\nKistler, Logan, Roselyn Ware, Oliver Smith, Matthew Collins, and Robin G. Allaby. 2017. “A New Model for Ancient DNA Decay Based on Paleogenomic Meta-Analysis.” Nucleic Acids Research 45 (11): 6310–20. https://doi.org/10.1093/nar/gkx361.\n\n\nLindahl, Tomas, and Barbro Nyberg. 1974. “Heat-Induced Deamination of Cytosine Residues in Deoxyribonucleic Acid.” Biochemistry 13 (16): 3405–10. https://doi.org/10.1021/bi00713a035.\n\n\nMeyer, Matthias, Juan-Luis Arsuaga, Cesare de Filippo, Sarah Nagel, Ayinuer Aximu-Petri, Birgit Nickel, Ignacio Martínez, et al. 2016. “Nuclear DNA Sequences from the Middle Pleistocene Sima de Los Huesos Hominins.” Nature 531 (7595): 504–7. https://doi.org/10.1038/nature17405.\n\n\nPeyrégné, Someone. 2020. “Title of the Article.” Journal Name 10: 100–110. https://doi.org/10.1000/j.journal.2020.01.001.\n\n\nSawyer, Susanna, Johannes Krause, Katerina Guschanski, Vincent Savolainen, and Svante Pääbo. 2012. “Temporal Patterns of Nucleotide Misincorporations and DNA Fragmentation in Ancient DNA.” Edited by Carles Lalueza-Fox. PLoS ONE 7 (3): e34131. https://doi.org/10.1371/journal.pone.0034131."
  },
  {
    "objectID": "eager.html#why-do-we-need-nf-coreeager",
    "href": "eager.html#why-do-we-need-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.1 Why do we need nf-core/eager?",
    "text": "5.1 Why do we need nf-core/eager?\n\nCompared to other Next-Generation Sequencing data, the chemical structure and increased risk of present-day contamination require methods specialized for ancient DNA in both the wet and the dry lab.\nEnsuring the authenticity of ancient genomic data is one of the main focuses of bioinformatic tools developed for the study of ancient DNA and underlines the necessity of reproducibility of results.\nWith an increasing number of laboratories contributing to the field, the available computational resources and previous bioinformatic experience varies greatly. To increase accessibility, newly developed tools should be adaptable to different environments, efficient, consistently maintained and well-documented."
  },
  {
    "objectID": "eager.html#what-can-nf-coreeager-do",
    "href": "eager.html#what-can-nf-coreeager-do",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.2 What can nf-core/eager do?",
    "text": "5.2 What can nf-core/eager do?\nnf-core/eager streamlines the initial steps of ancient DNA analysis from FASTQ files after sequencing to variant calling (Fellows Yates et al. 2021).\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2/fastp (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln/BWA mem/CircularMapper/Bowtie2 (alignment)\nSAMtools (mapping quality filtering)\nPicard MarkDuplicates/DeDup (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/BEDtools/Sex.DetERRmine/EndorSpy/MtNucRatio (mapping statistics)\n\naDNA evaluation:\n\nDamageProfiler/mapDamage2 (damage assessment)\nPMDtools (aDNA read selection)\nmapDamage2/Bamutils (damage removal)\nANGSD (human contamination estimation)\nBBduk/HOPS/Kraken & Kraken Parse/MALT & MaltExtract (metagenomic screening)\n\nVariant calling: GATK UnifiedGenotyper & HaplotypeCaller/sequenceTools pileupCaller/VCF2Genome/MultiVCFAnalyzer/freebayes/ANGSD\nReport generation: MultiQC (summarize all generated statistics)"
  },
  {
    "objectID": "eager.html#how-do-i-use-nf-coreeager",
    "href": "eager.html#how-do-i-use-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.3 How do I use nf-core/eager?",
    "text": "5.3 How do I use nf-core/eager?\n\n5.3.1 Installation\nYou need:\n\na Unix machine (HPC-cluster or a computer running Linux or MacOS)\nan installation of docker or apptainer (formerly known as singularity) or conda, java and nextflow (e.g. via conda)\ninternet connection\n\nDownload the latest version of nf-core/eager\nnextflow pull nf-core/eager \n# for a specific version\nnextflow pull nf-core/eager -r 2.5.0\nRun a test specifying your choice of conda, docker or singularity\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,docker\nTo optimize the use of available clusters, queues and resources, check if a Nextflow pipeline configuration is already available for your institution or computing environment. If not, prepare a custom profile tailored to your computational resources and setup. These are then added as a profile.\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,eva #for MPI-EVA\n\n\n5.3.2 Input preparation\nYou can run nf-core/eager by providing either a path to fastq or bam files or a path to a tab-separated table of input data to --input. For a large number of samples and convenience, a tsv is usually the preferred option. Using a tsv input also allows for merging of different files (e.g. different libraries, different UDG treatments, etc.) at different stages of the pipeline.\n\n\n\nPipeline stages and merging steps performed for a single sample with different libraries and different UDG treatments.\n\n\nA tsv input file contains the following columns, detailing the name of the sample, library, sequencing lane, colour chemistry depending on the sequencer used, target organism, library strandedness, UDG treatment, path to fastq with forward reads (SE and PE), path to reverse reads (only PE), path to bam (optional). nf-core/eager will treat the data according to the provided information, e.g. only trim UDG half data and genotype single-stranded libraries using single-stranded mode.\nSample_Name Library_ID Lane Colour_Chemistry SeqType Organism Strandedness UDG_Treatment R1                                                                                                                                  R2                                                                                                                                  BAM\nJK2782      JK2782     1    4                PE      Mammoth  single       half          https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R1_001.fastq.gz.tengrand.fq.gz https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R2_001.fastq.gz.tengrand.fq.gz NA\nJK2802      JK2802     2    2                SE      Mammoth  double       full          NA                                                                                                                                  NA                                                                                                                                  https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2802_AGAATAACCTACCA_L008_R1_001.fastq.gz.tengrand.bam\nCollecting and double-checking this information is time consuming, but crucial!\nIf you realize you have different libraries from same individual, you should enter the same Sample_Name for all respective libraries. nf-core/eager will then produce all steps for the independent libraries (e.g. endogenous DNA, sequencing quality control, contamination estimation, etc.), but merge the deduplicated bam files before genotyping, genetic sex estimation and coverage calculation. To avoid re-mapping the whole dataset and conserve computing resources, also consider providing the mapped bam files to nf-core/eager directly.\n\n\n\n\n\n\nTip\n\n\n\nAt DAG, we can take advantage of all the information entered in Pandora to produce a eager-ready tsv with pandora2eager.\n\n\n\n\n5.3.3 Parameter customization\nBy default nf-core/eager runs the following, when you only provide input data and a reference genome:\nnextflow run nf-core/eager --input &lt;INPUT&gt;.tsv --fasta '&lt;REFERENCE&gt;.fasta' -profile eva\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2 (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln (alignment)\nPicard MarkDuplicates (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/EndorSpy (mapping statistics)\n\naDNA evaluation: DamageProfiler (damage assessment)\nReport generation: MultiQC (summarize all generated statistics)\n\nThe most direct way to add analysis steps (e.g. turn on genotyping) or change settings (e.g. shorter read length cut-off) is to add more parameters to the command line, in this case --run_genotyping --genotyping_tool pileupcaller and --clip_readlength 25, respectively. However, this gets cumbersome for the rather extensive workflows we usually employ for human aDNA analysis, including read trimming based on UDG treatment, genetic sex estimation, human nuclear contamination estimation, mitochondrial to nuclear ratio estimation and genotyping.\nBut the power of nf-core/eager lies in its adaptability to your specific analysis needs and the possibility to ‘remember’ your favorite settings with a personal configuration file. This separate file can contain all parameters for your required tools, as well as custom computational resource requests. For 1240K capture data, a profile mapping to the hs37d5 reference genome with genotyping could look like this:\nprofiles{\n  TF_hs37 { #name of the profile\n    params {\n        config_profile_description = \"human 1240K data hs37d5 + genotyping\"\n        config_profile_contact = \"Selina Carlhoff (@scarlhoff)\"\n        email = \"selina_carlhoff@eva.mpg.de\"\n        snpcapture_bed = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        fasta = \"/PATH/hs37d5/hs37d5.fa\"\n        fasta_index = \"/PATH/hs37d5/hs37d5.fa.fai\"\n        bwa_index = \"/PATH/hs37d5/\"\n        skip_preseq = true\n        clip_readlength = 30\n        preserve5p = true\n        bwaalnn = 0.01\n        bwaalnl = 16500\n        run_bam_filtering = true\n        bam_mapping_quality_threshold = 30\n        bam_filter_minreadlength = 30\n        bam_unmapped_type = \"discard\"\n        run_trim_bam = true\n        bamutils_clip_double_stranded_half_udg_left = 2\n        bamutils_clip_double_stranded_half_udg_right = 2\n        bamutils_clip_single_stranded_none_udg_left = 0\n        bamutils_clip_single_stranded_none_udg_right = 0\n        run_genotyping = true\n        genotyping_tool = \"pileupcaller\"\n        genotyping_source = \"trimmed\"\n        pileupcaller_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        pileupcaller_snpfile = \"/PATH/1240K.snp\"\n        run_mtnucratio = true\n        mtnucratio_header = \"MT\"\n        run_sexdeterrmine = true\n        sexdeterrmine_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        run_nuclear_contamination = true\n        contamination_chrom_name = \"X\"\n    }\n    process {\n    maxRetries = 2\n        withName:bwa {\n        time = { task.attempt == 3 ? 1440.h : task.attempt == 2 ? 72.h : 48.h }\n        }\n        withName:markduplicates {\n        memory = { task.attempt == 3 ? 16.GB : task.attempt == 2 ? 8.GB : 4.GB }\n        }\n        withName: mtnucratio {\n            memory = '10.G'\n            time = '24.h'\n        }\n    }\n  }\n}\nThe configuration file is then provided to nf-core/eager via the -profile and -c flag.\nnextflow run nf-core/eager -–input &lt;INPUT&gt;.tsv -profile TF_hs37,eva,archgen -c /&lt;PATH&gt;/eager2.config\nFull documentation of all parameters is available on the nf-core/eager website.\n\n\n\n\n\n\nTip\n\n\n\nThe standardised parameters for the DAG automated pipeline can be found at /mnt/archgen/Autorun_eager/conf/Autorun.config.\n\n\n\n\n5.3.4 Run submission\nOnce all input files and parameters are prepared, you are ready for submission. To make sure that the workflow continues running when you disconnect from the cluster or shut down your computer, nf-core/eager should be run in a screen session.\n# create a screen session\nscreen -R eager\n# submit nf-core/eager run\nnextflow run nf-core/eager –input &lt;INPUT&gt;.tsv -profile &lt;YOUR_PROFILE&gt; -c /&lt;PATH&gt;/&lt;YOUR_CONFIG&gt;.config\n# disconnect from screen session by pressing Ctrl+A+D\n# reconnect to screen session\nscreen -r eager\n# end screen session after successful pipeline execution\nexit\nAfter submitting a command specifying all the parameters you would like to use, Nextflow generates the corresponding shell scripts and submits each job to your scheduler according to your requested computational resources. You can track the execution status live in the terminal or on Nextflow Tower. For use with tower, you should assign the run an identifiable name with -name and activate tracking using -with-tower.\n\n\n5.3.5 Output\nDuring the progression of the run, the results of each pipeline steps are collected in separate output directories. These contain the raw outputs of each tool, including any generated files (e.g. deduplicated bam files or genotypes).\n&lt;RUNNAME&gt;/\n- results/\n  - adapterremoval/\n  - damageprofiler/\n  - deduplication/\n  - documentation/\n  - endorspy/\n  - fastqc/\n  - genotyping/\n  - lanemerging/\n  - mapping/\n  - merged_bams/\n  - multiqc/\n  - nuclear_contamination/\n  - pipeline_info/\n  - qualimap/\n  - reference_genome/\n  - samtools/\n  - sex_determination/\n  - trimmed_bam/\n- work/\nBut as a first overview, we want to look at the summary of all statistics aggregated in multiqc/multiqc_report.html.\n\n\n\nScreenshot of the top section of a MultiQC report\n\n\nThis table collects the output from all tools, so you can get an overview of sequenced reads per sequencing run, endogenous DNA per library, covered SNPs per sample and much more. You can also inspect and export crucial plots, such as read length distribution and damage profile. The end of the report also contains a list of all software versions and an overview of which profiles were used.\n\n\n\nAncient DNA damage plot as generated by MultiQC\n\n\n\n\n5.3.6 Trouble shooting\nA common issue with nf-core/eager, especially when used in combination with the SGE scheduler, are memory issues with java-driven tools, e.g. MarkDuplicates. Sometimes the pipeline does not catch cases properly, where the allocated memory is exceeded, and the job keeps running instead of being re-submitted with larger memory allocation. Therefore, if you notice jobs running much longer than expected, it is worth checking the work/ directory, where all information about each submitted job is recorded. Each job is assigned a randomly generated name using numbers and letter which you can identify from the log printed in to your screen, the &lt;RUNNAME&gt;/.nextflow.log or Nextflow tower. Each work directory contains files tracing the execution of the job.\n&lt;RUNNAME&gt;/\n- work/\n  - &lt;WORKDIRECTORY&gt;/\n    - .command.sh # exact command run for this tool\n    - .command.run # exact command submitted to the scheduler\n    - .command.log # any messages during the execution\n    - .command.err # any error messages\n    - .command.out # any output messages\n    - .command.trace # assigned computational resources\nIf you spot java.lang.OutOfMemoryError: unable to create new native thread in .command.log or command.err, you can delete the individual job from the scheduling queue. It will be re-submitted automatically with larger memory allocation.\nIf you’ve had an issue with a run or want to restart the pipeline, you can do so using -resume. Nextflow will used cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. You can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names."
  },
  {
    "objectID": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "href": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.4 How do I report the usage of nf-core/eager?",
    "text": "5.4 How do I report the usage of nf-core/eager?\nIf you use nf-core/eager for your analysis, please cite Fellows Yates et al. (2021) and the release of nf-core/eager on zenodo, as well as the nf-core publication (Ewels et al. 2020). As nf-core/eager is only the pipeline connecting multiple tools, please also cite the version of each used tool, the respective individual publications and add the full command for easy reproducibility.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\nnextflow run nf-core/eager\n        -profile eva,archgen\n        -r 2.4.0\n        --input &lt;INPUT&gt;.tsv\n        --min_adap_overlap 1\n        --clip_readlength 30\n        --clip_min_read_quality 20\n        --preserve5p\n        --mapper bwaaln\n        --bwaalnnn 0.01\n        --bwaalno 2\n        --run_bam_filtering true\n        --bam_mapping_quality_threshold 30\n        --bam_filter_minreadlength 30\n        --bam_unmapped_type discard\n        --dedupper markduplicates\n        --damageprofiler_length 100\n        --damageprofiler_threshold 15\n        --damageprofiler_yaxis 0.3"
  },
  {
    "objectID": "eager.html#how-do-i-update-nf-coreeager",
    "href": "eager.html#how-do-i-update-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.5 How do I update nf-core/eager?",
    "text": "5.5 How do I update nf-core/eager?\nnextflow pull nf-core/eager\n#or for a specific version\nnextflow pull nf-core/eager -r 2.5.0"
  },
  {
    "objectID": "eager.html#whats-next-for-nf-coreeager",
    "href": "eager.html#whats-next-for-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.6 What’s next for nf-core/eager?",
    "text": "5.6 What’s next for nf-core/eager?\nWhile nf-core/eager 2.5.0 has only been released recently, behind the scenes the development team has been very busy re-writing nf-core/eager to be more efficient and include even more functionality. So look out for nf-core/eager 3.0 release sometime soon!\n\n\n\nOverview of the development status of nf-core/eager 3.0 as of October 2023, new functionality marked in purple."
  },
  {
    "objectID": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "href": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.7 How can I get help with problems or questions?",
    "text": "5.7 How can I get help with problems or questions?\nCheck the website for in-depth documentation of nf-core/eager.\n Raise your issue on GitHub.\nJoin the #eager channel on the nf-core Slack."
  },
  {
    "objectID": "mobest.html#overview",
    "href": "mobest.html#overview",
    "title": "6  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nmobest is an R package providing types and functions for spatiotemporal interpolation of human genetic ancestry components, probabilistic similarity search and the calculation of a derived measure of ancestry relocation and mobility. The workflow in mobest version 1.0.0 was specifically developed for Schmid and Schiffels (2023).\nmobest assumes you have a set of genetic samples with spatial (two coordinates in a projected reference system) and temporal positions (years BC/AD) for which you calculated a derived, numeric measure of genetic ancestry (e.g. coordinates in a PCA or MDS space).\n\n\n\nSpatiotemporal distribution of the archaeogenetic samples in Schmid and Schiffels (2023).\n\n\n\n\n\nDistribution of the archaeogenetic samples in a genetic space constructed via multidimensional scaling.\n\n\nThe package then provides functions to perform spatiotemporal interpolation using Gaussian process regression (GPR, “kriging”) with the laGP R package (Gramacy (2016)) to reconstruct an ancestry field based on the ancestry measure you provided.\n\nGramacy, Robert B. 2016. “laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R.” Journal of Statistical Software 72 (1): 1–46. https://doi.org/10.18637/jss.v072.i01.\n\n\n\n\n\n\nSparse point cloud in space and time.\n\n\n\n\n\n\n\nDense cloud after interpolation.\n\n\n\n\n\n\n\n\n\nOne timeslice of the interpolated field.\n\n\n\n\n\n\n\nInterpolated C1 mean and error for this timeslice.\n\n\n\n\n\nmobest finally allows to derive a similarity likelihood for samples of interest within the interpolated field, which – under certain circumstances – can be interpreted as an origin probability.\n\n\n\n\n\n\n“Cutting the field”, so measuring the likelihoods for a specific ancestry profile (C1 value) for every grid point in space.\n\n\n\n\n\n\n\nSimilarity probability for the Stuttgart sample calculated for the time slice at 5600 BC with two MDS-based ancestry components C1 and C2.\n\n\n\n\n\nThe Stuttgart sample used for illustrative purposes here was taken from an Early Neolithic individual from Southern Germany and first published in Lazaridis et al. (2014)."
  },
  {
    "objectID": "mobest.html#sec-install",
    "href": "mobest.html#sec-install",
    "title": "6  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "6.2 Installing mobest",
    "text": "6.2 Installing mobest\nmobest is an R package and can be installed directly from GitHub with the following code on the R console:\nif(!require('remotes')) install.packages('remotes')\nremotes::install_github('nevrome/mobest')\nYou can also install specific/older versions of mobest with the following syntax: nevrome/mobest[@ref|#pull|@*release]. For example to install the publication release version you can run remotes::install_github('nevrome/mobest@1.0.0').\nFor any of this to work a number of system libraries (mostly for processing geospatial data) have to be installed on your system, primarily for one particular dependency of mobest: the sf R package (Pebesma (2018)). The following table includes the libraries and the names of the relevant packages in the package management systems of various Linux distributions and MacOS.\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n\nSystem library\ndeb package(Ubuntu/Debian)\nrpm package(Fedora/CentOS)\npkgbuild package(Arch)\nbrew package(MacOS)\n\n\n\n\nGDAL\nlibgdal-dev\ngdal\ngdal\ngdal\n\n\nGEOS\nlibgeos-devlibgeos++-dev\ngeos-devel\ngeos\ngeos\n\n\nPROJ\nlibproj-dev\nproj-develsqlite-devel\nproj\nproj\n\n\nUDUNITS-2\nlibudunits2-dev\nudunits\nudunits\nudunits\n\n\n\nThe sf package maintainers provide a good explanation how to install these: https://r-spatial.github.io/sf/#installing"
  },
  {
    "objectID": "mobest.html#sec-basic",
    "href": "mobest.html#sec-basic",
    "title": "6  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "6.3 A basic similarity search workflow",
    "text": "6.3 A basic similarity search workflow\nThis section explains the setup for a basic ancestry similarity search with mobest in R.\nFor didactic purposes we use a simplified version of the data and code generated for the publication that introduced mobest: Schmid and Schiffels (2023) . This is a fairly generic setup you can adjust to the needs of other and more specific projects.\nThe script explained in the following sections as well as the data required for it can be downloaded in its entirety here:\n\nsimilarity_search.R\nsamples_basic.csv\n\n\n6.3.1 Preparing the computational environment\nFor this script we use various packages beyond base R, among which the following ones are required:\n\nreadr for loading .csv input data\nmagrittr for the pipe operator %&gt;%\nsf for loading and manipulating spatial data\nrnaturalearth for downloading spatial reference data (Massicotte and South (2024))\nggplot2 (and cowplot) to visualize intermediate and final results (Wilke (2024))\ndplyr for data manipulation of data.frames\nmobest (obviously)\n\n\nMassicotte, Philippe, and Andy South. 2024. Rnaturalearth: World Map Data from Natural Earth. https://docs.ropensci.org/rnaturalearth/.\n\nWilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://wilkelab.org/cowplot/.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nreadr, magrittr, ggplot2 and dplyr are all in the tidyverse (Wickham et al. (2019)) and can be installed in one go with install.packages(\"tidyverse\") on the R console. For the installation of sf and mobest please see the instructions above.\nWe will generally call functions explicitly with their namespace using :: (so e.g. readr::read_csv()). The only exceptions are magrittr and ggplot2, because we will use their functions so often that it becomes tedious to type them out. Instead we load them at the beginning.\nlibrary(magrittr)\nlibrary(ggplot2)\n\n\n6.3.2 Preparing the input data\n\n6.3.2.1 Generating the the spatial prediction grid\nmobest’s similarity search is typically run for a regular grid of spatial positions in the area of interest. It provides a function, mobest::create_prediction_grid(), to create such a grid, given a specification of the desired area. This area is typically the land area in a certain part of planet Earth.\n\n6.3.2.1.1 Defining the research area\nIn a first step we therefore have to define the research area for our analysis as a polygon in space. One way of doing so is to provide a list of latitude and longitude coordinates (extracted e.g. from Google Maps). The following code defines a simple research area covering large parts of Western Eurasia.\nresearch_area_4326 &lt;- sf::st_polygon(\n  list(\n    cbind(\n      c(35.91,11.73,-11.74,-15.47,\n        37.06,49.26,49.56,35.91), # longitudes\n      c(25.61,28.94,31.77, 62.73,\n        65.67,44.56,28.55,25.61)  # latitudes\n    )\n  )\n) %&gt;% sf::st_sfc(crs = 4326)\nSpatial coordinates require a coordinate references system (CRS). For lat-lon coordinates we typically use WGS84 with the EPSG code 4326. st_polygon() creates a simple polygon as a clockwise arrangement of individual coordinates and st_sfc() properly defines this polygon as a geographic area on Earth.\nA simple way to interactively inspect this polygon on a world map in R is provided by the mapview package (Appelhans et al. (2023)): mapview::mapview(research_area_4326).\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2023. Mapview: Interactive Viewing of Spatial Data in r. https://github.com/r-spatial/mapview.\n\n\n\nThe defined research area plotted on top of a map.\n\n\nWith the research area properly defined we can move to the next challenge and extract the land area in the research area. For that we have to obtain a dataset with polygons that trace the world’s coastlines. The naturalearthdata project provides open worldwide geodata in different resolutions and in easy to use data formats.\nThe rnaturalearth package makes it easy to download this data right into sf objects in R.\nworldwide_land_outline_4326 &lt;- rnaturalearth::ne_download(\n  scale = 50, type = 'land', category = 'physical',\n  returnclass = \"sf\"\n)\nWe can then crop the land outline to the research area to obtain the land area we are interested in.\nresearch_land_outline_4326 &lt;- sf::st_intersection(\n  worldwide_land_outline_4326,\n  research_area_4326\n)\nUsing ggplot2, we can finally plot the resulting spatial multi-polygon.\nggplot() + geom_sf(data = research_land_outline_4326)\n\n\n\nThe land area within the research area.\n\n\n\n\n6.3.2.1.2 Projecting the spatial data\nAt this point we run into a specific issue of mobest: It requires its “independent” spatial and temporal coordinates to be coordinates in a Cartesian system describing Euclidean space.\nFor the spatial coordinates that means we can not work with latitude and longitude coordinates on a sphere, but have to transform them. We have to apply map projection to represent the curved, two dimensional surface of our planet on a simple plane.\n\n\n\n\n\n\nNote\n\n\n\nThe question how exactly this should be done and which CRS to choose depends on the position, size and shape of the research area. Each map projection algorithm has different properties regarding whether they manage to preserve or distort size, shape, distances and directions of areas and lines compared to the actual circumstances on Earth. Generally the larger the research area the bigger the distortion of these properties becomes.\nBut for mobest we ideally want to represent all them accurately. mobest is therefore unfit for origin search on a global scale, but can usually be well applied for individual countries with the projections recommended by their cartographic agencies. For an intermediate, continental scale, as in this example, we have to choose our CRS wisely.\n\n\nWe decided to follow the recommendation of Annoni et al. (2003) and chose ETRS89 Lambert Azimuthal Equal Area coordinate reference system as in EPSG code 3035.\n\nAnnoni, A. et al. 2003. “Map Projections for Europe.” Technical Report EUR 20120 EN. European Commission Joint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\nTsoulos, Lysandros. 2003. “An Equal Area Projection for Statistical Mapping in the EU.” In Map Projections for Europe, edited by A. Annoni et al., 50–55. European Commission Joint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\nOur decision comes at the price of increased inaccuracy especially in the North- and South-East of the research area where we get very far away from the specified centre for EPSG:3035 at 52° latitude and 10° longitude (see Tsoulos (2003) p.53 for a visualization of the deformative effects).\nTo transform the land outline in the research area from EPSG:4326 to EPSG:3035 we can apply sf::st_transform().\nresearch_land_outline_3035 &lt;- research_land_outline_4326 %&gt;%\n  sf::st_transform(crs = 3035)\nNote how the change in the coordinate system affects the map plot.\nggplot() + geom_sf(data = research_land_outline_3035)\n\n\n\nThe research area land polygon now transformed to EPSG:3035.\n\n\n\n\n6.3.2.1.3 Creating the prediction grid\nTo finally create the prediction grid we can use mobest::create_prediction_grid(). It takes the land outline polygon and overlays its bounding box with a regular grid (using sf::st_make_grid()), where each cell has the size corresponding to the spatial_cell_size parameter. It then determines the centres of each grid cell and crops the resulting, regular point cloud with the land area.\nNote that spatial_cell_size uses the unit of the CRS, so in our case for EPSG:3035 meters. That means a value of 50000 translates to one point every 50km.\nspatial_pred_grid &lt;- mobest::create_prediction_grid(\n  research_land_outline_3035,\n  spatial_cell_size = 50000\n)\nThe total number of resulting spatial prediction positions is 4738 in this example.\ncreate_prediction_grid returns an object of class mobest_spatialpositions, which is derived from tibble::tibble(). That means we can print it on the R console and it will behave as a tibble. It will also work seamlessly as an input for ggplot2, which we can now use to visualize the point cloud.\nggplot() +\n  geom_sf(data = research_land_outline_3035) +\n  geom_point(\n    data = spatial_pred_grid,\n    mapping = aes(x, y),\n    color = \"red\",\n    size = 0.25\n  )\n\n\n\nThe 4738 spatial prediction grid points plotted on top of the land area.\n\n\n\n\n\n6.3.2.2 Reading the input samples\nmobest requires a set of data points, archaeogenetic samples, to inform the ancestry field interpolation. For each sample the position in space, time and a dependent variable space (e.g. the coordinates in a PCA analysis) must be known. This information must be provided in a specific format. A typical mobest workflow involves preparing a sample list in a .xlsx or .csv table, which could then be read into R and transformed to the correct format.\nFor this tutorial we rely on the data used and published in Schmid and Schiffels (2023). You can download a simplified version of this dataset here (samples_basic.csv) and load it into R.\nsamples_basic &lt;- readr::read_csv(\"path/to/your/downloaded/samples_basic.csv\")\nsamples_basic includes the following columns/variables:\n\n\n\n\n\n\n\n\nColumn\nType\nDescription\n\n\n\n\nSample_ID\nchr\nA sample identifier\n\n\nLatitude\ndbl\nThe latitude coordinate where this sample was recovered\n\n\nLongitude\ndbl\nThe longitude coordinate\n\n\nDate_BC_AD_Median\nint\nThe median age of this sample in years BC/AD(negative numbers for BC, positive ones for AD)\n\n\nMDS_C1\ndbl\nThe coordinate of this sample on dimension 1 of an MDS analysis.See Schmid and Schiffels (2023) for more details on how this was obtained\n\n\nMDS_C2\ndbl\nThe coordinate of this sample on MDS dimension 2\n\n\n\nThese variables are a minimum for a meaningful mobest run and must be known for all samples. Samples with missing information in any of these columns have to excluded from the input.\nJust as for the research area we have to transform the coordinates from longitude and latitude coordinates to a projected system, specifically the same as the one we selected above. To do this we can construct an sf object from the sample table, apply sf::st_transform() and then transform this result back to a tibble with the x and y coordinates of EPSG:3035 in extra columns. This last step makes the code a bit awkward.\nsamples_projected &lt;- samples_basic %&gt;%\n  # make the tibble an sf object\n  sf::st_as_sf(\n    coords = c(\"Longitude\", \"Latitude\"),\n    crs = 4326\n  ) %&gt;%\n  # transform the coordinates\n  sf::st_transform(crs = 3035) %&gt;% \n  # reshape the sf object back into a simple tibble\n  dplyr::mutate(\n    x = sf::st_coordinates(.)[,1],\n    y = sf::st_coordinates(.)[,2]\n  ) %&gt;%\n  sf::st_drop_geometry()\nWith the coordinates in the same reference system as the landmass polygons we prepared above we can now combine both in a single figure:\nggplot() +\n  geom_sf(data = research_land_outline_3035) +\n  geom_point(\n    data = samples_projected,\n    mapping = aes(x, y),\n    color = \"darkgreen\",\n    size = 0.25\n  )\n\n\n\nThe spatial distribution of the informative samples.\n\n\nA number of samples are outside of the area we want to predict here. That is no problem. They will inform the field in the north-eastern fringes of the area and do no harm. It is much more problematic that some zones within our research area are severely under-sampled. We have to keep sampling gaps like this in mind when we interpret the results of the similarity search.\n\n\n\n6.3.3 Specifying the search sample\nmobest’s similarity search usually takes the perspective of an individual sample for which we want to determine similarity probabilities for a spatial prediction grid at a specific point in time. For this sample, the “search sample”, we require the same information as for the input samples: The position in space, time and the dependent variable space (e.g. PCA or MDS space).\nTechnically this is only a requirement of the mobest interface. Conceptually such a similarity search only really requires the dependent variable space position of interest. The added benefit of having all information there is the relative time search setting (see below) and a very comprehensive output table for the most common use-case.\nIn this example we will locate one specific sample with a pretty well studied ancestry history: The sample named Stuttgart published in Lazaridis et al. (2014). We can select it as a subset of our sample table:\n\nLazaridis, Iosif, Nick Patterson, Alissa Mittnik, Gabriel Renaud, Swapan Mallick, Karola Kirsanow, Peter H Sudmant, et al. 2014. “Ancient Human Genomes Suggest Three Ancestral Populations for Present-Day Europeans.” Nature 513 (7518): 409–13. https://doi.org/10.1038/nature13673.\nsearch_samples &lt;- samples_projected %&gt;%\n  dplyr::filter(\n    Sample_ID == \"Stuttgart_published.DG\"\n  )\nWith this setup the search sample itself will be part of the samples used to inform the ancestry field interpolation (samples_projected). This is no problem - the search sample is a known data point in space and time that can very well be employed to build a better model of the past ancestry distribution. There may be research questions for which this might not be desired, though. Then it can just as well be excluded from the samples_projected table.\n\n\n6.3.4 Running mobest’s interpolation and search function\nWith the input data, both the spatial prediction grid and the samples to inform the ancestry field interpolation, prepared and ready, we can now run mobest::locate(). For that we first have to split and transform the input into the required data structures.\n\n6.3.4.1 Building the input data for the interpolation\nHere is how the interface of mobest::locate() looks:\nmobest::locate(\n  # spatiotemporal coordinates of the reference samples\n  # informing the ancestry field\n  independent = ...,\n  # genetic coordinates of the reference samples\n  dependent   = ...,\n  # ---\n  # interpolation settings for each ancestry component\n  kernel = ...,\n  # ---\n  # spatiotemporal coordinates of the sample of interest\n  search_independent = ...,\n  # genetic coordinates of the sample of interest\n  search_dependent   = ...,\n  # ---\n  # spatial search grid: where to search\n  search_space_grid  = ...,\n  # search time: when to search\n  search_time      = ...,\n  search_time_mode = ...,\n  # ---\n  # should the result be normalized\n  normalize = ...\n)\nEach of these arguments requires specific input.\n\n6.3.4.1.1 Independent and dependent positions\nThe locest() arguments independent and dependent take the spatiotemporal and genetic (as for example derived from MDS/PCA) positions of the interpolation-informing samples. The terms independent and dependent allude to the notion and terminology of a statistical model, where positions in dependent, genetic space are predicted based on positions in independent, spatiotemporal space.\nSpatiotemporal positions are encoded in mobest with a custom data type: mobest_spatiotemporalpositions. For the independent argument of locest() we have to construct an object of this type with mobest::create_spatpos() to represent the positions of the input samples in samples_projected.\nind &lt;- mobest::create_spatpos(\n  id = samples_projected$Sample_ID,\n  x  = samples_projected$x,\n  y  = samples_projected$y,\n  z  = samples_projected$Date_BC_AD_Median\n)\nThe dependent, genetic variables are also encoded in a custom, tabular type: mobest_observations with the constructor function mobest::create_observations().\ndep &lt;- mobest::create_obs(\n  C1 = samples_projected$MDS_C1,\n  C2 = samples_projected$MDS_C2\n)\nNote that you can have an arbitrary number of these components with arbitrary names. The only condition is, that the very same set and names are used below for the search samples and for the kernel parameter settings of each dependent variable.\nThe lengths of the vectors (samples_projected$...) used for create_spatpos() and create_obs() all have to be identical. And their order has to be the same as well: Although the input is distributed over two constructors they describe the same samples.\nFor the search sample in search_samples, finally, we have to construct objects of the same type and structure:\nsearch_ind &lt;- mobest::create_spatpos(\n  id = search_samples$Sample_ID,\n  x  = search_samples$x,\n  y  = search_samples$y,\n  z  = search_samples$Date_BC_AD_Median\n)\nsearch_dep &lt;- mobest::create_obs(\n  C1 = search_samples$MDS_C1,\n  C2 = search_samples$MDS_C2\n)\n\n\n6.3.4.1.2 Kernel parameter settings\nThe locest() argument kernel takes an object of the class mobest_kernelsetting. This type encodes kernel configurations for each dependent variable, so the parameters for the Gaussian process regression (GPR) interpolation that should be used for this variable. These include mostly the lengthscale parameters in space (x and y) and time, as well as the nugget parameter (Gramacy (2020), specifically here). In very simple terms:\n\n———. 2020. Surrogates: Gaussian Process Modeling, Design and  Optimization for the Applied Sciences. Boca Raton, Florida: Chapman Hall/CRC.\n\nLengthscale parameters: How far in space and time should an individual sample’s genetic position inform the interpolated field.\nNugget: Error term to model local variability of the dependent variable, so for observations from the same position in space and time.\n\nHere is a possible kernel configuration for our example. We construct two kernel settings, one for each ancestry component, with mobest::create_kernel() in mobest::create_kernset().\nkernset &lt;- mobest::create_kernset(\n  C1 = mobest::create_kernel(\n    dsx = 800 * 1000, dsy = 800 * 1000, dt = 800,\n    g = 0.1\n  ),\n  C2 = mobest::create_kernel(\n    dsx = 800 * 1000, dsy = 800 * 1000, dt = 800,\n    g = 0.1\n  )\n)\nNote how we scale the lengthscale parameters: dsx and dsy are set in meters (800 * 1000m = 800km) and dt in years (800y). g is dimensionless. With the setting specified here both dependent variables will be interpolated with the same, very smooth (several hundred kilometres and years in diameter) kernel.\nThe main question naturally arising from this, is how to set these parameters for a given dataset and research question. There are various empirical ways to find optimal values through numerical optimization. See Supplementary Text 2 of Schmid and Schiffels (2023) and the mobest documentation for the approaches we applied.\n\n\n\n\n\n\nNote\n\n\n\nWhile estimating the nugget is generally advisable, we would argue, that the computationally expensive crossvalidation workflow to estimate the lengthscale parameters is not always necessary for basic applications of mobest.\nThe analysis in Schmid and Schiffels (2023) showed that Gaussian process regression returns reasonably accurate interpolation results for a large range of kernel parameter settings, as long as they reflect a plausible intuition about the mobility behaviour of human ancestry, which generally operates on a scale of hundreds of kilometres and years.\nmobest is primarily a visualization method and adjusting its parameters to ones liking is legitimate if the choices are communicated transparently.\n\n\n\nSchmid, Clemens, and Stephan Schiffels. 2023. “Estimating Human Mobility in Holocene Western Eurasia with Large-Scale Ancient Genomic Data.” Proceedings of the National Academy of Sciences 120 (9). https://doi.org/10.1073/pnas.2218375120.\n\n\n6.3.4.1.3 Search positions\nWith input data and settings out of the way we can now specify the points in space and time where we actually want to perform the search. For these positions the GPR model is queried to return a mean and error, which are in turn used to calculate the probability density of a specific dependent variable space position, e.g. a specific coordinate on the first coordinate of an MDS analysis.\nWe already performed all necessary work for the search_space_grid argument, so the spatial positions of the prediction grid. We can just enter spatial_pred_grid here.\nThe search time can be specified as an integer vector of years: e.g. search_time = c(-500, -200, 100). This vector gets interpreted by mobest::locate() in two different ways, which can be selected with the switch argument search_time_mode. search_time_mode can either be \"relative\" (which is the default!) or absolute.\n\n\"relative\": The search_time is interpreted as a \\(\\Delta t\\) relative to the age of the search sample(s). Negative values point to ages that are older then the sample age, so in their relative past, and positive ones to younger ages in their relative future. In this example -500 would be interpreted as 500 years prior to the year the Stuttgart sample presumably died (so -5242-500 = -5742 BC/AD), and 100 as an age 100 years after their death (so -5242+100 = -5142 BC/AD).\n\"absolute\": The values in search_time are simply interpreted as absolute ages in years BC/AD.\n\nFor this example we will set the search time to an \"absolute\" value.\nsearch_time = -6800\nsearch_time_mode = \"absolute\"\nThis will search at exactly one point in time; a single timeslice 6800 BC.\n\n\n6.3.4.1.4 Normalization\nThe last relevant option of locate(), normalize, concerns the normalization of the output. mobest’s search calculates likelihoods for each search point. This is a dimensionless measure that is hard to compare across multiple runs with different parameter settings. If normalize is set to TRUE, then the densities for sets of spatial points that share all other parameters (including the timeslice) are rescaled to a sum of one, so to proper probabilities.\nWe assume users generally want to use mobest, specifically locate(), to calculate similarity probability density maps for individual samples, time slices and parameter settings. The most natural normalization for this case is to unify the scaling of these maps. This renders them comparable. normalize should therefore be set to TRUE for basic applications. This is also encoded as the the default setting.\n\n\n\n6.3.4.2 Calling mobest::locate()\nIn the previous sections we have thoroughly prepared the input for a first, simple run of mobest::locate(). We can now finally call the function.\nsearch_result &lt;- mobest::locate(\n  independent        = ind,\n  dependent          = dep,\n  kernel             = kernset,\n  search_independent = search_ind,\n  search_dependent   = search_dep,\n  search_space_grid  = spatial_pred_grid,\n  search_time        = -6800,\n  search_time_mode   = \"absolute\"\n)\nThis typically runs for a couple of seconds, uses every available processor core and returns an object search_result, which we will inspect below.\n\n\n\n6.3.5 Inspecting the computed results\nmobest::locate() returns an object of class mobest_locateoverview. It includes the relevant information for visualization and further processing of the analysis results.\n\n6.3.5.1 The mobest_locateoverview table\nmobest_locateoverview is derived from tibble and has a large set of columns, many not immediately relevant to the basic example here. This applies especially for the variables documenting the excessive permutation mechanics hidden behind the relatively simple interface of mobest::locate(). locate() is, in fact, a wrapper function for the more flexible function mobest::locate_multi(), which can handle permutations in various additional input parameters.\nEach row of the mobest_locateoverview table stores the calculated interpolated mean, error and similarity probability (field_mean, field_sd, probability) for one permutation of the input point positions in independent and dependent variable space (independent_table_id and dependent_setting_id), one dependent variable dependent_var_id, one iteration of the kernel settings (kernel_setting_id: dsx, dsy, dt, g), one prediction grid point emerging as a combination of spatial grid and search timeslice (pred_grid_id: field_id, field_geo_id, field_x, field_y, field_z, search_time) and finally one search sample (search_id, search_x, search_y, search_z, search_measured).\nHere is a list of the variables returned in mobest_locateoverview for each of these result iterations.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nindependent_table_id\nIdentifier of the spatiotemporal position permutation\n\n\ndependent_setting_id\nIdentifier of the dependent variable space position permutation\n\n\ndependent_var_id\nIdentifier of the dependent variable\n\n\nkernel_setting_id\nIdentifier of the kernel setting permutation\n\n\npred_grid_id\nIdentifier of the spatiotemporal prediction grid\n\n\ndsx\nKernel lengthscale parameter on the spatial x axis\n\n\ndsy\nKernel lengthscale parameter on the spatial y axis\n\n\ndt\nKernel lengthscale parameter on the temporal axis\n\n\ng\nKernel nugget parameter\n\n\nfield_id\nIdentifier of the spatiotemporal prediction point\n\n\nfield_x\nSpatial x axis coordinate of the prediction point\n\n\nfield_y\nSpatial y axis coordinate of the prediction point\n\n\nfield_z\nTemporal coordinate (age) of the prediction point\n\n\nfield_geo_id\nIdentifier of the spatial prediction point\n\n\nfield_mean\nMean value predicted by the GPR model for the dependent variable\n\n\nfield_sd\nError term predicted by the GPR model for the dependent variable\n\n\nsearch_id\nIdentifier of the search sample\n\n\nsearch_x\nSpatial x axis coordinate of the search sample\n\n\nsearch_y\nSpatial y axis coordinate of the search sample\n\n\nsearch_z\nTemporal coordinate (age) of the search sample\n\n\nsearch_time\nSearch time as provided by the user in locate()’s search_time argument\n\n\nsearch_measured\nGenetic coordinate of the search sample in the dependent variable space\n\n\nprobability\nProbability density for search_measured given all other parameters\n\n\n\nAs a result of the permutation of parameters, the size of the prediction grid and the number of search points, the number of rows in a mobest_locateoverview table can be calculated as a product of the individual counts of all relevant entities. One way to quickly validate the output of locate() and locate_multi() is to calculate the number of expected results based on the input and compare it with the actual number of rows in the output. For our example this calculation is fairly simple:\nWe have:\n\n\\(1\\) set of input point positions in independent variable space (independent_table_id)\n\\(1\\) set of input point positions in dependent variable space (dependent_setting_id)\n\\(2\\) dependent variables (dependent_var_id)\n\\(1\\) set of kernel parameter settings (kernel_setting_id)\n\\(4738\\) spatial prediction grid positions\n\\(1\\) time slice of interest\n\\(1\\) search sample\n\nThis means we expect exactly \\(2 * 4738 = 9476\\) rows in search_result, which we can confirm with nrow(search_result).\n\n\n6.3.5.2 Creating similarity probability maps for individual dependent variables\nThe most basic similarity probability map we can create with search_result is a map for just one parameter permutation, including only one dependent variable. In this case the relevant similarity probability observations are easy to obtain. We can just filter by dependent_var_id to only include either C1 or C2.\nresult_C1 &lt;- search_result %&gt;%\n  dplyr::filter(dependent_var_id == \"C1\")\nAnd this is then easy to plot with geom_raster(). We can then plot C1 and C2 together using cowplot::plot_grid().\np_C1 &lt;- ggplot() +\n  geom_raster(\n    data = result_C1,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n# for C2\nresult_C2 &lt;- search_result %&gt;%\n  dplyr::filter(dependent_var_id == \"C2\")\np_C2 &lt;- ggplot() +\n  geom_raster(\n    data = result_C2,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n# arrange both plots together\ncowplot::plot_grid(p_C1, p_C2, labels = c(\"C1\", \"C2\"))\n\n\n\nThe similarity probability search results for the sample Stuttgart for 6800 BC.\n\n\n\n\n6.3.5.3 Combining the information from multiple dependent variables\nThe results for individual dependent variables, so ancestry components like MDS or PCA dimensions, can be informative, but are usually under-powered to exclude highly improbable search results. Generally combining multiple ancestry components improves the accuracy of the results for individual samples, and we think this is best done by multiplying the results for the different dependent variables. This way spatial areas with high similarity probability for all dependent variables are naturally up-weighted, whereas areas that are unlikely similar for some dependent variables are down-weighted.\nTo perform the multiplication (and the re-normalization afterwards), mobest includes a function mobest::multiply_dependent_probabilities(). It works on objects of type mobest_locateoverview and yields tabular objects of type mobest_locateproduct. multiply_dependent_probabilities() is aware of the parameter permutations potentially encoded in the mobest_locateoverview table. It only combines the probabilities for dependent variables that share all other parameters. The number of rows in mobest_locateproduct will therefore be \\(\\frac{\\text{Number of rows in mobest\\_locateoverview}}{\\text{Number of dependent variables}}\\).\nIf we call it for search_result the output will thus have \\(9476/2=4738\\) rows.\nsearch_product &lt;-\n  mobest::multiply_dependent_probabilities(search_result)\nmobest_locateproduct tables feature a perfect subset of the columns in mobest_locateoverview. We can plot the combined similarity probability map with the code already applied for the individual dependent variables.\nggplot() +\n  geom_raster(\n    data = search_product,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n\n\nThe combined (\\(\\text{C1}*\\text{C2}\\)) similarity probability search results for the sample Stuttgart for 6800 BC.\n\n\nThis concludes a very basic similarity search workflow. Please see the documentation at https://nevrome.de/mobest for various other tutorials describing more advanced applications of mobest, starting with better map plotting."
  },
  {
    "objectID": "fst.html#theory-primer",
    "href": "fst.html#theory-primer",
    "title": "7  Measuring population structure using Fst",
    "section": "7.1 Theory primer",
    "text": "7.1 Theory primer\n\n7.1.1 Genetic drift\nGenetic drift is the process by which allele frequencies change randomly due to random fluctuations. Various models exist to model such fluctuations, but the most widely used one is the Wright-Fisher model. In that model, a parent generation of N individuals produces exactly N individuals as offspring, which make up the next generation. To model the fluctuations, every “child” gets assigned a random “parent” from the previous generation.\nExample: With \\(N=100\\) (haploid individuals), we might consider a genetic locus with two alleles \\(A\\) and \\(B\\), and in the parent generation, say, 50 individuals carried allele A, and 50 carried allele B. Then, the number of individuals carrying B in the next generation is the number of children who get assigned a parent with allele B. These will be close to 50, but not exactly, due to noise.\nAs a statistical process, this amounts to a binomial process, where the N offspring individuals are drawn with replacement, each carrying a 50% probability to have A vs. B. In the third generation, this probability may then have already shifted away from 50.\nHere is a simple function in R to model the allele frequency in a number of \\(g\\) successive generations, given a (haploid) population size \\(n\\) and a starting frequency \\(x0\\):\n\nwfsim &lt;- function(n, g, x0) {\n  res &lt;- numeric(g + 1)\n  res[1] &lt;- x0\n  for (i in 2:(g + 1)) {\n    res[i] &lt;- (rbinom(1, n, res[i - 1])) / n\n  }\n  return(res)\n}\n\nWe can test it:\n\nset.seed(1)\nwfsim(100, 10, 0.5)\n\n [1] 0.50 0.52 0.56 0.46 0.45 0.47 0.48 0.61 0.60 0.58 0.61\n\n\nSo indeed the allele frequency changes randomly. We can visualise it for more generations:\n\ntime_series &lt;- wfsim(100, 100, 0.5)\nplot(time_series, type = \"l\", ylim = c(0, 1),\n     xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nWe can better understand this random process, by simulating it many times and plotting the results together:\n\ngens &lt;- 100\nsims100 &lt;- replicate(50, wfsim(100, gens, 0.5))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nThis shows how the variance increases with time, and eventually more and more of these curves get absorbed at either \\(x=0\\) or \\(x=1\\), a process called “fixation”.\nHow does this process depend on the population size? We can take a look. Here are three families of simulations, with three different population sizes:\n\ngens &lt;- 1000\nsims100 &lt;- replicate(50, wfsim(100, gens, 0.5))\npar(mfrow = c(1, 3))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 100\")\n\nsims1000 &lt;- replicate(50, wfsim(1000, gens, 0.5))\nmatplot(sims1000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 1000\")\n\nsims10000 &lt;- replicate(50, wfsim(10000, gens, 0.5))\nmatplot(sims10000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 10000\")\n\n\n\n\nThis shows that larger populations have weaker fluctuations than small populations.\n\n\n7.1.2 \\(F_\\text{ST}\\) quantifies genetic drift\nTo quantify genetic drift, we can measure the variance of this process over time. The following plot uses the same data as shown above and estimates the variance:\n\npar(mfrow = c(1, 3))\nplot(apply(sims100,   1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 100\")\nplot(apply(sims1000,  1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 1000\")\nplot(apply(sims10000, 1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 10000\")\n\n\n\n\nso an increasing variance. But it doesn’t go up forever, but reaches a plateau. This is because of fixation: Once all curves reach fixaton at either \\(x=0\\) or \\(x=1\\), variance does no longer increase. In fact, the maximum variance corresponds to the state where all curves have been fixed. The variance to that state corresponds to the variance of a Bernoulli-process, which is \\(x_0(1-x_0)\\), so it depends on the starting frequency.\nIt is this plateau of the variance that defines \\(F_\\text{ST}=1\\)! Here is an illustration using again three families of simulations, but this time with the same population size but different starting frequencies:\n\nsims_x05 &lt;- replicate(1000, wfsim(100, gens, 0.5))\nsims_x03 &lt;- replicate(1000, wfsim(100, gens, 0.3))\nsims_x02 &lt;- replicate(1000, wfsim(100, gens, 0.2))\n\nplot_dat &lt;- cbind(\n  apply(sims_x05, 1, var),\n  apply(sims_x03, 1, var),\n  apply(sims_x02, 1, var)\n)\n\npar(mfrow = c(1, 1))\n\ncols &lt;- c(\"blue\", \"red\", \"green\")\nmatplot(plot_dat, type = \"l\", ylim = c(0, 0.25), lty = 1,\n             xlab = \"generations\", ylab = \"Variance\", col = cols)\nlegend(x = \"bottomright\",\n       legend = c(\"x = 0.5\", \"x = 0.3\", \"x = 0.2\", \"FST = 1\"),\n       lty = c(1, 1, 1, 2), col = c(cols, \"black\"))\n\ntheory_values &lt;- sapply(c(0.5, 0.3, 0.2), function(x) x * (1 - x))\nabline(h = theory_values, lty = 2, col = cols)\n\n\n\n\n\n\n7.1.3 Formal definition of \\(F_\\text{ST}\\)\n(Weir and Hill 2002) (explained and summarised in (Bhatia et al. 2013)) give a more formal evolutionary definition of \\(F_\\text{ST}\\), in terms of covariance between derived and ancestral populations. Specifically, for a given SNP, the definition involves the conditional probability of allele frequency \\(p_i\\) in population \\(i\\), given an ancestral allele frequency \\(p_\\text{anc}\\), which is defined as a random process with the expectation\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.” Annual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\\[E(p_i|p_\\text{anc}) = p_\\text{anc}\\]\nand variance \\[Var(p_i|p_\\text{anc}) = F_\\text{ST}^i p_\\text{anc}(1-p_\\text{anc}).\\]\nThis form of the conditional variance can be understood by analysing the equation for the two boundary cases: For \\(F_\\text{ST}^i=0\\), there is no variance, so the conditional probability of the derived frequency will be completely determined by the ancestral frequency with no random change. In contrast \\(F_\\text{ST}^i=1\\) means that the variance in the derived allele frequency is that of a binomial distribution with variance \\(p_\\text{anc}(1-p_\\text{anc})\\), indicating random but complete fixation of the frequency to 0 or 1.\n\\(F_\\text{ST}\\) between two populations A and B is then defined as \\[F_\\text{ST}(A,B) = \\frac{F_\\text{ST}^A+F_\\text{ST}^B}{2}\\].\n\n\n7.1.4 Estimating \\(F_\\text{ST}\\) from genomic data\nAll of the above considerations were made using only a single population. But when we usually measure \\(F_\\text{ST}\\), we measure it between two populations. While there are various mathematical definitions for both the theoretical definition and estimation for \\(F_\\text{ST}\\), which differ in subtle ways, we here follow the excellent paper by (Bhatia et al. 2013), which proposes the following estimator, termed Hudson-estimator, which in turn is based on a proposal by (Hudson, Slatkin, and Maddison 1992) and has been implemented in the ADMIXTOOLS package (Patterson et al. 2012):\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of Levels of Gene Flow from DNA Sequence Data.” Genetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\\[F_\\text{ST}=1-\\frac{H_w}{H_b}\\]\nHere, \\(H_w\\) is the average heterozygosity within each population, and \\(H_b\\) is the average heterozygosity between two populations. We can easily read off the two boundaries of the definition: At the lower end, we have \\(F_\\text{ST}=0\\) if and only if \\(H_w=H_b\\), so there is no difference between heterozygosity measured within or between groups, which is equivalent to saying that the two populations are the same. On the upper end we have \\(F_\\text{ST}=1\\) if and only if \\(H_w=0\\), so all observed variants are fully fixed in both populations (but not necessarily different between the populations).\nIt is fairly straight forward to see (and shown in (Bhatia et al. 2013)) that the Hudson-estimator above can be recast as\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013. “Estimating and Interpreting FST: The Impact of Rare Variants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\\[F_\\text{ST}(A,B)=\\frac{(a-b)^2}{a(1-b)+b(1-a)}\\]\nHere, \\(a\\) and \\(b\\) denote population allele frequencies, which are in principle unobserved, but can be approximated by sample allele frequencies. This approximation is biased, and (Patterson et al. 2012) gives additional formulae for an (asympotically) unbiased estimator (which is for example also used in Poseidon’s tool xerxes, as detailed in the whitepaper).\nFrom this definition, you can see that \\(F_\\text{ST}(A,B)\\) is closely related to F2-statistics, introduced in (Patterson et al. 2012):\n\\[F_2(A,B)=(a-b)^2\\].\nIn some sense, \\(F_\\text{ST}(A,B)\\) can be considered a normalised version of \\(F_2(A,B)\\). While both statistics range mathematically from 0 to 1, the upper bound 1 has very different meanings in both. A theoretical value of \\(F_2=1\\) would mean that both populations are fixed at different alleles in all studied SNPs, which is practically not possible (even completely random fixations would suggest that 1/4 of them would agree given that there are only four nucleotides, let alone the fact that such deeply diverged populations/species would not be alignable anymore). One can say that the time-scale on which \\(F_2\\) approaches 1, for non-ascertained SNPs, so the entire genome, is the time scale of nucleotide substitutions (i.e. mutations plus fixation) along species branches, which in neutral evolution is given by the inverse mutation rate \\(1/\\mu\\). This would mean something on the order of \\(10^8\\) generations, which is arguably of the same order of magnitude as the depth of the entire tree of life. In contrast, \\(F_\\text{ST}\\) approaches 1 on the time-scale of fixation of standing variation, which is \\(2N\\) generations, which for humans is on the order of 10000 generations, so around the depth of modern-human diversity from its origins in Africa several hundred thousand years ago. Arguably, this time scale is much more useful for data analyses and thus easier to interpret.\nOf course, in practice, one uses some ascertained SNP set, as also here in our examples below, in which case values are much higher because we consider only variants that are segregating in human populations within a relatively high allele frequency.\nIf you’ve gone through our chapter on F3 and F4 statistics, you will have already encountered our software xerxes. You can compute both the biased and the approximately unbiased estimators for \\(F_\\text{ST}(A,B)\\), using the FST or FSTvanilla statistics, as defined in the whitepaper.\nFor what follows, we will use the approximately unbiased form FST.\n\\(F_\\text{ST}(A,B)\\) has a convenient and untuitive scale: It ranges from 0 to 1, where \\(F_\\text{ST}(A,B)=0\\) denotes that \\(A\\) and \\(B\\) are the same population, with no differentiation whatsoever. On the other hand of the spectrum we have \\(F_\\text{ST}(A,B)=1\\), which would mean that two populations are fully separated.\nAnother way to see this measure is to consider it as relative shared variance: If you consider genetic variation between \\(A\\) and \\(B\\), and within each of \\(A\\) and \\(B\\), then \\(F_\\text{ST}(A,B)\\) can be considered to measure the average variance between populations relative to the average variance within populations, again with intuitive boundaries 0 and 1."
  },
  {
    "objectID": "fst.html#computing-fst-using-xerxes",
    "href": "fst.html#computing-fst-using-xerxes",
    "title": "7  Measuring population structure using Fst",
    "section": "7.2 Computing FST using xerxes",
    "text": "7.2 Computing FST using xerxes\nFor human present-day populations, we can compute pairwise FSt using xerxes.\nWe here chose a number of populations from (Patterson et al. 2012) with more than 10 samples per population, and prepare the following config file for xerxes:\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\nfstats:\n- type: FST\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n- type: F2\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\nThis will then produce all combinations of \\(FST(A, B)\\) and \\(F_2(A, B)\\) as indicated in the population lists.\n\n\n\n\n\n\nNote\n\n\n\nNote that the config-file engine in xerxes always computes all the combinations of populations, even for cases of \\(A=B\\). It also doesn’t know about symmetry, so will happily compute the redundant statistics \\(FST(\\text{Adygei}, \\text{Adygei})\\) and \\(FST(\\text{Adygei}, \\text{Adygei})\\). While this could be possibly improved, there is no big harm done, as this runs fairly quickly.\n\n\nWe run this config file using the command line\nREPO=/path/to/community-archive/2012_PattersonGenetics\n\nxerxes fstats -d $REPO --statConfig fstat_world_config.yaml -f fstat_world_output.tsv &gt; fstat_world_table.txt\n\n\n\n\n\n\nNote\n\n\n\nPlease see the Poseidon chapter on how to download Poseidon data from the community archive, as referenced in the following example\n\n\nThe standard output, is a nicely layouted ASCII Table, which looks like this in the beginning:\n.-----------.-----------------.-----------------.---.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |        a        |        b        | c | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=================:=================:===:===:=========:================:====================:==================:====================:\n| FST       | Adygei          | Adygei          |   |   | 593124  | 0.0000         | 0.0000             | 0.0000           | NaN                |\n| FST       | Adygei          | Balochi         |   |   | 593124  | 1.2789e-2      | 1.2789e-2          | 3.3572e-4        | 38.09517110646904  |\n| FST       | Adygei          | Basque          |   |   | 593124  | 1.8790e-2      | 1.8790e-2          | 4.0141e-4        | 46.810358341103225 |\n| FST       | Adygei          | BedouinA        |   |   | 593124  | 1.3017e-2      | 1.3017e-2          | 2.9647e-4        | 43.90737238689979  |\n| FST       | Adygei          | BedouinB        |   |   | 593124  | 3.3455e-2      | 3.3454e-2          | 5.7648e-4        | 58.03217592610529  |\n| FST       | Adygei          | Biaka           |   |   | 593124  | 0.1716         | 0.1716             | 1.2185e-3        | 140.85275693678508 |\n| FST       | Adygei          | Brahui          |   |   | 593124  | 1.4644e-2      | 1.4644e-2          | 3.4481e-4        | 42.46989237781921  |\n| FST       | Adygei          | Burusho         |   |   | 593124  | 1.8566e-2      | 1.8566e-2          | 3.8156e-4        | 48.6573908240317   |\n| FST       | Adygei          | Druze           |   |   | 593124  | 1.2173e-2      | 1.2173e-2          | 2.6659e-4        | 45.65975464203526  |\n| FST       | Adygei          | French          |   |   | 593124  | 9.7730e-3      | 9.7730e-3          | 3.1627e-4        | 30.9006924987833   |\n| FST       | Adygei          | Han             |   |   | 593124  | 9.8759e-2      | 9.8759e-2          | 1.1973e-3        | 82.48660429503893  |\n| FST       | Adygei          | Hazara          |   |   | 593124  | 3.0725e-2      | 3.0726e-2          | 7.1478e-4        | 42.98629834431124  |\n| FST       | Adygei          | Italian_North   |   |   | 593124  | 8.6600e-3      | 8.6601e-3          | 2.7883e-4        | 31.058813893781032 |\n\nbut of course has many more lines (&gt;1800 in this case). We also used the -f flag to output a tab-separated file, here named fstat_world_output.tsv, which is easier to read into R."
  },
  {
    "objectID": "fst.html#plotting-results-in-r",
    "href": "fst.html#plotting-results-in-r",
    "title": "7  Measuring population structure using Fst",
    "section": "7.3 Plotting results in R",
    "text": "7.3 Plotting results in R\nAll of the following code uses strictly only base-R for maximum compatibility. The code should run on any R installation.\nWe first load the data\n\ndat &lt;- dat &lt;- subset(read.table(\"fst_working/fstat_world_output.tsv\", sep=\"\\t\", header = TRUE),\n                     select=-c(c, d, Z_score_Jackknife))\ndatFST &lt;- dat[dat$Statistic == \"FST\",]\ndatF2 &lt;- dat[dat$Statistic == \"F2\",]\nhead(datFST)\n\n  Statistic      a        b NrSites Estimate_Total Estimate_Jackknife\n1       FST Adygei   Adygei  593124       0.000000           0.000000\n2       FST Adygei  Balochi  593124       0.012789           0.012789\n3       FST Adygei   Basque  593124       0.018790           0.018790\n4       FST Adygei BedouinA  593124       0.013017           0.013017\n5       FST Adygei BedouinB  593124       0.033455           0.033454\n6       FST Adygei    Biaka  593124       0.171600           0.171600\n  StdErr_Jackknife\n1       0.00000000\n2       0.00033572\n3       0.00040141\n4       0.00029647\n5       0.00057648\n6       0.00121850\n\n\nOk, this looks good. Let’s check out the largest values\n\nhead(dat[order(-dat$Estimate_Total),])\n\n    Statistic         a         b NrSites Estimate_Total Estimate_Jackknife\n166       FST     Biaka Karitiana  593124         0.3021             0.3021\n456       FST Karitiana     Biaka  593124         0.3021             0.3021\n473       FST Karitiana    Papuan  593124         0.3011             0.3011\n676       FST    Papuan Karitiana  593124         0.3011             0.3011\n468       FST Karitiana  Mandenka  593124         0.2798             0.2798\n526       FST  Mandenka Karitiana  593124         0.2798             0.2798\n    StdErr_Jackknife\n166        0.0016933\n456        0.0016933\n473        0.0026493\n676        0.0026493\n468        0.0017116\n526        0.0017116\n\n\nwhich shows that the largest FST values of around 0.3 are observed between Karitiana, from South America, and Biaka from Papua Neu Guinea (but note that these values are dependent on the ascertainment of SNPs, which here causes inflation)\nHere is a histogram of the values\n\nhist(datFST$Estimate_Total, xlab = \"FST\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nSo most values are in the range of a few percent and 20 percent, with a mean of\n\nmean(datFST$Estimate_Total)\n\n[1] 0.09015616\n\n\nWe can compare that to F2:\n\nhist(datF2$Estimate_Total, xlab = \"F2\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nwhich is an order of magnitude smaller.\nSo one of the key things to visualise is the pairwise matrix of FST, which we can quickly compute using the xtabs function from the stats package (part of base R):\n\nfstMat &lt;- xtabs(Estimate_Total ~ a + b, datFST)\nf2Mat &lt;- xtabs(Estimate_Total ~ a + b, datF2)\n\nand plot a simple heatmap using the powerful heatmap function from the stats package:\n\nheatmap(fstMat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nwhich we can compare to the output using F2, which looks almost the same:\n\nheatmap(f2Mat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nOK, let’s look at the dendrogram a bit closer:\n\nfstDist &lt;- as.dist(fstMat)\ndendro &lt;- hclust(fstDist, method=\"ward.D2\")\nplot(dendro, hang = -1, ylab = \"FST\", xlab = \"\", main = \"\")\n\n\n\n\nwhich again shows the strong drift that Native American populations (Karitiana) and Mayans experienced in their ancestral past.\nThis nicely shows how FST is affected by total drift, which is inversely proportional to population size, and proportional to total divergence time. A long branch can be caused by either low population size (as in the ancestral population of indigenous Americans) or long divergence time (as between populations from Africa and those outside of Africa)."
  },
  {
    "objectID": "pca_mds.html#theory",
    "href": "pca_mds.html#theory",
    "title": "8  Dimensionality reduction using PCA and MDS",
    "section": "8.1 Theory",
    "text": "8.1 Theory\n\n\n\n\n\n\nComparison of PCA and MDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\nMDS\n\n\n\n\nInput\noriginal data matrix / similarity matrix\npairwise distance matrix\n\n\nFocus\ncaptures maximum variance in data\npreserves pairwise distances\n\n\nMissing data?\nFill-in OR projection\nNot an issue if using summary statistics, but this hides the uncertainty of the statistic\n\n\n\n\n\n\n\n8.1.1 Rationale\nThe datasets used in human ancient DNA analysis are often extremely multidimensional, often including data from thousands of individuals, across hundreds of thousands (or millions!) of single nucleotide polymrphisms (SNPs) (Mallick et al. 2023). Even when choosing to summarise this genome-wide information to single statistics of genetic similarity (e.g. with Outgroup F3), a similarity matrix across individuals can become very large when comparing across hundreds of individuals. As the name implies, dimensionality reduction methods can reduce the number of dimensions in the underlying data, while also aiming to minimise the loss of information. The two such methods we will focus on in this tutorial are Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS). Both methods reveal structure within the dataset, and part of that structure is due to shared population history between individuals/populations. It is for that reason that both these methods are indispensible parts of an archaeogeneticist’s toolkit.\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\n8.1.2 Introduction to dimension reduction\nWhen using either of these methods, we are essentially representing the data on a new set of orthogonal axes, with its origin in the center of the data. In PCA we typically use the original data for this transformation (i.e. the genotype matrix), and attempt to find the axes that capture the most variation among the samples. A covariance matrix (i.e. a similarity matrix) is often calculated and used as a useful intermediate step in PCA. Instead, in MDS we start with a pairwise distance matrix (typically a matrix of 1-F3), and attempt to find a spatial representation that best captures the distances between points.\n\n\n\nA visual representation of the transformation PCA applies to a cloud of points, the range of which is represented by the blue oval. First, the data is rescaled around its own mean value, effectively moving the origin to the center of the data cloud (here shown s a red point). Then, the axes of maximal variation are discovered using linear algebra. Finally, the data is transformed to represent it along the identified axes of variation.\n\n\nThe results of both of these methods are (usually) a 2 dimensional plot in which the distances between individual points roughly correlates to the genetic distance between these individuals. Therefore, genetically similar individuals will be plotted close to one another, and further away from individuals that are more genetically dissimilar.\n\n\n8.1.3 The problem with missing data\nA recurring issue when analysing ancient DNA is the high degree of missing data (i.e. missingness). We often apply a minimum coverage filter to our datasets: A generally accepted rule-of-thumb for the 1240K dataset is a minimum of 15 000 covered (i.e. non-missing) SNPs. Another way to express this cutoff is to say that we will “happily” analyse data that is missing a genotype call in 98.8% of all SNPs in the dataset! So how does this high rate of missingness affect MDS and PCA?\n\n8.1.3.1 MDS\nMissingness does not affect MDS as adversely as it does PCA, on account of the use of a pairwise distance matrix of 1-F3. This matrix will only have missing values in cases where there is no overlapping coverage between two individuals/populations used in an F3 statistic. Instead, the issue with MDS is that all F3 statistics are treated as equally reliable, regardless of their associated error bar.\n\n\n8.1.3.2 PCA\nUnlike MDS, PCA is severely affected by missing data. During the rescaling of the data around its own mean values, missing data is “filled-in” to the mean value (mean imputation). This can cause points to shift towards the origin by a distance relative to the degree of missingness.\nBelow is a plot of the results of PCA on a dataset of differnt worldwide populations (Reich et al. 2012). In an attempt to limit the effects of colonial admixture on the studied Native American populations, the authors masked parts of the genomes of Native Americans that matched the European or African populations in their dataset, replacing those genotypes with missing data.\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane Mazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing Native American Population History.” Nature 488 (7411): 370–74.\n\n\n\nPCA results on the same dataset based on the raw data (left), and after masking parts of the genome in Native American populations that match European populations. Due to mean imputation, masked individuals are attracted to the Origin.\n\n\nAs you can see, individuals whose genotypes were masked are shifted towards the plot’s origin. So how can we use PCA with ancient samples that have high degrees of missingness? The answer is by using a Least Squares Projection, a.k.a. lsqproject!\n\n\n\n8.1.4 Projection\nThe idea of projection is simple, and applies similarly to both PCA and MDS. In PCA, you use a subset of the dataset to calculate your axes of variation, and then apply the resulting transformation to additional data, thus projecting them onto those axes. The important detail is that the variation between projected individuals is not taken into account when deciding which the axes of maximal variation are. Similarly, in MDS you project points to the MDS space based on their distances to the points that constructed the space, disregarding the distances of the projected points to one another.\nBelow is a PCA plot calculated on present-day West Eurasian populations together with some Mesolithic hunter-gatherer individuals (in light brown). In the right side plot, the ancient individuals have been included in the calculation of the principal components, while in the left side they are projected on the principal components of the present-day West Eurasians.\n\n\n\nPCA plot of ancient Mesolithic hunter-gatherers and present-day West Eurasian populations. On the right, the hunter gatherers are included in the principal component calculation, while on the left, they are projected on principal components calculated on the present-day populations only. The hunter-gatherers are part of three different groups: Eastern European hunter-gatherers (brown right-facing triangles), Scandinavian hunter-gatherers (brown diamonds), and Western European hunter-gatherers (brown half-filled circles).\n\n\nThere are two things to note here:\n\nFirst, comparing the placement of Eastern European hunter-gatherers (brown right-facing triangles) between the two plots, you can see that projecting these individuals does indeed provide results that are not affected by mean imputation, and thus are not shifted towards the origin.\nSecondly, if you compare the positions of the Western European hunter-gatherers (brown half-filled circles), you will notice that projection causes these individuals to be plotted closer to present-day populations.\n\nThe degree of missingness in the Western European hunter-gatherers (WHG) is relatively low, and hence the shift in their placement between the two plots is not the result of mean imputation. Instead, when projected the WHG illustrate the effects of shrinkage.\n\n\n8.1.5 Shrinkage\nShrinkage comes in two flavours:\n\nThe kind of shrinkage you saw with the WHGs above, is pretty intuitive. When projecting populations on axes of variation that do not capture all the variation of the projected populations, they will appear as if they have less variation than reality. This translates to the points “shrinking” towards the origin slightly. that is to say, because the WHG individuals come from a population that harboured far more genetic variation than is present within present-day West Eurasian populations, much of their true variation is “hidden” when projecting them.\nThe second kind of shrinkage (a.k.a. projection bias) arises because “samples used to calculate the PC axes”stretch” the axes” (from the smartpca documentation). This problem is exacerbated in datasets where the number of markers far exceeds the number of samples used for PC calculation. This is often the case in human population genomics.\n\nWhile the first shrinkage flavour can be argued to be a feature of PCA, projection bias can be a problem when trying to compare present-day populations to projected ancient populations. A demonstration of the effects of shrinkage can be seen below:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population reveals the effects of shrinkage on the positions of the projected individuals. In the absence of shrinkage, all points originating from the same population would be overlapping.\n\n\nShrinkage can be corrected by scaling the eigenvectors of the projected and/or non-projected individuals to bring them more in line with one another. Below is the same dataset as above, but ran through smartpca with the parameter shrinkmode: YES:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population. Shrinkage correction was done using ‘shrinkmode: YES’\n\n\nAs a note of caution, shrinkmode: Yes increases the runtime greatly. An alternative would be to identify specific present-day populations that are of interest for the ancient-to-modern comparison, and project those as well. So, for example, if we were to compare Iron Age individuals from Germany with present-day individuals from Germany, then we could decide to take out some or all present-day Germans and project those as well. That would make them fully comparable."
  },
  {
    "objectID": "pca_mds.html#practice",
    "href": "pca_mds.html#practice",
    "title": "8  Dimensionality reduction using PCA and MDS",
    "section": "8.2 Practice",
    "text": "8.2 Practice\n\n8.2.1 Preparation\n\n8.2.1.1 Get trident\nTrident is a Poseidon framework data management tool. It enables downloading Poseidon packages (genomic datasets, usually including ancient individuals, comprising genome-wide SNP data) from the Poseidon server, as well as creating and manipulating such packages.\ntrident for Linux:\n\n# download Trident v1.4.0.3 binary\nwget https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-Linux\n# rename to trident\nmv trident-Linux trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for MacOS:\n\n# download Trident v1.4.0.3 binary\ncurl -LO https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-macOS\n# rename to trident\nmv trident-macOS trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for Windows:\n\nDownload trident-Windows.exe file\n\n\n8.2.1.2 Prepare practise dataset:\nHere, we are downloading packages (listed in pca_mds_working/exampleData.fetchFile.txt) file from the Poseidon server into the scratch/poseidon-repository directory. The datasets come from the following publications: Patterson et al. 2012, Lazaridis et al. 2014, Raghavan et al. 2014, and Jeong et al. 2019\n\nmkdir -p scratch/poseidon-repository\n# This will take a few seconds to pull the data from the server\n./trident fetch -d scratch/poseidon-repository --fetchFile \"pca_mds_working/exampleData.fetchFile.txt\"\n\n\n# Check composition of one of the downloaded packages\nls scratch/poseidon-repository/2014_LazaridisNature-4.0.2\n\n\n# List all groups (populations) comprised by the downloaded packages\n./trident list --groups -d scratch/poseidon-repository/\n\n\n# Summarize information about  the downloaded packages\n./trident summarise -d scratch/poseidon-repository\n\n\n# Choose populations for the analysis (list for this exercise in \"exampleData.forgeFile.txt\"\")\nhead pca_mds_working/exampleData.forgeFile.txt\n\n\n# Count the number of listed populations to include\nwc -l pca_mds_working/exampleData.forgeFile.txt\n\n\n# Create (forge) a new repository with chosen groups from the downloaded packages\n./trident forge \\\n  -d scratch/poseidon-repository \\\n  -o scratch/forged_package \\\n  -n PCA_package_1 \\\n  --forgeFile pca_mds_working/exampleData.forgeFile.txt \\\n  --outFormat EIGENSTRAT\n\nThe created repository comprises genomic data for: 111 modern Eurasian populations, 6 modern Native American populations, and 1 Upper Palaeolithic Siberian individual MA-1 (“Mal’ta”).\n\n\n\n8.2.2 Run PCA\n\n# Prepare parameter file for the smartpca run\nmkdir -p scratch/smartpca_runs/poplist1 scratch/smartpca_runs/poplist2/\n\ncat &lt;&lt;EOF &gt; scratch/smartpca_runs/poplist1/parameters.par\ngenotypename:   scratch/forged_package/PCA_package_1.geno   ## Genotype data\nsnpname:    scratch/forged_package/PCA_package_1.snp        ## SNP information\nindivname:  scratch/forged_package/PCA_package_1.ind        ## Individual information\n\nevecoutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.evec           ## Eigenvectors\nevaloutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.eval           ## Eigenvalues\n\npoplistname:    pca_mds_working/PCA_poplists/PCA_poplist1.txt\n\nlsqproject: YES     ## Project individuals not included in PC calculation onto the PCs\noutliermode: 2      ## Turns off automatic outlier removal.\nnumoutevec:  4       ## The number of eigenvectors to print per sample. Default is 10.\nEOF\n\nSo prepared parameter file will cause smartpca to estimate PCs using only the individuals from the populations listed in PCA_poplist1.txt and project all the remaining individuals onto those estimated PCs.\n\n# Run smartpca\nsmartpca -p scratch/smartpca_runs/poplist1/parameters.par\n\n\n# Inspect the output files\nls scratch/smartpca_runs/poplist1/\nhead scratch/smartpca_runs/poplist1/PCA_poplist1.evec\n\nAdding populations (Native Americans)\n\n# Look into other provided poplists\nwc -l pca_mds_working/PCA_poplists/*\n\ndiff -y --suppress-common-lines pca_mds_working/PCA_poplists/PCA_poplist1.txt pca_mds_working/PCA_poplists/PCA_poplist2.txt\n\n\n# Replace poplist1 with poplist2 in the smartpca parameter file\nsed 's/poplist1/poplist2/g' scratch/smartpca_runs/poplist1/parameters.par &gt; scratch/smartpca_runs/poplist2/parameters.par\ncat scratch/smartpca_runs/poplist2/parameters.par\n\n\n# Rerun smartpca using poplist2 (additional populations)\nsmartpca -p scratch/smartpca_runs/poplist2/parameters.par\n\n# Inspect smartpca output\nls scratch/smartpca_runs/poplist2/\n\nSkipping projection (running smartpca without a poplist)\n\nmkdir -p scratch/smartpca_runs/all_pops\nhead -n 7 scratch/smartpca_runs/poplist1/parameters.par | sed 's/poplist1/all_pops/g' &gt; scratch/smartpca_runs/all_pops/parameters.par\ntail -n 2 scratch/smartpca_runs/poplist1/parameters.par &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\necho \"maxpops: 200\" &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\necho \"fastmode: YES\" &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\ncat scratch/smartpca_runs/all_pops/parameters.par\n\n\n## Runtime of about 2 minutes\nsmartpca -p scratch/smartpca_runs/all_pops/parameters.par\nls scratch/smartpca_runs/all_pops/\n\nAs a result we have three PCAs:\n\n\n\n\n\n\n\n\n\ndata used in PC estimation\ndata projected\n\n\n\n\nPCA_poplist1\nEurasians\nNative Americans, Mal’ta\n\n\nPCA_poplist2\nEurasians, Native Americans\nMal’ta\n\n\nPCA_all_pops\nEurasians, Native Americans, Mal’ta\n\n\n\n\n\n\n8.2.3 Plot PCA\n\nlibrary(tidyverse)\n\nif(!require('remotes')) install.packages('remotes')\nif (!require('janno')) remotes::install_github('poseidon-framework/janno')\n\n## Load in poplist data\n## poplist1 -- Eurasian populations\npoplist1 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n## poplist2 -- Eurasian populations + 6 Native American populations\npoplist2 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist2.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Load in eigenvector data\nPCA_poplist1_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/poplist1/PCA_poplist1.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_poplist2_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/poplist2/PCA_poplist2.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_all_pops_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/all_pops/PCA_all_pops.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\n\n## Finally, we load in the metadata from the forged package annotation file (janno). Here, we keep only the individual Ids, country and their Lat/Lon position.\nmetadata&lt;-janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%&gt;% select(Poseidon_ID, Latitude, Longitude, Country) %&gt;% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\nPCA_poplist1_ev &lt;- left_join(PCA_poplist1_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\nPCA_poplist2_ev &lt;- left_join(PCA_poplist2_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\nPCA_all_pops_ev &lt;- left_join(PCA_all_pops_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\n\n\n## First we subset the dataset to only the populations in the poplist\nmoderns_pl1 &lt;- PCA_poplist1_ev %&gt;% filter(Pop %in% poplist1$Pops)\n\n\np &lt;- ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal()\n\np + geom_point(\n        data=moderns_pl1, ##The input data for plotting\n        aes(x=PC1, y=PC2) ## Define the x and y axis\n        )\n\n\n## We can see how genetic similarity depends on geographical location by colouring the poins by longitude or latitude\nLon_plot &lt;- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Longitude)) ## Here we also define the colour of the points based on a variable\n\nLat_plot &lt;- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Latitude))\n\ngridExtra::grid.arrange(Lon_plot, Lat_plot, ncol=2)\n\n\n## As the orientation (+/-) of PC coordinates sometimes can change between runs of PCA, we use this code to ensure the same \"orientation\" for all users and thus enable making comparisons.\ncorner_inds_pl1 &lt;- moderns_pl1 %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl1$PC1[1] &gt; corner_inds_pl1$PC1[2]) { PCA_poplist1_ev &lt;- PCA_poplist1_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_pl1$PC2[1] &gt; corner_inds_pl1$PC2[2]) { PCA_poplist1_ev &lt;- PCA_poplist1_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_pl1 &lt;- PCA_poplist1_ev %&gt;% filter(Pop %in% poplist1$Pops)\n\n\n## Let's now colour the points by country. \nPCA_plot_1 &lt;- p +\n    geom_point(data=moderns_pl1, \n               aes(x=PC1, y=PC2, col=Country), \n               alpha=0.5    ## Makes points semi transparent (so that aggregations of points are visible).\n              )\nPCA_plot_1\n\n\n\n\nPractice plot A) PCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\n\n\n\n## Now, let's see where Mal'ta individual got projected\nPCA_plot_1 +\n    geom_point(\n        data=PCA_poplist1_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n    )\nggsave(\"scratch/PCA_plot_1.png\")\n\nPCA with Native American populations added to the analysis\n\n## First we reorient the PCA\ncorner_inds_pl2 &lt;- PCA_poplist2_ev %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl2$PC1[1] &gt; corner_inds_pl2$PC1[2]) { PCA_poplist2_ev &lt;- PCA_poplist2_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_pl2$PC2[1] &gt; corner_inds_pl2$PC2[2]) { PCA_poplist2_ev &lt;- PCA_poplist2_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_pl2 &lt;- PCA_poplist2_ev %&gt;% filter(Pop %in% poplist2$Pops)\n\n## Then we plot the output\nPCA_plot_2 &lt;-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_pl2 %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_pl2 %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n       geom_point(\n        data=PCA_poplist2_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_2\nggsave(\"scratch/PCA_plot_2.png\")\n\n\n\n\nPractice plot B) PCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\n\n\nAnd a PCA with all populations, including Mal’ta, used or estimation (no projection)\n\n## First we reorient the PCA\ncorner_inds_ap &lt;- PCA_all_pops_ev %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_ap$PC1[1] &gt; corner_inds_ap$PC1[2]) { PCA_all_pops_ev &lt;- PCA_all_pops_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_ap$PC2[1] &gt; corner_inds_ap$PC2[2]) { PCA_all_pops_ev &lt;- PCA_all_pops_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_ap &lt;- PCA_all_pops_ev #%&gt;% filter(Pop %in% poplist2$Pops) ## Poplist 2 contains all the present-day populations.\n\n## Then we plot the output\nPCA_plot_ap &lt;-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_ap %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_ap %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(\n        data=PCA_all_pops_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_ap\nggsave(\"scratch/PCA_plot_ap.png\")\n\n\n\n\nPractice plot C) PCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”\n\n\n\n\n8.2.4 PLINK MDS\nNow let’s get an MDS plot for pairwise distances for the same dataset\n\n##Convert package to PLINK format\n./trident genoconvert -d scratch/forged_package --outFormat PLINK\n\nls scratch/forged_package/\n\n# Compute pairwise distances of all individuals\nplink --bfile scratch/forged_package/PCA_package_1 --distance-matrix --out scratch/pairwise_distances\n\n\n## Read in individual IDs from MDS results\ninds &lt;- readr::read_tsv(\"scratch/pairwise_distances.mdist.id\", col_types=\"cc\", col_names=c(\"Population\", \"Poseidon_ID\"))\ninds\n\n\nmetadata &lt;- janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%&gt;% select(Poseidon_ID, Latitude, Longitude, Country) %&gt;% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\ninds &lt;- left_join(inds, metadata, by=\"Poseidon_ID\")\n\ndist_mat &lt;- matrix(scan(\"scratch/pairwise_distances.mdist\"), ncol=nrow(inds))\ndim(dist_mat)\n\n\n?heatmap\n\n\n# first try and filter for a few populations:\nunique(inds$Population)\n\nindices &lt;- inds$Population %in% c('French', 'Greek', 'Nganasan')\nhead(indices, 40)\n\n## Generate a heatmap of pairwise distances\nheatmap(dist_mat[indices,indices], labRow = inds$Population[indices], labCol = inds$Population[indices])\n\n\nlibrary(ggplot2)\nlibrary(magrittr) # This is for the pipe operator %&gt;%\nmds_coords &lt;- cmdscale(dist_mat)\ncolnames(mds_coords) &lt;- c(\"C1\", \"C2\")\nmds_coords &lt;- tibble::as_tibble(mds_coords) %&gt;%\n    dplyr::bind_cols(inds)\nmds_coords\n\n\ncorner_inds &lt;- mds_coords %&gt;% dplyr::select(Poseidon_ID, C1, C2) %&gt;% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds$C1[1] &gt; corner_inds$C1[2]) { mds_coords &lt;- mds_coords %&gt;% mutate(C1=-C1)}\nif (corner_inds$C2[1] &gt; corner_inds$C2[2]) { mds_coords &lt;- mds_coords %&gt;% mutate(C2=-C2)}\n\nggplot(mds_coords) + \n       geom_point(data=mds_coords %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"),                 aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal() +\n     geom_point(\n        data=mds_coords%&gt;% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nggsave(\"scratch/MDS_plot_ap.png\")\n\n\n\n\nPractice plot D) MDS estimated using modern Eurasian and Mal’ta genomic data (“MDS_plot_ap”)\n\n\n\n## How does MDS compare to PCA if we restrict to the populations in poplist1?\n## Read in the poplist\npoplist1 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Filter distance matrix\nindices_pl1 &lt;- inds$Population %in% poplist1$Pops\n\ndist_mat[indices_pl1, indices_pl1]\n\n\n## Do MDS\nmds_coords_pl1 &lt;- cmdscale(dist_mat[indices_pl1,indices_pl1])\ncolnames(mds_coords_pl1) &lt;- c(\"C1\", \"C2\")\nmds_coords_pl1 &lt;- tibble::as_tibble(mds_coords_pl1) %&gt;%\n    dplyr::bind_cols(inds %&gt;% dplyr::filter(inds$Population %in% poplist1$Pops))\nmds_coords_pl1\n\n\n## Reorient\ncorner_inds_mds1 &lt;- mds_coords_pl1 %&gt;% dplyr::select(Poseidon_ID, C1, C2) %&gt;% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_mds1$C1[1] &gt; corner_inds_mds1$C1[2]) { mds_coords_pl1 &lt;- mds_coords_pl1 %&gt;% mutate(C1=-C1)}\nif (corner_inds_mds1$C2[1] &gt; corner_inds_mds1$C2[2]) { mds_coords_pl1 &lt;- mds_coords_pl1 %&gt;% mutate(C2=-C2)}\n\n## Plot\nggplot(mds_coords_pl1) + \n     geom_point(data=mds_coords_pl1 %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords_pl1 %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal()+\n     geom_point(\n        data=mds_coords%&gt;% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\n\nggsave(\"scratch/MDS_poplist1.png\")\n\n\n\n\nPractice plot E) MDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\n\n\n\n8.2.5 Compare plots\nLet’s now sum up, by directly comparing all the plots we have generated:\n\n\n\n\n\n\nPCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\nPCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\nPCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”)\nMDS estimated using modern Eurasian, and Mal’ta genomic data (“MDS_plot_ap”)\nMDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\nThe dataset excluding Native American data comprises much less variation that makes up PC2 and hence the PC2 variation of Eurasians in plot A is stretched up compared to these in plots B and C (including Native Americans). In plot B and C it is therefore more difficult to observe differences within Eurasians along PC2 than in plot A. The variation making PC1 is comparable between the three plots. Individual Mal’ta is more closely related to Native Americans than an average Eurasian, so without Native Americans in the dataset (A) it ends up within the Eurasian variation as there is no Native American genetic signal present that would “pull” him away from the Eurasian variation towards the American variation. When Mal’ta is included in the estimation in plot C (not only projected, like in A and B), we can observe the effects of missing data, inherent for ancient genomes, causing this individual to be pulled towards the plot’s origin.\nIn MDS the missingness in Mal’ta’s data does not affect its position as it does in PCA. Inclusion of the diverged Native American data in the estimation does cause the decrease of the distances with the Eurasian population along the C2, but the difference between plot E and plot D is not as pronounced as between B and A."
  },
  {
    "objectID": "pca_mds.html#conclusions",
    "href": "pca_mds.html#conclusions",
    "title": "8  Dimensionality reduction using PCA and MDS",
    "section": "8.3 Conclusions",
    "text": "8.3 Conclusions\nIn PCA it is important to estimate the Eigenvalues using high-coverage samples, hence usually modern datasets, such as the 1000 Genomes Project (1kGP) or Human Genome Diversty Project (HGDP), are used for the estimation and the ancient samples are then projected onto the estimated PCs. Also, as mentioned above in the “Shrinkage” section, it is a good approach to project also some modern data if they are to be directly compared to the ancient samples.\nWhile MDS will be less sensitive to the effects of missing data, it disregards the uncertainty of the underlying pairwise distance estimates.\nIt is thus the best practice to perform both analyses and compare them taking the shortcomings of each into account when interpreting the relative positions of the studied individuals/populations obtained using these methods."
  },
  {
    "objectID": "pmrread.html#background-measures-of-relatedness",
    "href": "pmrread.html#background-measures-of-relatedness",
    "title": "9  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "9.1 Background: Measures of relatedness",
    "text": "9.1 Background: Measures of relatedness\n\n9.1.1 Coefficient of relationship\nCoefficient of relationship, denoted \\(r_{ij}\\) , defined by Sewall Wright in 1922 (Wright 1922), is a measure of the degree of biological relationship between two individuals, commonly used in genetics and genealogy. It calculates the proportion of genes that two individuals have in common as a result of their genetic relationship. The coefficient of relationship is a derivative of the coefficient of inbreeding (\\(f_k\\) or \\(C_{I_k}\\)) defined by Wright a year earlier. A coefficient of inbreeding for an individual is typically one-half the coefficient of relationship between the parents.\nCoefficient of relationship between the parents approaches a value of 1 as the level of inbreeding increases and approaches 0 the more remote the common ancestors are.\nIn human relationships, coefficient of relationship is often calculated based on the knowledge of the family tree, typically extending to up to three or four generations, using a formula:\n\\[\nr_{ij} = \\sum (^1/_2)^{L_{ij}}\n\\] where \\(L\\) is the numbers of generation links between two individuals (\\(i\\) and \\(j\\)). E.g. full siblings are linked by two links through the mother (siblingA - mother - siblingB) and two links through the father (siblingA - father - siblingB), therefore the coefficient of relationship between them is \\(r = (^1/_2)^2 + (^1/_2)^2 = (^1/_4) + (^1/_4) = (^1/_2)\\) , while e.g. a person with their aunt are linked by three links through the shared grandmother/mother and three through the shared grandfather/father, so \\(r=(^1/_2)^3 + (^1/_2)^3 = (^1/_8) + (^1/_8) = (^1/_4)\\) .\nNote that under such definition, the coefficient of relationship is a lower bound and an actual value that may be up to a few percent higher due to unaccounted for consanguinity within the pedigree. The value is accurate to within 1% if the full family tree of both individuals is known to a depth of seven generations.\n\n\n9.1.2 Kinship coefficient\nKinship coefficient, denoted \\(\\phi_{ij}\\) [fa:i], is the probability that one allele sampled from individual \\(i\\) and one allele sampled from the same locus from individual \\(j\\) are identical by descent.\n\\(1 - \\phi_{ij}\\) can thus be interpreted as the probability that a randomly sampled allele from each individual is not identical by descent. Assuming that alleles are not under linkage disequilibrium, this value can be estimated from genome-wide data for a pair of individuals.\nKinship coefficient \\(\\phi_{ij}\\) , under some assumptions such as limited inbreeding, is related to Wright’s coefficient of relationship, denoted \\(r_{ij}\\) , via:\n\\[\n\\phi_{ij} = (^1/_2)r_{ij}\n\\] and hence provides a direct relationship between the degrees of relatedness from the pedigree and the expected kinship coefficient \\(\\phi_{ij}\\) .\n\n\n\nTable 1. Values of the coefficient of relatedness and the kinship coefficient for different pedigree relationships up to the second-degree, assuming that \\(C_I\\) = 0. From Rohrlach et al. (2023)."
  },
  {
    "objectID": "pmrread.html#pairwise-mismatch-rate-pmr",
    "href": "pmrread.html#pairwise-mismatch-rate-pmr",
    "title": "9  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "9.2 Pairwise Mismatch Rate (PMR)",
    "text": "9.2 Pairwise Mismatch Rate (PMR)\nMethods of relatedness estimation applied routinely to modern data are not applicable to ancient genomes due to small numbers of individuals sampled and high rate of data missingness (ie. low coverage), as well as due to lack of diploid phased genomic data available for majority of such samples. Thus, estimation of the coefficient of relatedness for ancient individuals using these methods is prone to biases and generally unreliable.\nPairwise mismatch rate (PMR) was introduced by (Kennett et al. 2017) as a means to estimate relatedness between ancient individuals (Figure 1). For each pair of individuals they computed the average mismatch rate across all autosomal SNPs covered by at least one sequence read for both of the two compared individuals (when &gt;1 sequence read was present for one individual at a given site, a random read was sampled for the analysis) and computed standard errors using a weighted block jackknife. Mismatch rates significantly lower (Z&gt;3) than the highest observed value, provided putative evidence of relatedness.\nThe PMR can be used to estimate the kinship coefficient, which, assuming that we can account for the inbreeding coefficient \\(C_I\\), can be used to estimate the degree of relatedness. Hence, we may gain insights into the pedigree joining many individuals (to a certain resolution). Kennett et al’s (2017) \\(PMR\\) estimation, however, did not include a hard-classification method nor was wrapped into any particular software piece.\n\n\n\nFig 1. Pairwise mismatch rate calculation (PMR). A) Pseudohaploid genomes are compared within each pair of individuals. B) Only sites called in both individuals are considered (filled circles) and classified as match (green) or mismatch (red). Accounting for linkage disequilibrium: C) Estimation of PMR using sliding window (as implemented in READ) as well as genome-wide (optional in READv2). D) Estimation of PMR on thinned SNP data (as implemented in BREADR)\n\n\n\n9.2.1 Accounting for pseudohaploidization\nDue to pseudohaploidisation (ie. drawing one allele randomly for each position) identical individuals will have an expected PMR of half of this between unrelated individuals.\nThe estimate of relatedness coefficient \\(r\\) needs therefore be corrected using the expected mismatch rate in non-related individuals. In (Kennett et al. 2017) they chose correction based on the approximate maximum mismatch rates observed: \\(b = max(PMR_{observed})/2\\) . The estimator they used is thus:\n\\[\nr = 1 - ((PMR_{ij} - b)/b) .\n\\]\nPMR estimation in ancient-DNA-based inference of relatedness has first been implemented as separate software with READ (Monroy Kuhn, Jakobsson, and Günther 2018) and then by its successor - READv2 (Alaçamlı et al. 2024). Other software used in relatedness estimation among ancient individuals, such as BREADR (Rohrlach et al. 2023), also build on PMR.\n\n\n9.2.2 Accounting for linkage disequilibrium\nLinkage disequilibrium (LD, non-independent co-inheritance) of the loci included in the PMR estimation will bias the results towards falsely positive relatedness detection. To minimize this effect, different approaches can be employed. This is particularly crucial, when analyzing genome-wide (shotgun) data. In the 1240k SNP panel widely used in ancient genomics the analysed loci have already been selected taking LD into account. The approaches to minimize the LD bias that have been employed in PMR estimation comprise PMR estimation separately over consecutive genome fragments (sliding window) and obtaining median estimate among them (e.g., implemented in READv1, and as an option in READv2; Figure 1C), and decreasing the number of SNPs included in the analysis using a threshold of physical proximity of SNPs along the genome (thinning; e.g., implemented in BREADR; Figure 1D).\n\n\n9.2.3 Good practice\nOnly pairs with at least 10,000 overlapping SNPs of the 1240k SNP panel should be included in PMR estimation (Furtwängler et al. 2020)"
  },
  {
    "objectID": "pmrread.html#read-version-1",
    "href": "pmrread.html#read-version-1",
    "title": "9  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "9.3 READ (version 1)",
    "text": "9.3 READ (version 1)\nRelationship Estimation from Ancient DNA (Monroy Kuhn, Jakobsson, and Günther 2018) - formalized implementation of PMR for ancient genomic data.\n\nPseudohaploid input data (TPED/TFAM format)\nDivision of the genome into non-overlapping windows of 1 Mbps each and calculation of the proportion of non-matching alleles inside each window (P0) for each pair of individuals.\nNormalization of P0 using the value expected for a randomly chosen pair of unrelated individuals from the population (in order to make the classification independent of within population diversity, SNP ascertainment and marker density).\nThis value is estimated by calculation of the median of all average pairwise P0 per window across all pairs of individuals, which, under sufficient sample size, can be treated as a proxy for an expected P0 in a pair of unrelated individuals. Normalization can also be performed using parameters other than median, e.g., maximum observed P0 value among the pairs.\nClassification of each pair of individuals as unrelated, second-degree (i.e. nephew/niece-uncle/aunt, grandparent-grand- child or half-siblings), first-degree (parent-offspring or siblings) or identical individuals/identical twins from the average across the per-window proportions of non-matching alleles (P0).\n\n\n\n\nFigure 2. Outline of the general READ workflow to estimate the degree of relationship between two individuals. From (Monroy Kuhn, Jakobsson, and Günther 2018)\n\n\n\n9.3.1 Pluses\nThe method has been shown to work quite well with as little as 0.1x shotgun coverage per genome. It has very simple assumptions estimating the expected pairwise mismatch rate from the data without the need for population allele frequencies. It can thus be used as part of initial QC procedures (e.g. identifying duplicated individuals) or in populations (or species) for which little additional information is available.\n\n\n\n9.3.2 Minuses\nREAD had been implemented as a Python 2 script. The last version of Python 2 was released in 2020 and some systems have already stopped supporting the language. Furthermore, READ wrote a large number of temporary files to the hard disk which were then analyzed by a separate R script called from the Python script."
  },
  {
    "objectID": "pmrread.html#readv2",
    "href": "pmrread.html#readv2",
    "title": "9  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "9.4 READv2",
    "text": "9.4 READv2\nNew implementation of READ (Alaçamlı et al. 2024)\n\n9.4.1 Improvements over version 1\n\nAll analyses are carried out within a single Python3 script using NumPy (Harris et al. 2020) and pandas (McKinney 2010) libraries.\nMajor analysis speed improvement (although more memory-intensive)\nAvoids excessive use of temporary files and the calling of a separate R script.\nMinor gain in accuracy due to changes in some default values (based on performance tests results).\nIntroduction of the “effective number of overlapping SNPs” (number of overlapping SNPs times the pairwise mismatch rate expected for unrelated individuals) representing a measure of the amount of information available for kinship estimation in a given pair of individuals. Provides benchmarking and increases comparability between studies.\nAbility to classify up to third-degree relatives (requires at least 5000 effectively overlapping SNPs).\nAbility to differentiate between different types of first-degree relationships (ie. full siblings vs parent-offspring; requires at least 10,000 effectively overlapping SNPs).\nFeasible for as much as 696 individuals (241 860 pairs) provided enough available memory.\nGenome-wide P0 calculation as default; with the uncertainty estimated using a block-jackknife approach with block sizes of 5 Mb (as commonly employed in human population genomic studies; (Patterson et al. 2012)). Option to use window-based approach of chosen window size also available.\n\n\n\n\nFigure 3. READv2 flowchart. From (Alaçamlı et al. 2024)\n\n\n\n\n9.4.2 Noteworthy properties\nOverall, READv2 performs well down to at least 0.1X sequence data in the simulated dataset. This corresponded to on average about 1,878 overlapping SNPs for each pair of individuals at an expected mismatch for unrelated individuals of ∼0.247.\nIn default settings, READv2 performs a genome-wide estimate of the pairwise mismatch rate, based on which it will assess the degree of relationship in each pair of individuals. This is followed by a separate round of classification for first-degree relatives. Here, the genome is divided into 20Mb windows and the proportion of windows that are classified as either “identical/twin” or “unrelated” is estimated. These proportions correspond to Cotterman coefficients \\(k_0\\) – windows classified as unrelated (i.e. no shared chromosome), and \\(k_2\\) – windows classified as identical (i.e. both chromosomes shared). Expected \\(k_0 + k_2\\) is low for parent-offspring and around 0.5 for full siblings when sufficient data are available.\nIf that proportion is less than 0.3, the pair is classified as “parent-offspring”; if it is between 0.35 and 0.6, the pair is classified as “siblings”. For other proportions, or when the number of effectively overlapping SNPs is below 10,000, the pair remains classified as “first-degree” without further specification.\nThe two types of relations are well separated down to 0.5X coverage in the simulated dataset (or ∼8,000 “effectively overlapping SNPs”), but they overlap at 0.2X and below. Hence, to avoid wrongly classifying parent-offspring pairs as siblings, READv2 applies a default cutoff of 10,000 effectively overlapping SNPs, below which classification is not performed. (In (Rivollat et al. 2020) a cutoff of 7000 effective SNP number cutoff was used.)\n\n\n9.4.3 Warnings\nAt low coverage (0.05x and 0.1x) there are high false positive rates for second- and third-degree relatedness; many unrelated pairs are classified into these categories. At 0.01X, unrelated individuals are even classified as first-degree or identical twins, resulting in a reduced false positive rate for second- and third-degree but an increased false positive rate for first-degree. To avoid false classifications in empirical data, READv2 applies a conservative threshold of 5000 “effectively overlapping SNPs”, below which no attempt to classify third-degree relatives is taken.\nAccording to the authors READv2 alone can lead to very similar results as the combination of READv1 and lcMLkin. output table by including information such as the number of overlapping SNPs, the number of effectively overlapping SNPs, and the kinship coefficient \\(μ\\) (= 1 - normalized P0)\n\n\n9.4.4 Comparison with other methods\nREADv2 is very similar in its approach to BREADR (Rohrlach et al. 2023) and TKGWV2 (Fernandes et al. 2021), with each tool having its own unique features. READv2 has the functionality to separate the different first-degree relationships, BREADR has a better quantification of uncertainty, and TKGWV2 works well with lower amounts of input data.\n\n\n\nTable 2. Comparison of methods available for relatedness reconstruction in ancient genomic data. From Alaçamlı et al. (2024).\n\n\nFor further comparison of some of the methods available, you can also refer to the paper by Akturk et al. (2023)"
  },
  {
    "objectID": "pmrread.html#usage",
    "href": "pmrread.html#usage",
    "title": "9  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "9.5 Usage",
    "text": "9.5 Usage\n\n9.5.1 Running READ (version 1)\nREAD version 1 is implemented in Python 2.\nThe input for READ is a pair of files in Plink’s TPED/TFAM.\n## Get READ off Bitbucket\ngit clone https://bitbucket.org/tguenther/read.git\nIf starting from eigenstrat files you will need to use for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create a parameter file for Eigensoft convertf:\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenotToBed.convertf.param\n\n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n\n## Transpose packedped to tped\nplink --bfile myGenotypeFile --recode transpose --out myGenotypeFile \n## The output is myGenotypeFile.tped, myGenotypeFile.tfam, myGenotypeFile.nosex, myGenotypeFile.log\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted\ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the generated PLINK files you can no run READ:\n## Run READ\npython pathTo_Read.py myGenotypeFile &lt;normalization_method&gt;\n\n9.5.1.1 Options\nnormalization_method:\n\nmedian (default) - assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean - READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax - READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first degree relative to both others.\nvalue &lt;val&gt; - READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. A value can be obtained from the NonNormalizedP0 column of the file meansP0_AncientDNA_normalized from a previous run of READ.\n\nOptionally, one can add --window_size &lt;value&gt; at the end in order to modify the window size used (default: 1000000).\n\n\n9.5.1.2 Output:\n“READ_results”:\nPairIndividuals Relationship Z_upper Z_lower I0232I0358 Unrelated NA -17.0238 ... I0354I0360 First Degree 3.8987 -9.6782 I0354I0361 Unrelated NA -15.4353 ... I0421I0430 First Degree 7.5325 -12.1995 I0421I0431 Unrelated NA -3.9037 ...\n““meansP0_AncientDNA_normalized”:\nPairIndividuals Normalized2AlleleDifference StandardError NonNormalizedP0 NonNormalizedStandardError I0232I0358 1.0069 0.0059 0.2531 0.00149 ... I0354I0360 0.7587 0.0138 0.1907 0.0035 I0354I0361 1.0092 0.0067 0.2537 0.0017 ... I0421I0430 0.7409 0.0095 0.1862 0.0024 I0421I0431 1.0115 0.0270 0.2543 0.0068 ...\n“READ_results_plot.pdf”:\n\n\n\n\n\n\n\n\n9.5.2 Running READ v2\nREADv2 is recommended to be ran as conda environment.\n## Preparing the environment\ngit clone https://github.com/GuntherLab/READv2.git\n\nconda create -n readv2 python=3.7 pandas=1.1.1 numpy=1.18.3 pip=22.3.1 matplotlib=3.5.3\n\nconda activate readv2\n\npip install plinkio  ## Used for format conversion of the inpt data.\n## Personally I couldn't make plinkio work and am using Poseidon/trident or eigensoft convertf (as described below).\nThe input for READv2 is a trio of files in Plink’s BED/BIM/FAM format.\nIf starting with files in EIGENSTRAT format, they can be converted to PLINK format using for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create convertf parameter file for converting .geno to .bed files (this can probably also be done using PLINKIO, as implemented in READv2 documentation):\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenoToBed.convertf.param\n    \n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n## The output is myGenotypeFile.bed, myGenotypeFile.fam, myGenotypeFile.bim\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted       \ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the PLINK format files you can run READv2:\n## Activate READv2 conda environment\nconda activate readv2\n\n## Run READv2\npython pathTo_Readv2.py -i myGenotypeFile\n\n9.5.2.1 Options\nREADv2 options:\n\n-i, --input_file val – Input file prefix (required). The current READ version only supports Plink bed/bim/fam files.\n-n, --norm_method val – Normalization method (either ‘mean’, ‘median’, ‘max’ or ‘value’).\n\nmedian (default) – assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean – READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax – READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first-degree relative to both others.\nvalue – READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. Normalization value needs to be provided through --norm_value\n\n--norm_value val – Provide a user-defined normalization value\n--window_size val – Change window size for block jackknife or for window-based P0 estimates (as in READv1), default: 5000000\n--window_est – Window based estimate of P0 (as opposed to the genome-wide estimate, default in READv2)\n-h, --help – Print help message\n-v, --version – Print version\n\n\n\n9.5.2.2 Output:\n“meansP0_AncientDNA_normalized_READv2”\nPairIndividuals Norm2AlleleDiff StError_2Allele_Norm    Nonnormalized_P0    Nonnormalized_P0_serr   OverlapNSNPs\nI0232I0358  1.0052  0.0076    0.2553  0.0019    261263\n...\nI0234I0423  0.9659  0.0083  0.2453  0.0021    195053\nI0234I0424  0.9985  0.0071    0.2536 0.0018   224327\n...\nI0354I0360  0.7464  0.0137    0.1896 0.0035    21056\nI0354I0361  1.0046  0.0089    0.2552  0.0023    147438\n...\n“Read_Results.tsv”\nPairIndividuals Rel Zup Zdown   P0_mean Nonnormalized_P0    Nonnormalized_P0_serr   1st_Type    Perc_Win_1stdeg_P0  OverlapNSNPs    NSNPsXNorm  KinshipCoefficient\nI0232I0358  Unrelated   NA  12.9726  1.0052  0.2553  0.0019    N/A 0.9150  261263  66362.1298    -0.0052\n...\nI0234I0423  Third Degree    0.3487  7.1920   0.9659  0.2453  0.0021    N/A 0.7974  195053  49544.4533   0.0341\nI0234I0424  Unrelated   NA  12.9201  0.9985  0.2536 0.0018   N/A 0.9020  224327  56980.1981    0.0015\n...\nI0354I0360  First Degree    4.7682   8.7575   0.7464  0.1896 0.0035   N/A 0.3816  21056   5348.3310   0.2536\nI0354I0361  Unrelated   NA  11.0926 1.0046  0.2552  0.0023    N/A 0.9281  147438  37450.0013  -0.0046\n...\n“READ.pdf”:\n\n\n\n\n\n\n\n\n\nAktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra Katırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023. “Benchmarking Kinship Estimation Tools for Ancient Genomes Using Pedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor Mapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and Torsten Günther. 2024. “READv2: Advanced and User-Friendly Detection of Biological Relatedness in Archaeogenomics.” bioRxiv.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi. 2021. “TKGWV2: An Ancient DNA Relatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun Data.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar U Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes Reveal Social and Genetic Structure of Late Neolithic Switzerland.” Nat. Commun. 11 (1): 1915.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton, Adam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017. “Archaeogenomic Evidence Reveals Prehistoric Matrilineal Dynasty.” Nat. Commun. 8 (February): 14115.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018. “Estimating Genetic Kin Relationships in Prehistoric Populations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı, Marie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020. “Ancient Genome-Wide DNA from France Highlights the Complexity of Interactions Between Mesolithic Hunter-Gatherers and Neolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak. 2023. “BREADR: An r Package for the Bayesian Estimation of Genetic Relatedness from Low-Coverage Genotype Data.” bioRxiv. https://doi.org/10.1101/2023.04.17.537144.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and Relationship.” Am. Nat. 56 (645): 330–38."
  },
  {
    "objectID": "yleaf.html#resources",
    "href": "yleaf.html#resources",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.1 Resources",
    "text": "10.1 Resources\n\n10.1.1 Original publication\nRalf, Arwin, et al. “Yleaf: software for human Y-chromosomal haplogroup inference from next-generation sequencing data.” Molecular biology and evolution 35.5 (2018): 1291-1294. (Ralf et al. 2018)\n\n\n10.1.2 Software and manual\nPlease see the Yleaf-github-page"
  },
  {
    "objectID": "yleaf.html#background-to-y-chromosomal-haplogroup-assignment",
    "href": "yleaf.html#background-to-y-chromosomal-haplogroup-assignment",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.2 Background to Y-chromosomal haplogroup assignment",
    "text": "10.2 Background to Y-chromosomal haplogroup assignment\nThe human Y-chromosome haplogroup is determined by single-nucleotide polymorphisms (SNP), markers that are located in the non-recombining portion of the chromosome (NRY) which is passed down from father to son. Knowing the haplogroup can therefore be informative about biological relatedness through the paternal lineage, as well as for exploring human mobility and migration, as one can trace haplotypes’ dispersal times and geographic distributions. The mutations that accumulate over time provide us with a Y-chromosome phylogenetic tree, and a system has been developed for naming the major clades and subclades. As more SNPs are discovered, the phylogenetic tree continues to expand with the nomenclature having to continually adapt."
  },
  {
    "objectID": "yleaf.html#what-is-yleaf",
    "href": "yleaf.html#what-is-yleaf",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.3 What is Yleaf?",
    "text": "10.3 What is Yleaf?\nYleaf is a python-based tool for Linux for high-resolution NRY SNP calling and haplogroup inference from NGS data, independent of library and sequencing methods\n\n10.3.1 What are the advantage of using Yleaf?\n\npublicly available\nautomated\nuser-friendly\ncustomisable\nversatile\nmultithreading option\nbuilt-in batch option\noptimized for large NGS data sets such as whole genomes\n\n\n\n10.3.2 How does Yleaf compare to other tools?\nYleaf was tested on modern and ancient published datasets, and produced high concordance in NRY-SNP calling compared to well-established tools such as SAMtools/BCFtools (Li et al. 2009), and GATK HaplotypeCaller (McKenna et al. 2010). Yleaf called the lowest number of markers due to stringent options set, however it also appears to provide the highest overall accuracy\n\n\n10.3.3 How does Yleaf work?\n\n\n\n\n\n\nNote\n\n\n\nYleaf requires python 3.6 and currently only works on Linux\n\n\n\nYleaf works with both raw and aligned sequencing data in a single command\nIt accepts raw fastq files, bam, cram and vcf files\nThe batch function allows you to provide a path to a folder and it will pick up all the files sharing the same extension\nThe raw reads are aligned against the reference human genome (either hg19 or hg38) with minimap2 using default settings, which creates a SAM file. (There is also a multi-threading option -t)\nThe SAM file is converted to a BAM file with SAMtools, sorted and indexed.\n\n\n\n\n\n\n\nNote\n\n\n\nFor ancient DNA it is advised to rather supply Yleaf with your quality-filtered BAM files\n\n\n\nThe BAM file is filtered on reads only aligning to the Y-chromosome.\nSAMtools mpileup function creates a raw data file for variant calling.\nYleaf has a function embedded to perform variant calling using the generated pileup file\nAn output folder is created containing output files in the root path for each of the input files."
  },
  {
    "objectID": "yleaf.html#installation-and-first-use",
    "href": "yleaf.html#installation-and-first-use",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.4 Installation and first use",
    "text": "10.4 Installation and first use\nThe recommended method for installing Yleaf is with conda. The github page provides easy-to-follow instructions as follows:\n# On the command line, clone the repository:\ngit clone https://github.com/genid/Yleaf.git\ncd Yleaf\n\n# Create a conda environment called yleaf:\nconda env create --file environment_yleaf.yaml\nconda activate yleaf\n\n# Enable modification of config file in your cloned folder:\npip install --e\nManual installation instructions are also provided on the github page.\nYour folder will then look like this:\n\nOn first use Yleaf will download reference human genome files which you will find inside the directory data, shown above:\n\nAlternatively, you may add custom paths to your own references that you want to use in the config.txt file (shown in Fig.1 above), which looks like this:"
  },
  {
    "objectID": "yleaf.html#usage",
    "href": "yleaf.html#usage",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.5 Usage",
    "text": "10.5 Usage\nTo use, on your command line type:\nConda activate yleaf\nYleaf –h\nThis can be called from any directory on your system. The -h (help) flag will show you the various usage options available to adjust parameters such as quality thresholds.\n\n\n10.5.1 Options that are most relevant for ancient DNA analysis\n\n-r , --reads_threshold: The minimum number of reads for each base. (default=10)\n-q , --quality_thresh: Minimum quality for each read, integer between 10 and 40. (default=20)\n-b , --base_majority: The minimum percentage of a base result for acceptance, integer between 50 and 99.  (default=90) (i.e. 9 out of 10 reads need to agree with each other for the call to be accepted)\n-dh, --draw_haplogroups: Draw the predicted haplogroups in the haplogroup tree.\n-pq , --prediction_quality: stringent default of 0.95. Minimum quality of the prediction for it to be accepted (QC-scores)\n-old , --use_old: In version 3, Yleaf switched to using Yfull for the underlying tree structure of the haplogroups so predictions are slightly different to earlier versions that used ISOGG. You can use this flag to force it to use the previous method.\n-aDNA: recently added flag which ignores G &gt; A and C &gt; T mutations.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is recommended to still use Yleaf without the -aDNA option (or run it twice, with and without) so that you can manually check all mutations in case you wish to consider them in your haplogroup-calling (see further details below).\n\n\n\n\n10.5.2 Minimum usage examples\nThe recommended input for ancient DNA samples is to supply bam files that you have processed with your usual quality filtering. (This could either be a BAM file that contains only reads you have extracted that align to the Y-chromosome, or you can provide your original BAM file and Yleaf will do that for you):\n\n10.5.2.1 BAM input\nYleaf -bam file.bam -o bam_output --rg hg19\n\n\n10.5.2.2 BAM input with drawing predicted haplogroups in a tree and showing all private mutations\nYleaf -bam file.bam -o bam_output --rg hg19 -dh -p\n\n\n\n10.5.3 Optimising settings for ancient DNA data\n\nRead number threshold (-r): The default for this is set to 10 – Note that this should be changed to -r 1, as some SNPs, especially in low coverage genomes, are expected to only have a single read covering them. If you run Yleaf with the default setting, you will obtain a results file but no haplogroup assignment.\nBase quality threshold (-q): The default for this is set to 20, which would allow most bases to be accepted. You may however wish to raise this to -q 30.\n\nExamples:\n\n10.5.3.1 BAM input\nYleaf -bam filename.bam -o bam_output --rg hg19 --r 1 --q 30\n\n\n10.5.3.2 Batch option by providing path to files\nYleaf -bam /path/to/files -o bam_output --rg hg19 --r 1 --q 30"
  },
  {
    "objectID": "yleaf.html#output-and-post-processing",
    "href": "yleaf.html#output-and-post-processing",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.6 Output and post-processing",
    "text": "10.6 Output and post-processing\n\n10.6.1 The Log file\nAfter running Yleaf you will find a log file in your output directory detailing each process that was run and how long it took. As an example, here is the log file after running a single bam file:\n\n\n\n10.6.2 Yleaf Haplogroup prediction function\n\n\n\n\n\n\nWarning\n\n\n\nnot suitable for ancient DNA data – see below\n\n\nYou will also find the file Hg_prediction.hg, which is the final haplogroup prediction that Yleaf outputs, using the marker nomenclature from Yfull v10.01 (or if using the -old flag, ISOGG nomenclature), together with quality scores. The quality scores indicate whether the predicted haplogroup follows the expected backbone of the haplogroup tree structure and within-haplogroup tree structure. For further detailed information about these quality scores, please refer to the Yleaf documentation.\n\n\n\n\n\n\n\nNote\n\n\n\nThis file is discussed here for completeness, however it is not suitable for use with ancient DNA data – for ancient DNA it is essential to carry out manual inspection of the intermediate output files provided, particularly the .out and .fmf files, as explained further below. In addition, if using present-day data, it is nevertheless recommended to perform manual inspection as prediction is not perfect.  \n\n\nYou will also find a subdirectory which contains additional files:\n\nDepending on the flags you used when running Yleaf, you may also find here additional files such as .pmu files for private mutations (see below) or .dh files for graphical output of a haplogroup tree.\n\n\n10.6.3 Quality control files\nThe .chr file is useful for quality control and is informative about the number and percentage of mapped reads per chromosome (this comes from the output of the SAMtools command, idxstats).\n\nThe .info file provides general information such as total number of mapped and unmapped reads, number of markers that met the threshold or failed:\n\nThis file can be useful for finetuning your quality thresholds when running Yleaf. For example, this file would be helpful for identifying if you forgot to change the read number flag (-r) to 1.\n\n\n10.6.4 Manual inspection and haplotype calling of ancient DNA data using the .out file\nThe most relevant file for performing our own manual inspection and haplotype calling of our ancient DNA data is the .out file. The default output uses the Yfull haplogroup nomenclature (terminal SNP names). Here are the first few rows:\n\nThe column ‘reads’ refers to the number of reads after quality filtering, while the column ‘called_per’ refers to the percentage of reads that agree with the final base call, noted in column ‘called_base’. Under ‘state’, A refers to ancestral state (as in column ‘anc’), and D to derived state (column ‘der’), while ‘depth’ indicates the depth of the hit in the YFull(v10.01) tree.\nTo assign the most derived haplogroup in ancient DNA data, the following steps should be carried out, which you can of course run or automate using your own preferred method:\n\nSort the .out file on the “Haplogroup” column\nFilter on derived alleles (D) in the “State” column\nFilter out transition mutations (A-G and C-T mutations) in the “mutation” column for the most conservative approach to calling the Y-chromosome haplogroup. This is because transitions commonly represent ancient damage. However, since C-&gt;T and G-&gt;A mutations are very common and sometimes the only SNPs for many haplogroup branches, you may wish to consider them too. If your data is single-stranded, you can filter for C-&gt;T on the forward strand, and G-&gt;A on the reverse strand.\n\nAfter these steps, you can inspect the most derived SNP with the highest depth, which you will find towards the bottom of the table. Here are the last few rows of the sorted and filtered table. In this example, the data comes from double-stranded, non-UDG-treated libraries, and an approach was taken to filter on transversion-only SNPs:\n\nIn this example, we find many positions supporting a G haplogroup, with the highest depth of 23 for the haplogroup G-Y140827 according to the Yfull nomenclature (highlighted in green). (More commonly I would only find one highest-depth call however here we have three). This call is supported by additional upstream markers for the G haplogroup. If there are no other derived markers directly upstream of the most derived haplogroup, such as we see in the last four rows of this table, it suggests a spurious call due to, for example, a sequencing error, contamination, or a private mutation, and therefore you should look instead for the most derived SNP that also has upstream markers along the same branch.\nNext, you can visit the website of Yfull (https://www.yfull.com/tree/G/) to find the path of Y-chromosome SNPs back to the root, and check if the derived SNPs make sense (this is what the automated haplogroup prediction does).\n\nYou can also visit the ISOGG website (https://isogg.org/tree/ ) and find the haplogroup name according to the ISOGG nomenclature. You can search the page by position or marker name from your output table. For example, the first marker name listed for the highest depth haplogroup is Z31430, which tells us it is haplogroup G2a2a1a2a2a1a:\n\nIf we were to search for Z6778, we would find it listed under the slightly less derived G2a2a1a2a2a. We do not find Y140827 in ISOGG, most likely because it is a more recently confirmed marker that is not included in the current version. Haplogroup names change as the tree keeps growing so it is worth looking up the most derived marker in the latest version and use the latest nomenclature. There may also be new downstream markers that are not yet included in Yleaf.\nIf you use the -old flag when running Yleaf, this outputs ISOGG haplogroup nomenclature as seen in the fourth column of the table below, which shows the last few rows (it may be slightly different from the Yfull results):\n\nThis way it is simpler to follow ISOGG’s “path” to see you are on the right track. Here we can see straight away that the most derived haplogroup (after dismissing the last three spurious rows)  is G2a2a1a2a2a1 with upstream markers that lead to it. Yfull does not use such a nomenclature system because the ISOGG nomenclature could change over time with the discovery of new SNPs, and it has not been updated in a while, so using the terminal SNP names as in Yfull offers more continuity moving forward.\n\n\n\n\n\n\nNote\n\n\n\nWhere you have few markers supporting a call due to the low coverage of your data, you have the option to be conservative and report a slightly more upstream haplogroup, for example, G2, or G2a2a1a2 if you prefer.\n\n\n\n\n10.6.5 Troubleshooting\nAn additional file, the .fmf file, is an extended .out file which includes an additional column “description”. This provides information about why a marker did not pass the criteria for haplogroup assignment, for example due to low coverage or being below the base calling threshold. This file can therefore be used to troubleshoot and optimize the settings, for example if you see that many markers were filtered out. If many markers fall below the base majority threshold, it can indicate a contaminated sample, and it could even be possible to identify the haplogroups of both contributors using the .fmf file.\n\n\n\n10.6.6 Private mutations\nOne useful flag is the -p flag, which produces a .pmu file. This lists all identified private mutations with a minor allele frequency below 1% (this threshold can be changed with the -mr flag). It works by ignoring all SNPs that are included in Yfull and therefore associated with a haplogroup, and outputs all other SNPs found in the data. If the position is included in dbSNP (https://www.ncbi.nlm.nih.gov/snp/) it will also output its reference number (rn_no) and population frequency. This can be useful for identifying new variants in understudied populations or identifying subgroups among individuals who share the same haplogroup."
  },
  {
    "objectID": "yleaf.html#follow-up-steps-for-further-verification",
    "href": "yleaf.html#follow-up-steps-for-further-verification",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.7 Follow-up steps for further verification",
    "text": "10.7 Follow-up steps for further verification\nFor questionable calls, it can be helpful to use a graphical interface such as IGV: Integrative Genomics Viewer. Here you can load the bam and bai files (upload them via the “Tracks” option in the menu bar), ensuring you have selected the appropriate reference genome that you used for alignment (you can change it under the “Genome” option in the menu bar), and navigate to the position of the SNP along the read. Let’s say I want to take a closer look at this position from our output table:\n\nI can use the search field to navigate to position chrY: 7682648 and zoom in.\n\nThis shows us there is one read covering this position, which has a G-&gt;C mutation, corresponding with what the output table states. We see that this SNP falls well inside the middle of the read. SNPs in the middle of a read can be interpreted as more reliable than ones at the ends of reads, which could be a result of ancient DNA damage. In such cases, especially where your data might be limited to transitions, you can check where in the read they are and if you choose, take a more conservative approach by discarding a questionable result, and instead report the haplogroup associated with the next upstream marker. Visualising the results this way can also be helpful in cases where you have two related males with slightly differing haplogroups along the same branch - in this way you can easily visualise if one of them simply lacks reads covering the more derived position that the other individual has."
  },
  {
    "objectID": "yleaf.html#acknowledgements",
    "href": "yleaf.html#acknowledgements",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.8 Acknowledgements",
    "text": "10.8 Acknowledgements\nThe author thanks Ralf Arwin and Dion Zandstra for helpful comments and clarifications."
  },
  {
    "objectID": "yleaf.html#data-availability",
    "href": "yleaf.html#data-availability",
    "title": "10  Y-chromosomal haplogroup inference with Yleaf",
    "section": "10.9 Data availability",
    "text": "10.9 Data availability\nThe bam file used in this chapter was analysed as part of Freilich, Suzanne, et al. “Reconstructing genetic histories and social organisation in Neolithic and Bronze Age Croatia.” Scientific Reports 11.1 (2021): 16729 (Freilich et al. 2021), and is available from the European Nucleotide Archive ENA, under run accession ERR6324186, study accession number PRJEB46357.\n\n\n\n\nFreilich, Suzanne, Harald Ringbauer, Dženi Los, Mario Novak, Dinko Tresić Pavičić, Stephan Schiffels, and Ron Pinhasi. 2021. “Reconstructing Genetic Histories and Social Organisation in Neolithic and Bronze Age Croatia.” Scientific Reports 11 (1): 1–16. https://doi.org/10.1038/s41598-021-94932-9.\n\n\nLi, Heng, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, Richard Durbin, and 1000 Genome Project Data Processing Subgroup. 2009. “The Sequence Alignment/Map Format and SAMtools.” Bioinformatics 25 (16): 2078–79. https://doi.org/10.1093/bioinformatics/btp352.\n\n\nMcKenna, Aaron, Matthew Hanna, Eric Banks, Andrey Sivachenko, Kristian Cibulskis, Andrew Kernytsky, Kiran Garimella, et al. 2010. “The Genome Analysis Toolkit: A MapReduce Framework for Analyzing Next-Generation DNA Sequencing Data.” Genome Research 20 (9): 1297–1303. https://doi.org/10.1101/gr.107524.110.\n\n\nRalf, Arwin, Diego Montiel González, Kaiyin Zhong, and Manfred Kayser. 2018. “Yleaf: Software for Human Y-Chromosomal Haplogroup Inference from Next-Generation Sequencing Data.” Molecular Biology and Evolution 35 (7): 1820. https://doi.org/10.1093/molbev/msy080."
  },
  {
    "objectID": "admixfrog.html#archaic-ancestry",
    "href": "admixfrog.html#archaic-ancestry",
    "title": "11  Detection of archaic ancestry segments using admixfrog",
    "section": "11.1 Archaic ancestry",
    "text": "11.1 Archaic ancestry\nNeandertals and Denisovans lived in Europe and Asia for hundreds of thousands of years before their disappearance around 40 thousand years ago (kya) (Higham et al. 2014). In the last few thousand years of their documented existence, they met and interbred with modern humans who arrived from Africa, and as a result, 2-3% of the ancestry of present-day non-Africans derives from Neandertals and between 0.2 to 4% from Denisovans ((Green et al. 2010),(Reich et al. 2010),(Meyer et al. 2012),(Prüfer et al. 2017), (Hajdinjak et al. 2021)).\n\nHigham, Tom, Katerina Douka, Rachel Wood, Christopher Bronk Ramsey, Fiona Brock, Laura Basell, Marta Camps, et al. 2014. “The Timing and Spatiotemporal Patterning of Neanderthal Disappearance.” Nature 512 (7514): 306–9. https://doi.org/10.1038/nature13621.\n\nGreen, Richard E., Johannes Krause, Adrian W. Briggs, Tomislav Maricic, Udo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft Sequence of the Neandertal Genome.” Science 328 (5979): 710. https://doi.org/10.1126/science.1188021.\n\nReich, David, Richard E. Green, Martin Kircher, Johannes Krause, Nick Patterson, Eric Y. Durand, Bence Viola, et al. 2010. “Genetic History of an Archaic Hominin Group from Denisova Cave in Siberia.” Nature 468 (December): 1053. https://doi.org/10.1038/nature09710.\n\nMeyer, Matthias, Martin Kircher, Marie-Theres Gansauge, Heng Li, Fernando Racimo, Swapan Mallick, Joshua G. Schraiber, et al. 2012. “A High-Coverage Genome Sequence from an Archaic Denisovan Individual.” Science (New York, N.Y.) 338 (6104): 222–26. https://doi.org/10.1126/science.1224344.\n\nPrüfer, Kay, Cesare de Filippo, Steffi Grote, Fabrizio Mafessoni, Petra Korlević, Mateja Hajdinjak, Benjamin Vernot, et al. 2017. “A High-Coverage Neandertal Genome from Vindija Cave in Croatia.” Science 358 (6363): 655–58. https://doi.org/10.1126/science.aao1887.\n\nHajdinjak, Mateja, Fabrizio Mafessoni, Laurits Skov, Benjamin Vernot, Alexander Hübner, Qiaomei Fu, Elena Essel, et al. 2021. “Initial Upper Palaeolithic Humans in Europe Had Recent Neanderthal Ancestry.” Nature 592 (7853): 253–57. https://doi.org/10.1038/s41586-021-03335-3.\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and F-Statistics.” Genetics 202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913.\nGenome-wide average archaic ancestry can be detected using F-statistics or D-statistics ((Peter 2016)). However, for more detailed investigation of the archaic ancestry in the modern human genomes, segment calling is preferred.\n\n11.1.0.1 What can we learn from the segments?\n\nNumber and duration of the introgression event(s)\nGenerations past since the introgression event\nIf the archaic segments in different individuals result from the same introgression event or not (Iasi et al. 2024)\nSelection, functional consequences..\n\n\nIasi, Leonardo N. M., Manjusha Chintalapati, Laurits Skov, Alba Bossoms Mesa, Mateja Hajdinjak, Benjamin M. Peter, and Priya Moorjani. 2024. “Neandertal Ancestry Through Time: Insights from Genomes of Ancient and Present-Day Humans.” http://dx.doi.org/10.1101/2024.05.13.593955.\n\nSkov, Laurits, Ruoyun Hui, Vladimir Shchur, Asger Hobolth, Aylwyn Scally, Mikkel Heide Schierup, and Richard Durbin. 2018. “Detecting Archaic Introgression Using an Unadmixed Outgroup.” Edited by Fernando Racimo. PLOS Genetics 14 (9): e1007641. https://doi.org/10.1371/journal.pgen.1007641.\n\nPrüfer, Kay, Cosimo Posth, He Yu, Alexander Stoessel, Maria A. Spyrou, Thibaut Deviese, Marco Mattonai, et al. 2021. “A Genome Sequence from a Modern Human Skull over 45,000 Years Old from Zlatý kůň in Czechia.” Nature Ecology & Evolution 5 (6): 820–25. https://doi.org/10.1038/s41559-021-01443-x.\n\n———. 2020. “100,000 Years of Gene Flow Between Neandertals and Denisovans in the Altai Mountains.” http://dx.doi.org/10.1101/2020.03.13.990523.\nHidden Markov Models (HMMs) are used for calling the archaic segments, and there are various methods including hmmix which is reference free method that currently do not work on low coverage data (Skov et al. 2018), and other HMMs which can detect segments using genotype calls (again, not suitable for low coverage data) (Prüfer et al. 2021). In this chapter, we will focus on admixfrog (Peter 2020), as it can call segments in low and contaminated data."
  },
  {
    "objectID": "admixfrog.html#calling-archaic-segments-with-admixfrog",
    "href": "admixfrog.html#calling-archaic-segments-with-admixfrog",
    "title": "11  Detection of archaic ancestry segments using admixfrog",
    "section": "11.2 Calling archaic segments with admixfrog",
    "text": "11.2 Calling archaic segments with admixfrog\nAdmixfrog can call segments of archaic ancestry from unphased nuclear genomes with coverage as low as 0.2x, both from shotgun and capture data. It is well tested on not only detecting Neandertal/Denisovan ancestry in modern human genomes, but also for detecting Denisovan ancestry in Neandertal genomes. Admixfrog is a HMM with hidden states being all possible pairwise combinations of ancestry between user defined source populations. The method does not rely on simulations and model assumptions much, and uses the data to estimate parameters with a flexible Bayes model. It requires two main input files:\n\nTarget genome that belongs to the individual one wants to detect archaic ancestry in. This file can be created from a bam file using the command admixfrog-bam. Alternatively, if contamination and error should not be estimated, it is possible to use genotypes provided in eigenstrat-format. It contains observed number of alternative and reference reads, and is stratified by read length, library ID, and presence/absence of deamination on the read.\nReference file that includes the high coverage genomes of the “sources” admixfrog uses to model the ancestries in the target genome. This file can be created from vcf files using the command admixfrog-ref. Reference file is very important and should be generated carefully. If capture data is being used for the “target”, reference file should also be ascertained with the same capture sites. The assumption of admixfrog is that the source genomes are unadmixed. It contains the counts of alternative and reference alleles for all reference populations, and specifies the genetic map that is used.\n\nWhen these two input files are provided and various parameters (such as bin size, states, recombination map, run penalty, error rate..) are specified, admixfrog splits the target genome into bins of a given size and estimates the probability of (homozygous or heterozygous) reference ancestry (combination) for each bin. It does that from the genotypes in the bin by estimating the probability of observing the target genotypes given a reference population. If genotypes are not available, admixfrog can estimate the genotype likelihood from the read date, while taking contamination into account. For this, it uses a genotype likelihood model :\n\\(P(O_r|G, c_r, n_r, e_r, p^c) ~ Binom(O_r; n_r, p)\\)\nwhere \\(p = (1-e_r)p'+e_r(1-p')\\) and \\(p'=c_rp_c + (1-c_r)G\\).\n\n\n\nFigure 1: Admixfrog segment calling and genotype likelihood models.\n\n\nThe contamination is estimated from the observed allele on the reads (\\(O_r\\)). The reads are clustered into read groups (\\(r\\) with \\(n_r\\) reads in the read group) which stem from a certain sequence library, with a given length and presence or absence of contamination. From this the proportion of contamination (\\(c_r\\)) and error (\\(e_r\\)) is estimated. The basic idea here is that if there is no contamination \\(,p\\) the probability of seeing a derived allele, should be the same across all read groups for a certain SNP. If there is, however, contamination and error it will lead to variation between read groups from, which the error and contamination parameter can be estimated.\n\n11.2.0.1 Installing admixfrog\nAdmixfrog requires python version 3.8 or higher, along with several packages. Dependencies can be installed with pip install cython scipy --upgrade and admixfrog can be installed from github with pip install git+https://github.com/benjaminpeter/admixfrog.\nMore information can be found in the github page, installation explained in: https://github.com/BenjaminPeter/admixfrog/tree/master.\n\n\n11.2.0.2 Preparing input files\n\nTarget file\n\nIf you have called trustworthy genotypes for your target sample, you can use the VCF and create the input file:\nadmixfrog-bam --ref ref_example.xz --vcfgt {input.vcf} --target {sample_name}\"\n  --force-target-file \n  --out {output.csv} \nIf bam file is being used, make sure the RG tag in the bam file is specifying the correct library the read came from (quite often not the case). Also if library is fully UDG treated it might me difficult to determine deaminations:\nadmixfrog-bam --bam ${target}.bam --ref ref_example.xz --out ${target}.in.xz\nThis file should looks something like this:\nchrom  pos        lib                         tref  talt  tdeam  tother\n1      847983     Lib.L.9150_0_nodeam         1     0     0      0\n1      851309     Lib.L.9150_1_nodeam         1     0     0      0\n1      853596     Lib.K.4356_1_nodeam         1     0     0      0\n1      853962     Lib.K.4356_2_nodeam         1     0     0      0\n1      858641     Lib.L.9150_0_deam           1     0     0      0\n1      859871     Lib.L.9150_0_deam           1     0     0      0\n1      867151     Lib.L.9150_0_nodeam         1     0     0      0\n1      867404     Lib.L.9150_4_deam           1     0     0      0\n1      872687     Lib.K.4356_0_nodeam         1     0     0      0\n1      873541     Lib.K.4356_2_deam           1     0     0      0\n...\n\nReference file\n\nThis file contains the population allele counts for the reference and alternative allele, for the reference populations the target genome is ‘painted’ with. This requires high quality genomes for accurate allele frequencies, especially if the number of individuals per reference is very small or even just a single individual. The closer the reference population is to the introgression population the better. If reference populations are themselves admixed (with one of the other reference populations), as many individuals as possible should be provided for a good inference. Usually only ancestry informative sites are used (sites where the are substantial frequency differences between reference populations or even differentially fixed sites). This also increases the speed of admixfrog.\nFor each site, the reference file contains the chromosome, physical position and at least one genetic map giving the genetic distance between sites. Using the command presented here the reference file is constructed from a single vcf file containing all reference individuals. The individuals are specified in a .yaml file with their names being the same as in the vcf file, like this:\nAFR:\n    - S_Yoruba-1.DG\n    - S_Yoruba-2.DG\n    - B_Yoruba-3.DG\n    - S_Yoruba-1.DG\nNEA:\n    - Altai_snpAD.DG\n    - Vindija_snpAD.DG\nDEN:\n    - Denisova_snpAD.DG\nthe rec file specifies the genetic map to be used in the form of the AA Map, with tab separated columns being: Physical_Pos and Name of genetic map (column has the genetic distances in cM).\nadmixfrog-ref --vcf x_{CHROM}.vcf.gz --out x.ref.xz  \\\n    --states AFR VIN=Vindija33.19 DEN=Denisova \\\n    --pop-file data.yaml \\\n    --rec-file rec.{CHROM}\nThis file should look something like this:\nchrom  pos        ref  alt  map        AA_Map     deCODE     YRI_LD     CEU_LD     AFK_ref  AFK_alt  AFR_ref  AFR_alt  ALT_ref  ALT_alt  ARC_ref  ARC_alt  CHA_ref  CHA_alt  D11_ref  D11_alt  DEN_ref  DEN_alt  EAS_ref  EAS_alt  EUR_ref  EUR_alt \n1      812425     G    A    0.00000    0.00000    0.00000    0.30581    0.16677    414      0        80       0        1        1        7        1        2        0        1        0        2        0        94       0        148      0       \n1      812751     T    C    0.00000    0.00000    0.00000    0.30810    0.16771    116      298      0        0        2        0        6        2        2        0        1        0        0        2        0        0        0        0       \n1      813034     A    G    0.00000    0.00000    0.00000    0.31009    0.16853    383      31       0        0        0        2        3        5        0        2        0        1        2        0        0        0        0        0       \n1      834198     T    C    0.00000    0.00000    0.00000    0.45887    0.22962    394      20       73       3        0        2        2        6        0        2        0        1        2        0        73       13       123      21      \n1      834360     C    T    0.00000    0.00000    0.00000    0.46001    0.23009    414      0        80       0        1        1        4        4        0        2        0        1        2        0        94       0        148      0       \n1      837238     G    A    0.00000    0.00000    0.00000    0.48024    0.23840    414      0        72       0        2        0        6        2        2        0        0        0        0        2        88       0        124      0       \n1      845938     G    A    0.00000    0.00000    0.00000    0.54139    0.26349    178      236      41       27       2        0        6        2        2        0        0        0        0        2        61       19       98       24      \n1      846687     C    T    0.00000    0.00000    0.00000    0.54665    0.26565    402      12       66       2        2        0        8        0        2        0        1        0        2        0        78       0        138      0       \n1      847041     C    T    0.00000    0.00000    0.00000    0.54790    0.26625    391      23       64       4        2        0        6        2        2        0        0        0        0        2        72       0        118      0       \n1      847491     G    A    0.00000    0.00000    0.00000    0.54790    0.26638    260      154      50       26       2        0        6        2        2        0        1        0        0        2        80       8        107      27      \n...\n\n\n11.2.0.3 Running admixfrog\nMost of the time the default parameter should work fine. Make sure to specify all reference populations you want to use in the –states parameter. The reference population from which you think the contamination stems from is specified with the –cont-id parameter (usually a modern human population, can also be one of the states e.g. AFR). The –ancestral parameter specify the reference population used to polarize the alleles. If not specified the ancestral states are unknown. With the –filter-ancestral you can filter all alleles where that do not have any ancestral information.\nadmixfrog --infile ${target}.in.xz --ref ref_ascertainment.csv.xz -o ${target_output} \\\n--states AFR NEA DEN --cont-id EUR --ll-tol 1e-2 --bin-size 5000 \\\n--est-F --est-tau --freq-F 1 --freq-contamination 3 --e0 1e-2 --est-error \\\n--ancestral PAN --run-penalty 0.1 --max-iter 250 --n-post-replicates 200 \\\n--filter-pos 50 --filter-map 0.000 --init-guess AFR --map-column AA_Map\nAdmixfrog has many optional parameters that are not used in the command above, and most up to date parameter descriptions can be accessed with the command:\nadmixfrog --help\nNames provided for the reference genomes after the –states should be specified as they are in the reference input file. It is possible to provide only two, in case Denisovan ancestry is not expected or relevant. Ancestral state needs to specified also with –ancestral, and this is always the chimpanzee genome (PAN).\nIn the above example, we are trying to detect archaic ancestry in a modern human genome. That is why –init-guess is set to AFR. If we were trying to detect modern human ancestry in the genome of a Neandertal, –init-guess should have been set to NEA. Population name specified after –cont-id stands for the most likely source for the present-day human contamination in the sample, and also should be named as it is specified in the reference file. Optional parameter –ll-tol stops EM when Delta log likelihood is less than ll-tol specified. Bin size is controlled by the value provided after –bin-size. By default, this is given as 1e-8 cM, so that the unit is approximately the same for runs on physical / map positions.\nAdmixfrog can estimate several parameters if wanted. These are\n\nF:  Distance from reference (estimated if –est-F given). A related parameter is –freq-F that specifies the frequency of updating the estimate of F (e.g. in how many iterations it should be updated).\ntau: population structure in references and is estimated if –est-tau is included. Initial value of tau is 0 by default.\nerror: Sequencing error can be estimated per read group, if –est-error is a parameter. Value given after –e0 parameter determines the initial error rate.\n\nValue provided after the parameter –max-iter specifies the maximum number of iterations. –n-post-replicates is important for the estimation of parameters listed above, and controls the number of replicates that will be sampled from the posterior. The distance between the positions is controlled by –filter-pos. Value provided after this parameter is the number of bases there should be between the positions. Value of –filter-map determines the distance between the positions based on the recombination map provided. Recombination map preferred should be specified with –map-column. Name of the recombination map given after should match the column name in the reference file.\nRun penalty value provided after –run-penalty determines how likely nearby segments are joined or broken up. If 0.1, next bin should have at least &gt; 0.9 posterior probability to continue an archaic segment. 0.25 expects at least &gt; 0.75 posterior probability etc.\n\n\n\nFigure 2: Run penalty example.\n\n\n\n\n11.2.0.4 Output files\nThere are 6 output files admixfrog can (and will) produce and they are all compressed with LZMA. They can be looked at with\nxzless ${output}.xz | column -s, -t | less -S\nWe present the 10 top rows of each output file as an example below. Input files provided for this run were resritced and ascertained with the sites on the “Archaic admixture array” (Fu et al. 2015), as we use this capture array to obtain data from the sites best represent the diversity in the archaic genomes.\n\nFu, Qiaomei, Mateja Hajdinjak, Oana Teodora Moldovan, Silviu Constantin, Swapan Mallick, Pontus Skoglund, Nick Patterson, et al. 2015. “An Early Modern Human from Romania with a Recent Neanderthal Ancestor.” Nature 524 (7564): 216–19. https://doi.org/10.1038/nature14558.\n\n${output}.cont.xz : contamination estimates for each read group (library and deamination status) but only if the bam file is provided as the input. This output file contains the contamination estimates per read group, along with the error proportion and the number of reads in the read group. The overall contamination proportion estimate can be computed by taking the weighted average of the contamination proportion per read group weighted by the number of reads in that group.\n\nrg                          cont      error     lib                len_bin  deam    n_reads  tot_n_snps\nLib.L.9150_0_nodeam         0.006364  0.011476  Lib.L.9150         0        nodeam  140015   1333880\nLib.K.4356_2_nodeam         0.060802  0.015108  Lib.K.4356         2        nodeam  195136   1333880\nLib.L.9150_1_nodeam         0.033591  0.014670  Lib.L.9150         1        nodeam  118054   1333880\nLib.K.4356_0_deam           0.016506  0.021292  Lib.K.4356         0        deam    117600   1333880\nLib.L.9150_0_deam           0.045305  0.020753  Lib.L.9150         0        deam    78027    1333880\nLib.L.9150_3_nodeam         0.070068  0.013863  Lib.L.9150         3        nodeam  20157    1333880\nLib.K.4356_2_deam           0.044722  0.020644  Lib.K.4356         2        deam    87877    1333880\nLib.K.4356_0_nodeam         0.001356  0.010723  Lib.K.4356         0        nodeam  200039   1333880\nLib.L.9150_1_deam           0.049495  0.021663  Lib.L.9150         1        deam    65808    1333880\nLib.L.9150_2_nodeam         0.014923  0.017070  Lib.L.9150         2        nodeam  53843    1333880\n\n${output}.bin.xz : posterior decoding for each bin along the genome. This output file contains information of the posterior probability of all possible ancestries of a genomic bin (sum to 1), the number of SNPs in the bin, the maximum likelihood ancestry (not used very often) and if the bin is haploid or not (usually False unless phased data is used).\n\nchrom  map         pos        id      haploid  viterbi  n_snps  AFR       NEA       DEN       AFRNEA    AFRDEN    NEADEN\n1      0.000000    847983     0       False    AFR      13      0.999923  0.000000  0.000000  0.000077  0.000000  0.000000\n1      0.005000    1530722    1       False    AFR      0       0.997923  0.000000  0.000066  0.000065  0.001793  0.000153\n1      0.010000    1570398    2       False    AFR      0       0.997395  0.000000  0.000092  0.000051  0.002246  0.000217\n1      0.015000    1610074    3       False    AFR      0       0.997410  0.000000  0.000092  0.000035  0.002246  0.000217\n1      0.020000    1649751    4       False    AFR      0       0.997969  0.000000  0.000066  0.000018  0.001793  0.000154\n1      0.025000    1689427    5       False    AFR      23      1.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n1      0.030000    1874494    6       False    AFR      6       0.999138  0.000000  0.000000  0.000001  0.000852  0.000009\n1      0.035000    1891627    7       False    AFR      1       0.998529  0.000000  0.000033  0.000001  0.001409  0.000028\n1      0.040000    1898648    8       False    AFR      7       0.999991  0.000000  0.000000  0.000000  0.000009  0.000000\n1      0.045000    1907155    9       False    AFR      3       0.999497  0.000000  0.000003  0.000001  0.000493  0.000006\n\n${output}.snp.xz : posterior genotype likelihoods for each SNP, taking contamination into account. This output file is only produced if a bam file is used as the input file. It contains the observed counts of alternative and reference alleles per SNP. The likelihood of the 3 genotypes (contamination and error corrected) are given in the provided in the form of log likelihoods (the bigger the better) as: G0 = homozygous reference, G1 = heterozygous and G2 = homozygous alternative. p is the probability of an alternative allele.\n\nsnp_id  tref  talt  chrom  pos        map         G0          G1         G2          p         random_read  bin\n0       1     0     1      847983     0.000000    -0.000068   -5.572808  -7.777585   0.000001  0            0\n1       1     0     1      1191389    0.000100    -0.000068   -5.549067  -7.154152   0.000001  0            0\n2       1     0     1      1323257    0.000126    -0.000068   -5.535249  -6.988727   0.000002  0            0\n3       2     0     1      1324797    0.000137    -0.000068   -5.845946  -8.770707   0.000001  0            0\n4       1     0     1      1329665    0.000173    -0.000075   -4.771963  -6.432002   0.000009  0            0\n5       1     0     1      1330931    0.000182    -0.000068   -5.565269  -7.451757   0.000001  0            0\n6       1     0     1      1333290    0.000199    -0.000068   -5.561073  -7.349369   0.000001  0            0\n7       1     0     1      1485659    0.000987    -0.000068   -5.553857  -7.213933   0.000001  0            0\n8       2     0     1      1491054    0.001129    -0.000068   -5.840065  -8.857775   0.000001  0            0\n9       3     0     1      1501297    0.001806    -0.000067   -6.127400  -10.322657  0.000000  0            0\n10      2     0     1      1502866    0.001843    -0.000068   -5.823670  -8.342186   0.000001  0            0\n11      3     0     1      1503581    0.001860    -0.000067   -6.128254  -10.415260  0.000000  0            0\n\n${output}.pars.yaml : parameter estimates\n${output}.rle.xz : called runs of ancestry. This is the output file we usually use for plotting the called segments, and it includes the start and end position (in bp or cM) of a segment of a certain ancestry. The “type” column could have het, homo, or state as values. This indicates either the segment is homozygous or heterozygous, or irrelevant from the ploidy, the state of the segment is what it is given in the column before. One should use either the het/homo, or state, as state will include the previous two.\n\n\n\nFigure 3: Naming of the called segments in the type column.\n\n\n\nchrom  start   end     score        target  type   map         pos        id      map_end     pos_end    id_end  len    map_len     pos_len    nscore\n1      7352    8455    99.127010    AFRNEA  het    36.760000   19184015   7352    42.275000   22643831   8455    1103   5.515000    3459816    0.089870\n1      47205   47979   72.983646    AFRNEA  het    236.025000  217356752  47205   239.895000  221257830  47979   774    3.870000    3901078    0.094294\n1      14808   15393   55.264982    AFRNEA  het    74.040000   48226810   14808   76.965000   54369374   15393   585    2.925000    6142564    0.094470\n1      50930   51464   49.448453    AFRNEA  het    254.650000  233984167  50930   257.320000  234906868  51464   534    2.670000    922701     0.092600\n1      49458   49854   37.247516    AFRNEA  het    247.290000  230142184  49458   249.270000  231273184  49854   396    1.980000    1131000    0.094059\n1      543     956     34.198481    AFRNEA  het    2.715000    3467027    543     4.780000    4089283    956     413    2.065000    622256     0.082805\n1      44726   45217   30.486520    AFRNEA  het    223.630000  208767924  44726   226.085000  210563695  45217   491    2.455000    1795771    0.062091\n1      54433   54727   26.948920    AFRNEA  het    272.165000  241326256  54433   273.635000  241810161  54727   294    1.470000    483905     0.091663\n1      45717   45886   15.501402    AFRNEA  het    228.585000  212806999  45717   229.430000  213596175  45886   169    0.845000    789176     0.091724\n1      48255   48389   10.877327    AFRNEA  het    241.275000  223011019  48255   241.945000  223649605  48389   134    0.670000    638586     0.081174\n\n${output}.res.xz : simulated runs of ancestry. Last column indicates the iteration. A summary of this file can be found in another output file, names .res2.xz where ancestry and its overall estimated proportion throughout the genome is given with maximum and minimum estimates. However, the estimates in .res2.xz file also contains the ILS. Overall ancestry proportion can be calculated form the .res.xz file as described in the next section.\n\nstate  chrom  start  end    len    it\nAFR    1      0      499    499    0\nAFR    1      0      539    539    0\nAFR    1      500    1364   864    0\nAFR    1      1366   1443   77     0\nAFR    1      958    1512   554    0\nAFR    1      1570   1666   96     0\nAFR    1      1667   1937   270    0\nAFR    1      1938   2417   479    0\nAFR    1      2419   2601   182    0\nAFR    1      1487   2905   1418   0\n\n\n11.2.0.5 Filtering and checking the output files\nIt is important to make sure the segments detected by admixfrog are real segments. Incomplete lineage sorting (ILS) might be incorrectly detected as an introgressed segment. In most publications only segments with a minimum length of 0.2 cM are retained for very ancient individuals (such as early modern humans) and 0.05 cM for more recent ancient and present day humans.\nOverall Neandertal ancestry can be calculated from the ${output}.res.xz file after filtering this file for ILS as following:\nres=read.csv(\"output.res.xz\")\nres_len_it = res %&gt;% filter(state=='NEA', len&gt;=40) \n# len 10 for 0.05 cM and 40 for 0.2 cM\n\n# filtering out the ILS in the called Neandertal segments\nres_Nead_sumlen = res_len_it %&gt;% group_by(it) %&gt;% mutate(sum_len_NEA=sum(len)) \nres_Nead_sumlen = res_Nead_sumlen %&gt;% summarize(it, sum_len_NEA) %&gt;% filter(row_number()==1)\n\n# filtering out the ILS in all called segments\nres_all_sumlen = res %&gt;% filter(len&gt;=40) %&gt;% group_by(it) %&gt;% mutate(sum_len_ALL=sum(len))\nres_all_sumlen = res_all_sumlen %&gt;% summarize(it, sum_len_ALL) %&gt;% filter(row_number()==1)\n\n# calculating the ratio of Neandertal vs. all segments, per iteration\n\nres_cat = left_join(res_Nead_sumlen, res_all_sumlen) %&gt;%\n  mutate(nea_all_ratio = sum_len_NEA/sum_len_ALL, specimen = \"Specimen_name\") \n\n# maximum and minimum estimates from all iterations: \nmax(res_cat$nea_all_ratio) # 0.0312594\nmin(res_cat$nea_all_ratio) # 0.02978274\nmean(res_cat$nea_all_ratio) # 0.0303691\n\n\n11.2.0.6 Limitations of admixfrog\n\nShort segments might not be called or called with high false positive rate.\nRun penalty, which determines how often the segments are broken down, needs to be provided by the used. A run penalty that is too conservative (0.1) could break the real segments while a run penalty that is not conservative enough (0.4) could merge separate segments together. The best penalty to use depends on the age of the target genome, and the expected length of the segments. Best practice currently is running admixfrog with different run penalties and comparing the results. Alternative ways of merging/breaking segments is currently being developed.\nFor the genomes of relatively recent individuals, segment calling results depend a lot on the recombination map used. For older specimens (e.g. early modern humans) the effect of recombination map is small. Best practice currently is running admixfrog with different recombination maps and comparing the results.\nBetter assignment for deeply divergent references, drop in performance when the references are not deeply diverged. Can result in possible missasignment of ancestries as seen sometimes with Denisovan segments are being called as Neandertal in present day populations.\nBetter assignment when reference and actual source population are close. For example, the Denisovan populations that introgressed into modern humans are not well represented by the Denisova3 high-coverage genome that is used as the reference representing the Denisovans. This makes the detection of the Denisovan segments challenging."
  },
  {
    "objectID": "quality_control.html",
    "href": "quality_control.html",
    "title": "12  Quality Control",
    "section": "",
    "text": "Your data has made it through the sequencer, now you want to know: am I good?\nThere are several ways to assess if you got what you payed for, and if your hopes to have a project with this data aren’t crushed. This chapter will walk you through how to check your sequencing went well, assess the quality of your data, what you can do if something is off, and how to decide what to do next.\nYour data has been sequenced, and you have run nf-core/eager to get the statistics. Now you can use the information to have a quick check if everything went well, and decide how to continue. First, let’s assess if everything went well in the sequencing. Runs can fail because the flow cells are faulty, expired or human mistakes were made in setting up the sequencing run. Severe problems affecting an entire run are usually already caught by the lab, but sometimes minor problems/mistakes affect single samples, and they can affect your downstream analysis/assessment of the statistics.\nIn your sequencing request, you will have asked for a certain amount of reads to be sequenced (i.e. 5-10 M reads for shotgun sequencing, 40M for Capture, 100M for shotgun genomes). This is the amount of reads you should expect from the “Nr. Input Reads” column (the number of reads that was assigned to your sample in demultiplexing the data based on the unique index combination).\nIn most cases, you should get the requested amounts of reads (summed up over the lanes if the pool was distributed over many), with a little bit more or less, depending on the pipetting accuracy.\n\n\n\n\n\nHowever, if you have much less, then you need to trouble shoot. The lack of reads assigned to your library can be a wet-lab problem, or a dry-lab problem. When loading the sequencer, the amount of reads requested is controlled by the pool of samples it is loaded with. If you have much less than what you payed for, or your sample is not among the ones listed in the run, you should check the “unknown.txt” in the folder containing the raw data. Sometimes, typos in the index combination, and your sample was not properly demultiplexed. In this case, check if the unknowns contain sequences with an index combination that is suspiciously close to the one recorded for your library. If this is the case, ask the lab to correct the wrong entry and see that your data is demultiplexed again. If your sample is not among the unknowns, you should reach out to the lab and let them know. The sample migh thave failed because it was not pooled for the sequencing, or the capture might have failed, so there is no data there and the experiments have to be repeated. Sometimes, the wrong sample is drawn for a capture plate, and instead of your library, another lucky person gets some free extra data.\nIf (close to) all the reads you requested are there, you can go check if those reads are of sound quality. One aspect is the sequencing Quality, another the authenticity and quality of your data.\nPart of eager is MultiQC, which contains an array of tools that allow you to assess the quality of your data. Let’s start by assessing the sequencing quality. For this, we can use FastQC - a quality control tool for high throuput data contained in MultiQC. FastQC gives you statistics on various aspects of your data that can help assess the quality, and raises warnings or points out failures of your data clearly colour-coded: Green for “pass”, yellow for “warning”, red for “fail”.\n Nf-core/egaer provides the statistics for untrimmed data, which still contains the adapters, and – if you scroll down - for trimmed data (where adapters are removed). One (me) could argue that the untrimmed is not so relevant, and you might get some confusing alerts that are actually not that terrible and unexpected in untrimmed data. However, there are instances where the comparison of the untrimmed vs. trimmed data might be informative.\nIn the interest of conciseness, I will not go through every plot individually to explain what it does, because that is something that the MultiQC report can do. Just click the tiny “Help” button, and you will get a window with an explanation of what the respective statistic shows. If you need more information on a specific field, you can also dive into detail in the FastQC help.\n\n\n\n\n\nI will point out a few statistics that (I believe) one should check to make sure that the run went ok, and no problems should be expected downstream that are attributable to the sequencing quality.\nIf you want to be lazy, skip straight to the “Status Checks”. This gives an overall impression through the colour code.\n\n\n\n\n\nIn the untrimmed summary, there are a couple of “red flags” that are expected, like over represented sequences and the adapter content (as they have not yet been removed and are therefore over represented). Should they still be flagged in the “trimmed” summary, one should investigate further.\nTo assess the quality of the run, you want to watch some statistics closely:\nThe sequencing quality scores. In the above example, there are two libraries that are marked yellow. This is acceptable, especially because the quality is expected to go down as the run progresses. A closer inspection shows that the two libraries descend into the yellow area only towards the end of the read. Should more libraries fall into the red, or start falling in the middle of the read, you might ask the lab to check the run quality.\n\n\n\n\n\nI like to go straight for the trimmed ones, and make sure there is no drastic drop in the quality. It is expected to drop into the yellow field towards the end, but should not drop into the red or drop already in the middle of the read.\nThe Per Sequence Quality Scores should be overall high, but some lower scores can be acceptable.\nThe Per Base N content, should be low. N is a substitute for when the eqeuncer is unable to confidently identify a base.\nIf those three are ok, the sequencing run probably was, too.\nTo assess the structure and quality of your data, and to decide how to continue with your journey in this project a few statistics are enough. But for some specific needs, you might want to check an extended set.\nLet’s say we are assessing preservation in a certain set of individuals. You have put your blood sweat and tears into sampling, have seen the glow in the Archaeologists eyes and your hopes are high. Is there something we can work with?\nYou can determine this essentially with two statistics: The Endogenous DNA Post (%), and the 5Prime C&gt;T1st base columns in the “General Statistics” section of your MultiQC report.\n\n\n\n\n\nLet’s check damage first. In the “maturing” of DNA to ancient DNA, hydrolytic (catalysed by H2O) processes alter one of the bases (Cytocin) by removing an Amino group (NH3), turning it into Uracil. In the library protocols, Uracil (which in living creatures is only present in RNA) is “misread” as a Thymin, and therefor the synthetization of new strands in the process of library building misincorporates a Thymin, where a Cytosin should be – the famous “C to T misincorporation”. In an authentically ancient library, we expect high levels of this observation (A T where a C is expected) especially towards the end of reads (because DNA strands also tend to break in such deaminated positions, because they have lost some stability). So we want to see a high percentage of C&gt;T substitutions especially in the 1st base of our read. In our example above, this is the case for the first two libraries. Above 30% is great, around 15% is very common, 5% is really low, and our threshold for a “properly damaged” library. When the damage is enzymatically repaired during the library preparation (UDG treatment), we expect much lower levels, but the 5% threshold stands.\nNow we know that our library has damage, but does it have enough damaged reads of our target organism (humans?)? This is reflected in the % endogenous DNA, which is the percentage of mapped (to the target reference genome) reads over the total reads that went into the mapping. As the nf-core/eager documentation so nicely puts it “Assuming a perfect ancient sample with no modern contamination, this would be the amount of true ancient DNA in the sample…”\n“…However this value most likely include contamination and will not entirely be the true ‘endogenous’ content.”\nwhich is why I recommend to look specifically at the Endogenous DNA Post (%) value. Here, the reads mapping to the target reference also have to pass a certain mapping quality filter (30 is the default in most eager.configs I have seen. This value is NOT provided in the autoprocessing, so when you check that for your data, bear that in mind!). Here, the threshold to consinder the sample preserved enough is 0.05. (previously 0.1). So in our example, we are sorry to inform you that your project is not feasible with the current methods. All individuals are under 0.05%, and it would not be economic or good for your mental health to try and squeeze something out of these libraries for the next 3 years. If you are above 0.05 or even 0.1, congratulations, you can proceed with a targeted enrichment approach that will be also absolutely necessary in this case. If this value is above 30%, you might want to consider even producing a shotgun genome.\nWhen considering a shotgun genome, or when contemplating deeper sequencing of a targeted enriched library, the Sequence Duplication Levels become very relevant. They tell you about the complexity of your library: Low duplication rates promise high levels of coverage, high duplication reads tell you not to expect too much from this library. Schiffels’ law #4 states that if you have more than 25% duplications, then you should stop using the library you are currently squeezing, and produce a new library or ask for another capture.\nIn some edge cases, the Length Distribution (after trimming) is a relevant statistic to consider. Literature says we should expect authentic ancient DNA to be around 30 -65 bp. But this might be different for pathogens, or for modern DNA that you might have intentionally sampled and treated to be processed with the ancient DNA protocols. A majority of very small fragements can indicate high levels of microbial contamination. A lot of very long reads? Maybe your favourite Archaeologist greets you here. Or an indication you should shear your DNA more when you want to process modern samples and are too cheap to outsource that to a company.\nGC-content: can be indicative of contamination, BUT: Since we don’t know the the GC content of the genome the modal GC content is calculated from the observed data and used to build a reference distribution, so it is not a standardized measure across runs, and therefore I don’t pay too much attention/overinterpret this result."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra\nKatırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023.\n“Benchmarking Kinship Estimation Tools for Ancient Genomes Using\nPedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor\nMapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and\nTorsten Günther. 2024. “READv2: Advanced and\nUser-Friendly Detection of Biological Relatedness in\nArchaeogenomics.” bioRxiv.\n\n\nAnnoni, A. et al. 2003. “Map Projections for Europe.”\nTechnical Report EUR 20120 EN. European Commission Joint Research\nCentre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan\nWoellauer. 2023. Mapview: Interactive Viewing of Spatial Data in\nr. https://github.com/r-spatial/mapview.\n\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013.\n“Estimating and Interpreting FST: The Impact of Rare\nVariants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel,\nJohannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso,\nand Sven Nahnsen. 2020. “The Nf-Core Framework for\nCommunity-Curated Bioinformatics Pipelines.” Nature\nBiotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\n\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida\nAndrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia,\nJudith Neukamm, and Alexander Peltzer. 2021. “Reproducible,\nPortable, and Efficient Ancient Genome Reconstruction with\nNf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi.\n2021. “TKGWV2: An Ancient DNA\nRelatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun\nData.” Sci. Rep. 11 (1): 21262.\n\n\nFreilich, Suzanne, Harald Ringbauer, Dženi Los, Mario Novak, Dinko\nTresić Pavičić, Stephan Schiffels, and Ron Pinhasi. 2021.\n“Reconstructing Genetic Histories and Social Organisation in\nNeolithic and Bronze Age Croatia.” Scientific Reports 11\n(1): 1–16. https://doi.org/10.1038/s41598-021-94932-9.\n\n\nFu, Qiaomei, Mateja Hajdinjak, Oana Teodora Moldovan, Silviu Constantin,\nSwapan Mallick, Pontus Skoglund, Nick Patterson, et al. 2015. “An\nEarly Modern Human from Romania with a Recent Neanderthal\nAncestor.” Nature 524 (7564): 216–19. https://doi.org/10.1038/nature14558.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar\nU Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes\nReveal Social and Genetic Structure of Late Neolithic\nSwitzerland.” Nat. Commun. 11 (1): 1915.\n\n\nGramacy, Robert B. 2016. “laGP:\nLarge-Scale Spatial Modeling via Local Approximate Gaussian Processes in\nR.” Journal of Statistical Software 72 (1):\n1–46. https://doi.org/10.18637/jss.v072.i01.\n\n\n———. 2020. Surrogates: Gaussian Process Modeling,\nDesign and  Optimization for the Applied Sciences. Boca Raton,\nFlorida: Chapman Hall/CRC.\n\n\nGreen, Richard E., Johannes Krause, Adrian W. Briggs, Tomislav Maricic,\nUdo Stenzel, Martin Kircher, Nick Patterson, et al. 2010a. “A\nDraft Sequence of the Neandertal\nGenome.” Science 328 (5979): 710. https://doi.org/10.1126/science.1188021.\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic,\nUdo Stenzel, Martin Kircher, Nick Patterson, et al. 2010b. “A\nDraft Sequence of the Neandertal Genome.” Science 328\n(5979): 710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nHajdinjak, Mateja, Fabrizio Mafessoni, Laurits Skov, Benjamin Vernot,\nAlexander Hübner, Qiaomei Fu, Elena Essel, et al. 2021. “Initial\nUpper Palaeolithic Humans in Europe Had Recent Neanderthal\nAncestry.” Nature 592 (7853): 253–57. https://doi.org/10.1038/s41586-021-03335-3.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62.\n\n\nHigham, Tom, Katerina Douka, Rachel Wood, Christopher Bronk Ramsey,\nFiona Brock, Laura Basell, Marta Camps, et al. 2014. “The Timing\nand Spatiotemporal Patterning of Neanderthal Disappearance.”\nNature 512 (7514): 306–9. https://doi.org/10.1038/nature13621.\n\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of\nLevels of Gene Flow from DNA Sequence Data.”\nGenetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\n\nIasi, Leonardo N. M., Manjusha Chintalapati, Laurits Skov, Alba Bossoms\nMesa, Mateja Hajdinjak, Benjamin M. Peter, and Priya Moorjani. 2024.\n“Neandertal Ancestry Through Time: Insights from Genomes of\nAncient and Present-Day Humans.” http://dx.doi.org/10.1101/2024.05.13.593955.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton,\nAdam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017.\n“Archaeogenomic Evidence Reveals Prehistoric Matrilineal\nDynasty.” Nat. Commun. 8 (February): 14115.\n\n\nKistler, Logan, Roselyn Ware, Oliver Smith, Matthew Collins, and Robin\nG. Allaby. 2017. “A New Model for Ancient DNA Decay Based on\nPaleogenomic Meta-Analysis.” Nucleic Acids Research 45\n(11): 6310–20. https://doi.org/10.1093/nar/gkx361.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela,\nAnna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018.\n“Ancient Fennoscandian Genomes Reveal Origin and Spread of\nSiberian Ancestry in Europe.” Nature Communications 9\n(1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nLazaridis, Iosif, Nick Patterson, Alissa Mittnik, Gabriel Renaud, Swapan\nMallick, Karola Kirsanow, Peter H Sudmant, et al. 2014. “Ancient\nHuman Genomes Suggest Three Ancestral Populations for Present-Day\nEuropeans.” Nature 513 (7518): 409–13. https://doi.org/10.1038/nature13673.\n\n\nLi, Heng, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils\nHomer, Gabor Marth, Goncalo Abecasis, Richard Durbin, and 1000 Genome\nProject Data Processing Subgroup. 2009. “The Sequence\nAlignment/Map Format and SAMtools.”\nBioinformatics 25 (16): 2078–79. https://doi.org/10.1093/bioinformatics/btp352.\n\n\nLindahl, Tomas, and Barbro Nyberg. 1974. “Heat-Induced Deamination\nof Cytosine Residues in Deoxyribonucleic Acid.”\nBiochemistry 13 (16): 3405–10. https://doi.org/10.1021/bi00713a035.\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif\nLazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023.\n“The Allen Ancient DNA Resource (AADR):\nA Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015.\n“Evaluating the Use of ABBA-BABA Statistics to Locate\nIntrogressed Loci.” Molecular Biology and Evolution 32\n(1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nMassicotte, Philippe, and Andy South. 2024. Rnaturalearth: World Map\nData from Natural Earth. https://docs.ropensci.org/rnaturalearth/.\n\n\nMcKenna, Aaron, Matthew Hanna, Eric Banks, Andrey Sivachenko, Kristian\nCibulskis, Andrew Kernytsky, Kiran Garimella, et al. 2010. “The\nGenome Analysis Toolkit: A MapReduce Framework for\nAnalyzing Next-Generation DNA Sequencing Data.”\nGenome Research 20 (9): 1297–1303. https://doi.org/10.1101/gr.107524.110.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in\nPython,” 56–61.\n\n\nMeyer, Matthias, Juan-Luis Arsuaga, Cesare de Filippo, Sarah Nagel,\nAyinuer Aximu-Petri, Birgit Nickel, Ignacio Martínez, et al. 2016.\n“Nuclear DNA Sequences from the Middle Pleistocene Sima de Los\nHuesos Hominins.” Nature 531 (7595): 504–7. https://doi.org/10.1038/nature17405.\n\n\nMeyer, Matthias, Martin Kircher, Marie-Theres Gansauge, Heng Li,\nFernando Racimo, Swapan Mallick, Joshua G. Schraiber, et al. 2012.\n“A High-Coverage Genome Sequence from an Archaic\nDenisovan Individual.” Science (New York,\nN.Y.) 338 (6104): 222–26. https://doi.org/10.1126/science.1224344.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018.\n“Estimating Genetic Kin Relationships in Prehistoric\nPopulations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin\nRohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich.\n2012a. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\n———. 2012b. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93.\n\n\nPebesma, Edzer. 2018. “Simple Features for R:\nStandardized Support for Spatial Vector Data.”\nThe R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPeter, Benjamin M. 2016a. “Admixture,\nPopulation Structure, and F-Statistics.” Genetics\n202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913.\n\n\n———. 2016b. “Admixture, Population Structure, and\nF-Statistics.” Genetics 202 (4): 1485–1501.\nhttps://doi.org/10.1534/genetics.115.183913.\n\n\n———. 2020. “100,000 Years of Gene Flow Between Neandertals and\nDenisovans in the Altai Mountains.” http://dx.doi.org/10.1101/2020.03.13.990523.\n\n\nPeyrégné, Someone. 2020. “Title of the Article.”\nJournal Name 10: 100–110. https://doi.org/10.1000/j.journal.2020.01.001.\n\n\nPrüfer, Kay, Cesare de Filippo, Steffi Grote, Fabrizio Mafessoni, Petra\nKorlević, Mateja Hajdinjak, Benjamin Vernot, et al. 2017. “A\nHigh-Coverage Neandertal Genome from Vindija Cave in Croatia.”\nScience 358 (6363): 655–58. https://doi.org/10.1126/science.aao1887.\n\n\nPrüfer, Kay, Cosimo Posth, He Yu, Alexander Stoessel, Maria A. Spyrou,\nThibaut Deviese, Marco Mattonai, et al. 2021. “A Genome Sequence\nfrom a Modern Human Skull over 45,000 Years Old from Zlatý\nkůň in Czechia.” Nature Ecology\n& Evolution 5 (6): 820–25. https://doi.org/10.1038/s41559-021-01443-x.\n\n\nRalf, Arwin, Diego Montiel González, Kaiyin Zhong, and Manfred Kayser.\n2018. “Yleaf: Software for Human Y-Chromosomal\nHaplogroup Inference from Next-Generation Sequencing Data.”\nMolecular Biology and Evolution 35 (7): 1820. https://doi.org/10.1093/molbev/msy080.\n\n\nReich, David, Richard E. Green, Martin Kircher, Johannes Krause, Nick\nPatterson, Eric Y. Durand, Bence Viola, et al. 2010. “Genetic\nHistory of an Archaic Hominin Group from Denisova\nCave in Siberia.” Nature 468\n(December): 1053. https://doi.org/10.1038/nature09710.\n\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane\nMazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing\nNative American Population History.” Nature 488 (7411):\n370–74.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı,\nMarie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020.\n“Ancient Genome-Wide DNA from France Highlights the\nComplexity of Interactions Between Mesolithic Hunter-Gatherers and\nNeolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak.\n2023. “BREADR: An r Package for the Bayesian Estimation of Genetic\nRelatedness from Low-Coverage Genotype Data.” bioRxiv.\nhttps://doi.org/10.1101/2023.04.17.537144.\n\n\nSawyer, Susanna, Johannes Krause, Katerina Guschanski, Vincent\nSavolainen, and Svante Pääbo. 2012. “Temporal Patterns of\nNucleotide Misincorporations and DNA Fragmentation in Ancient\nDNA.” Edited by Carles Lalueza-Fox. PLoS ONE 7 (3):\ne34131. https://doi.org/10.1371/journal.pone.0034131.\n\n\nSchmid, Clemens, and Stephan Schiffels. 2023. “Estimating Human\nMobility in Holocene Western Eurasia with Large-Scale Ancient Genomic\nData.” Proceedings of the National Academy of Sciences\n120 (9). https://doi.org/10.1073/pnas.2218375120.\n\n\nSkov, Laurits, Ruoyun Hui, Vladimir Shchur, Asger Hobolth, Aylwyn\nScally, Mikkel Heide Schierup, and Richard Durbin. 2018.\n“Detecting Archaic Introgression Using an Unadmixed\nOutgroup.” Edited by Fernando Racimo. PLOS Genetics 14\n(9): e1007641. https://doi.org/10.1371/journal.pgen.1007641.\n\n\nTsoulos, Lysandros. 2003. “An Equal Area Projection for\nStatistical Mapping in the EU.” In Map Projections for\nEurope, edited by A. Annoni et al., 50–55. European Commission\nJoint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.”\nAnnual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot\nAnnotations for ’Ggplot2’. https://wilkelab.org/cowplot/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific\nData Management and Stewardship.” Scientific Data 3 (1).\nhttps://doi.org/10.1038/sdata.2016.18.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and\nRelationship.” Am. Nat. 56 (645): 330–38."
  }
]